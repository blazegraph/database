package com.bigdata.bop.rdf.joinGraph;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FilenameFilter;
import java.io.InputStream;
import java.util.Arrays;
import java.util.LinkedList;
import java.util.List;
import java.util.Properties;
import java.util.UUID;

import junit.framework.TestCase2;

import org.openrdf.rio.RDFFormat;

import com.bigdata.bop.BOp;
import com.bigdata.bop.BOpContextBase;
import com.bigdata.bop.Constant;
import com.bigdata.bop.IPredicate;
import com.bigdata.bop.IVariable;
import com.bigdata.bop.NV;
import com.bigdata.bop.Var;
import com.bigdata.bop.controller.JoinGraph;
import com.bigdata.bop.controller.JoinGraph.Edge;
import com.bigdata.bop.controller.JoinGraph.JGraph;
import com.bigdata.bop.controller.JoinGraph.Path;
import com.bigdata.bop.controller.JoinGraph.Vertex;
import com.bigdata.bop.engine.QueryEngine;
import com.bigdata.bop.fed.QueryEngineFactory;
import com.bigdata.journal.ITx;
import com.bigdata.journal.Journal;
import com.bigdata.rdf.axioms.RdfsAxioms;
import com.bigdata.rdf.inf.ClosureStats;
import com.bigdata.rdf.model.BigdataURI;
import com.bigdata.rdf.model.BigdataValue;
import com.bigdata.rdf.model.BigdataValueFactory;
import com.bigdata.rdf.rio.LoadStats;
import com.bigdata.rdf.spo.SPOPredicate;
import com.bigdata.rdf.store.AbstractTripleStore;
import com.bigdata.rdf.store.DataLoader;
import com.bigdata.rdf.store.LocalTripleStore;
import com.bigdata.rdf.store.DataLoader.ClosureEnum;

/**
 * Unit tests for runtime query optimization using {@link JoinGraph} and the
 * LUBM U1 data set.
 * 
 * @author <a href="mailto:thompsonbry@users.sourceforge.net">Bryan Thompson</a>
 * @version $Id: TestJoinGraph.java 3918 2010-11-08 21:31:17Z thompsonbry $
 */
public class TestJoinGraphOnLubm extends TestCase2 {

    /**
     * 
     */
    public TestJoinGraphOnLubm() {
    }

	/**
	 * @param name
	 */
	public TestJoinGraphOnLubm(String name) {
		super(name);
	}

	@Override
	public Properties getProperties() {

		final Properties p = new Properties(super.getProperties());

//		p.setProperty(Journal.Options.BUFFER_MODE, BufferMode.Transient
//				.toString());

		/* 
		 * Enable RDFS entailments.
		 */
		p.setProperty(
				com.bigdata.rdf.store.AbstractTripleStore.Options.AXIOMS_CLASS,
				RdfsAxioms.class.getName());

		/*
		 * Don't compute closure in the data loader since it does TM, not
		 * database at once closure.
		 */
		p.setProperty(DataLoader.Options.CLOSURE, ClosureEnum.None.toString());

		return p;

	}

    /**
     * Reads files matching the filter from the directory and return
     * an array containing their path names.
     * 
     * @param dir
     *            The directory.
     * @param filter
     *            The filter.
     */
    private String[] readFiles(File dir, FilenameFilter filter) {

        assertTrue("No such file or directory: " + dir, dir.exists());

        assertTrue("Not a directory: " + dir, dir.isDirectory());

        final File[] files = dir.listFiles(filter);

        assertNotNull("Could not read directory: " + dir, files);
        
        final String[] resources = new String[files.length];
        
        for(int i=0; i<files.length; i++) {
            
            resources[i] = files[i].toString();
            
        }
        
        return resources;
        
    }
    
    private Journal jnl;
    
	private String namespace;

	/**
	 * The {@link UUID} of a {@link Journal} resource used by this test.
	 * 
	 * @todo It would be nice to have a comment for the journal so we could tell
	 *       what was in each one. That would probably be a one time thing, or
	 *       at least something which was linked from the root blocks.
	 * 
	 * @todo Verify that we can correctly open RW and WORM journals without any
	 *       hints.
	 */
	private static final UUID resourceId = UUID.fromString("bb93d970-0cc4-48ca-ba9b-123412683b3d");
	
	/**
	 * Loads LUBM U1 into a triple store.
	 */
	protected void setUp() throws Exception {

		super.setUp();

//		System.err.println(UUID.randomUUID().toString());
//		System.exit(0);
		
		final Properties properties = getProperties();

		final File file;
		if (false) {
			/*
			 * Use a persistent file that is generated once and then reused by
			 * each test run.
			 */
			final File tmpDir = new File(System.getProperty("java.io.tmpdir"));
			final File testDir = new File(tmpDir, "bigdata-tests");
			testDir.mkdirs();
			file = new File(testDir, resourceId + ".jnl");
			namespace = "LUBM_U1";
		} else {
			/*
			 * Use a specific file generated by some external process.
			 */
			file = new File("/data/lubm/U50/bigdata-lubm.WORM.jnl");
			namespace = "LUBM_U50";
		}
		
		properties.setProperty(Journal.Options.FILE, file.toString());

//		properties.setProperty(Journal.Options.BUFFER_MODE,BufferMode.DiskRW.toString());
		
		if (!file.exists()) {

			jnl = new Journal(properties);

			final String[] dataFiles = readFiles(new File(
					"bigdata-rdf/src/resources/data/lehigh/U1"),
					new FilenameFilter() {
						public boolean accept(File dir, String name) {
							return name.endsWith(".owl");
						}
					});

			// And add in the ontology.
			final List<String> tmp = new LinkedList<String>();
			tmp.add("bigdata-rdf/src/resources/data/lehigh/univ-bench.owl");
			tmp.addAll(Arrays.asList(dataFiles));
			final String[] resources = tmp.toArray(new String[tmp.size()]);

			final AbstractTripleStore tripleStore = new LocalTripleStore(jnl,
					namespace, ITx.UNISOLATED, getProperties());

			// Create the KB instance.
			tripleStore.create();

			// Load LUBM U1, including its ontology, and compute the RDFS
			// closure.
			loadData(tripleStore, resources);

			// Truncate the journal (trim its size).
			jnl.truncate();
			
			// Commit the journal.
			jnl.commit();

			// Close the journal.
			jnl.close();
			
		}

		// Open the test resource.
		jnl = new Journal(properties);

	}

	/**
	 * Loads the data into the closureStore and computes the closure.
	 */
	private void loadData(final AbstractTripleStore closureStore,
			final String[] resources) throws Exception {

		final LoadStats totals = new LoadStats();
		
		for (String resource : resources) {

			InputStream is = null;
			String baseURI;

			try {

				is = new FileInputStream(new File(resource));
				baseURI = new File(resource).toURI().toString();

			} catch (FileNotFoundException ex) {

				is = getClass().getResourceAsStream(resource);
				java.net.URL resourceUrl = getClass().getResource(resource);

				// if the resource couldn't be found in the file system
				// and couldn't be found by searching from this class'
				// package (com.bigdata.rdf.rules) as root, then use
				// the class loader to try searching from the root of
				// the JAR itself
				if (resourceUrl == null) {
					is = getClass().getClassLoader().getResourceAsStream(
							resource);
					resourceUrl = getClass().getClassLoader().getResource(
							resource);
				}

				if (resourceUrl == null) {
					log.warn("resource not found [" + resource + "]");
					throw new Exception("FAILURE: resource not found ["
							+ resource + "]");
				}

				// must encode spaces in URL before new URI
				String encodedUrlStr = resourceUrl.toString().replaceAll(" ",
						"%20");
				java.net.URI resourceUri = new java.net.URI(encodedUrlStr);
				baseURI = resourceUri.toString();
			}

			if (is == null) {

				fail("No such file or resource: " + resource);

			}

			final LoadStats tmp = closureStore.getDataLoader()
					.loadData(is, baseURI, RDFFormat.RDFXML);

			totals.add(tmp);
			
		}

//      if(log.isInfoEnabled())
//    	log.info
		System.out.println(totals.toString());
		
		/*
		 * Compute the database at once closure.
		 */
		final ClosureStats closureStats = closureStore.getInferenceEngine()
				.computeClosure(null/* focusStore */);

//        if(log.isInfoEnabled())
//        	log.info
        	System.out.println(closureStats.toString());
        	
	}

	public void test_queryOptimizer() throws Exception {

		final AbstractTripleStore database = (AbstractTripleStore) jnl
				.getResourceLocator()
				.locate(namespace, jnl.getLastCommitTime());

		if (database == null)
			throw new RuntimeException("Not found: " + namespace);

		/*
		 * Resolve terms against the lexicon.
		 */
		final BigdataValueFactory f = database.getLexiconRelation()
				.getValueFactory();

		final BigdataURI rdfType = f
				.createURI("http://www.w3.org/1999/02/22-rdf-syntax-ns#type");

		final BigdataURI graduateStudent = f
				.createURI("http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl#GraduateStudent");

		final BigdataURI university = f
				.createURI("http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl#University");

		final BigdataURI department = f
				.createURI("http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl#Department");

		final BigdataURI memberOf = f
				.createURI("http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl#memberOf");

		final BigdataURI subOrganizationOf = f
				.createURI("http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl#subOrganizationOf");

		final BigdataURI undergraduateDegreeFrom = f
				.createURI("http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl#undergraduateDegreeFrom");

		final BigdataValue[] terms = new BigdataValue[] { rdfType,
				graduateStudent, university, department, memberOf,
				subOrganizationOf, undergraduateDegreeFrom };

		// resolve terms.
		database.getLexiconRelation()
				.addTerms(terms, terms.length, true/* readOnly */);

		{
			for (BigdataValue tmp : terms) {
				System.out.println(tmp + " : " + tmp.getIV());
				if (tmp.getIV() == null)
					throw new RuntimeException("Not defined: " + tmp);
			}
		}

		final IPredicate[] preds;
		{
			final IVariable<?> x = Var.var("x");
			final IVariable<?> y = Var.var("y");
			final IVariable<?> z = Var.var("z");

			// The name space for the SPO relation.
			final String[] relation = new String[] { namespace + ".spo" };

			final long timestamp = jnl.getLastCommitTime();

			int nextId = 0;

			// ?x a ub:GraduateStudent .
			final IPredicate p0 = new SPOPredicate(new BOp[] { x,
					new Constant(rdfType.getIV()),
					new Constant(graduateStudent.getIV()) },//
					new NV(BOp.Annotations.BOP_ID, nextId++),//
					new NV(IPredicate.Annotations.TIMESTAMP, timestamp),//
					new NV(IPredicate.Annotations.RELATION_NAME, relation)//
			);

			// ?y a ub:University .
			final IPredicate p1 = new SPOPredicate(new BOp[] { y,
					new Constant(rdfType.getIV()),
					new Constant(university.getIV()) },//
					new NV(BOp.Annotations.BOP_ID, nextId++),//
					new NV(IPredicate.Annotations.TIMESTAMP, timestamp),//
					new NV(IPredicate.Annotations.RELATION_NAME, relation)//
			);

			// ?z a ub:Department .
			final IPredicate p2 = new SPOPredicate(new BOp[] { z,
					new Constant(rdfType.getIV()),
					new Constant(department.getIV()) },//
					new NV(BOp.Annotations.BOP_ID, nextId++),//
					new NV(IPredicate.Annotations.TIMESTAMP, timestamp),//
					new NV(IPredicate.Annotations.RELATION_NAME, relation)//
			);

			// ?x ub:memberOf ?z .
			final IPredicate p3 = new SPOPredicate(new BOp[] { x,
					new Constant(memberOf.getIV()), z },//
					new NV(BOp.Annotations.BOP_ID, nextId++),//
					new NV(IPredicate.Annotations.TIMESTAMP, timestamp),//
					new NV(IPredicate.Annotations.RELATION_NAME, relation)//
			);

			// ?z ub:subOrganizationOf ?y .
			final IPredicate p4 = new SPOPredicate(new BOp[] { z,
					new Constant(subOrganizationOf.getIV()), y },//
					new NV(BOp.Annotations.BOP_ID, nextId++),//
					new NV(IPredicate.Annotations.TIMESTAMP, timestamp),//
					new NV(IPredicate.Annotations.RELATION_NAME, relation)//
			);

			// ?x ub:undergraduateDegreeFrom ?y
			final IPredicate p5 = new SPOPredicate(new BOp[] { x,
					new Constant(undergraduateDegreeFrom.getIV()), y },//
					new NV(BOp.Annotations.BOP_ID, nextId++),//
					new NV(IPredicate.Annotations.TIMESTAMP, timestamp),//
					new NV(IPredicate.Annotations.RELATION_NAME, relation)//
			);

			// the vertices of the join graph (the predicates).
			preds = new IPredicate[] { p0, p1, p2, p3, p4, p5 };
		}

		// final JoinGraph op = new JoinGraph(//
		// new NV(JoinGraph.Annotations.VERTICES, preds),//
		// new NV(JoinGraph.Annotations.SAMPLE_SIZE, 100) //
		// );

		final JGraph g = new JGraph(preds);

		final int limit = 100;

		final QueryEngine queryEngine = QueryEngineFactory
				.getQueryController(jnl/* indexManager */);

		final BOpContextBase context = new BOpContextBase(queryEngine);

		System.err.println("joinGraph=" + g.toString());

		/*
		 * Sample the vertices.
		 * 
		 * @todo Sampling for scale-out not yet finished.
		 * 
		 * @todo Re-sampling might always produce the same sample depending
		 * on the sample operator impl (it should be random, but it is not).
		 */
		g.sampleVertices(context, limit);

		System.err.println("joinGraph=" + g.toString());

		/*
		 * Estimate the cardinality and weights for each edge, obtaining the
		 * Edge with the minimum estimated cardinality. This will be the
		 * starting point for the join graph evaluation.
		 * 
		 * @todo It would be very interesting to see the variety and/or
		 * distribution of the values bound when the edge is sampled. This
		 * can be easily done using a hash map with a counter. That could
		 * tell us a lot about the cardinality of the next join path
		 * (sampling the join path also tells us a lot, but it does not
		 * explain it as much as seeing the histogram of the bound values).
		 * I believe that there are some interesting online algorithms for
		 * computing the N most frequent observations and the like which
		 * could be used here.
		 * 
		 * @todo We should be saving the materialized intermediate results,
		 * if not for the initialization, then for the chain sampling.
		 * 
		 * FIXME ROX is choosing the starting edge based on the minimum
		 * estimated cardinality. However, it is possible for there to be
		 * more than one edge with an estimated cardinality which is
		 * substantially to the minimum estimated cardinality. It would be
		 * best to start from multiple vertices so we can explore join paths
		 * which begin with those alternative starting vertices as well.
		 * (LUBM Q2 is an example of such a query).
		 */
		g.estimateEdgeWeights(queryEngine, limit);

		System.err.println("joinGraph=" + g.toString());

		/*
		 * The starting vertex for chain sampling is the vertex in the edge
		 * with the minimum estimated cardinality whose range count is
		 * smallest. Beginning with that vertex, create a set of paths for
		 * each edge branching from that vertex. Since those edges were
		 * already sampled, we can immediately form a set of join paths with
		 * their estimated cardinality and hit ratios (aka scale factors).
		 * 
		 * The initial set of paths consists of a single path whose sole
		 * edge is the edge with the minimum estimated cardinality.
		 */
		final Edge startEdge = g
				.getMinimumCardinalityEdge(null/* visited */);
		
		if (startEdge == null)
			throw new RuntimeException("No weighted edges.");

		/*
		 * Generate a set of paths by extending that starting vertex in one step
		 * in each possible direction. For the initial one-step extension of the
		 * starting vertex we can reuse the estimated cardinality of each edge
		 * in the join graph, which was already computed above.
		 * 
		 * @todo This starts out with the minimum cardinality vertex of the
		 * minimum cardinality edge. This means that it will not explore plans
		 * which start from the next best cardinality vertex or edge. It seems
		 * to me that such plans could in some cases dominate the plans
		 * considered by ROX for the same reason that ROX does a bit of hill
		 * climbing to see if it is in a local minimum. I can see two reasons
		 * why ROX might not consider such plans. One is to prune the search
		 * space. Another has to do with the termination criteria, which might
		 * not be able to compare plans which start from (or branch from)
		 * different vertices.
		 */
//		final Path[] paths;
//		{
//
//			System.err.println("startEdge="+startEdge);
//
//			// The starting vertex is the one with the minimum est.
//			// cardinality.
//			final Vertex startVertex = startEdge
//					.getMinimumCardinalityVertex();
//
//			System.err.println("startVertex=" + startVertex);
//
//			// Find the set of edges branching from the starting vertex.
//			final List<Edge> branches = g
//					.getEdges(startVertex, null/* visited */);
//
//			if (branches.isEmpty()) {
//
//				// No vertices remain to be explored so we should just execute something.
//				throw new RuntimeException("Paths can not be extended");
//				
//			} else if (branches.size() == 1) {
//
//				final Edge e = branches.get(0);
//
//				final Path path = new Path(new Edge[] { e });
//
//				// The initial sample is just the sample for that edge.
//				path.sample = e.sample;
//
//				System.err.println("path=" + path);
//
//				paths = new Path[] { path };
//
//			} else {
//
//				final List<Path> list = new LinkedList<Path>();
//
//				// Create one path for each of those branches.
//				for (Edge e : branches) {
//
//					if (e.v1 != startVertex && e.v2 != startVertex)
//						continue;
//
//					// Create a one step path.
//					final Path path = new Path(new Edge[] { e });
//
//					// The initial sample is just the sample for that edge.
//					path.sample = e.sample;
//
//					System.err
//							.println("path[" + list.size() + "]: " + path);
//
//					list.add(path);
//
//				}
//
//				paths = list.toArray(new Path[list.size()]);
//
//			}
//
//			System.err.println("selectedJoinPath: "
//					+ g.getSelectedJoinPath(paths));
//			
//		}

		/**
		 * Initial one-step paths starting from V2. The estimated cardinality of
		 * these paths is just the estimated cardinality of the corresponding
		 * edge, which is something that we have already computed.
		 * 
		 * (V2,V3)
		 * 
		 * (V2,V4)
		 * 
		 * Paths which are one step extensions of those initial edges:
		 * 
		 * (V2,V3),(V{2.3},V4)
		 * 
		 * (V2,V3),(V3,V0)
		 * 
		 * (V2,V3),(V3,V5)
		 * 
		 * (V2,V4),(V4,V1)
		 * 
		 * (V2,V4),(V4,V3)
		 * 
		 * (v2,V4),(V4,V5)
		 * 
		 * @todo Next compute the estimated cardinality of these 2-step paths.
		 * 
		 * @todo If we are able to compare paths which branch from different
		 *       vertices then we can also compare paths which start from
		 *       different vertices.
		 * 
		 * @todo ROX probably chooses a single starting vertex in order to
		 *       reduce the size of the search space. Allowing the path to
		 *       branch in other ways might make the search space much bigger
		 *       and may also require a different rule for comparing plans.
		 */
		{

			final Vertex v0 = g.getVertex(0);
			final Vertex v1 = g.getVertex(1);
			final Vertex v2 = g.getVertex(2);
			final Vertex v3 = g.getVertex(3);
			final Vertex v4 = g.getVertex(4);
			final Vertex v5 = g.getVertex(5);

			/*
			 * The initial set of paths starting from the minimum estimated
			 * cardinality vertex of the minimum estimated cardinality edge.
			 * 
			 * @todo build this programmtically by finding the minimum
			 * cardinality edge and the minimum cardinality vertex and then
			 * creating one path for each edge joining the starting vertex to
			 * another vertex. [ROX has the constraint that there is a single
			 * starting vertex. We might relax that constraint to allow paths to
			 * start at more than one low cardinality vertex.]
			 */
			final Path p0 = new Path(g.getEdge(v2, v3));
			final Path p1 = new Path(g.getEdge(v2, v4));
			final Path p2 = new Path(g.getEdge(v4, v1));
			final Path[] paths_t0 = new Path[] { p0, p1, p2 };
			System.err.println("\n*** Paths @ t0\n"
					+ JoinGraph.showTable(paths_t0));

			int round = 1;
			final Path[] paths_t1 = g.expand(queryEngine, limit, round++, paths_t0);
			final Path[] paths_t2 = g.expand(queryEngine, limit, round++, paths_t1);
			final Path[] paths_t3 = g.expand(queryEngine, limit, round++, paths_t2);
			final Path[] paths_t4 = g.expand(queryEngine, limit, round++, paths_t3);

		}


	} // test

}

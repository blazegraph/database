nciOncology.owl, embedded federation.

INFO : 31844   Main Thread com.bigdata.rdf.rio.BasicRioLoader.loadRdf(BasicRioLoader.java:194): parse complete: elapsed=28187ms, toldTriples=464841, tps=16491

INFO : 882875   Main Thread com.bigdata.rdf.store.DataLoader.loadData(DataLoader.java:517): Loaded 1 resources: 464841 stmts added in 28.265 secs, rate= 528, commitLatency=0ms
rule    	ms	#entms	entms/ms
RuleFastClosure13	15	0	0
RuleOwlEquivalentProperty	16	0	0
RuleRdfs02	5890	395806	67
RuleRdfs03	3297	395806	120
RuleRdfs08	31	41618	1342
RuleRdfs09	12109	41724	3
RuleRdfs10	110	41618	378
RuleRdfs11	720126	5324314	7
totals: elapsed=741594, nadded=376849, numComputed=6241034, added/sec=508, computed/sec=8415

Note: this appears to be incremental TM rather than database at once closure.

========================================

Modified to use database at once closure.

nciOncology.owl, embedded federation.

INFO : 36188   Main Thread com.bigdata.rdf.store.DataLoader.loadData2(DataLoader.java:628): 464841 stmts added in 32.109 secs, rate= 14476, commitLatency=0ms

rule    	ms	#entms	entms/ms
RuleOwlEquivalentProperty	157	0	0
RuleRdf01	110	43	0
RuleRdfs02	8859	395958	44
RuleRdfs03	7125	395958	55
RuleRdfs08	250	41631	166
RuleRdfs09	11406	41759	3
RuleRdfs10	219	41631	190
RuleRdfs11	240719	3951672	16
totals: elapsed=268845, nadded=3951672, numComputed=4868778, added/sec=14698, computed/sec=18109

Computed closure in 301500ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=1249


============================================================


nciOncology.owl, no closure.

ids: #entries(est)=289871
SPO: #entries(est)=464993
POS: #entries(est)=464993
OSP: #entries(est)=464993
just: #entries(est)=0

!!!Note: be careful to choose the line that reports after the commit on the store!!!

local, unisolated:

run 1: Loaded 1 resources: 464841 stmts added in 23.656 secs, rate= 19650, commitLatency=172ms
run 2: Loaded 1 resources: 464841 stmts added in 24.094 secs, rate= 19292, commitLatency=156ms
run 3: Loaded 1 resources: 464841 stmts added in 24.328 secs, rate= 19107, commitLatency=235ms (after refactor for procedures)
(Computed closure in 141047ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=2671)

local, isolated:

run 1: Loaded 1 resources: 464841 stmts added in 26.735 secs, rate= 17386, commitLatency=438ms
run 2: Loaded 1 resources: 464841 stmts added in 25.719 secs, rate= 18073, commitLatency=297ms

embedded data service:
run 1: Loaded 1 resources: 464841 stmts added in 27.532 secs, rate= 16883, commitLatency=0ms (SPOArrayIterator)
(Computed closure in 375953ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=1002)
run 2: Loaded 1 resources: 464841 stmts added in 27.016 secs, rate= 17206, commitLatency=0ms (SPOIterator - no mem cap)
(Computed closure in 482453ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=781)
ren 3: Loaded 1 resources: 464841 stmts added in 27.485 secs, rate= 16912, commitLatency=0ms (SPOIterator - no mem cap)
(Computed closure in 436266ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=863)

embedded federation:

run 1: Loaded 1 resources: 464841 stmts added in 32.313 secs, rate= 14385, commitLatency=31ms

jini federation:

run 1: Loaded 1 resources: 464841 stmts added in 57.204 secs, rate= 8126, commitLatency=16ms
run 2: Loaded 1 resources: 464841 stmts added in 49.172 secs, rate= 9453, commitLatency=16ms

(done) Report more data about the scale-out indices, including the #of
partitions, where each partition is located, and the size on disk on
the partition (the btrees on the journal are conflated so the journal
space needs to be factored out but we can report the #of entries on
the journal and maybe even the bytes written on the journal by the
btree).  Call out the time spent on each index - we need better
counters to report that correctly, or even counters on the data
service.

The embedded federation has a substantial drop in performance when
compared to the local store using isolated indices (the data services
always use isolated indices so that is the point for comparison), but
the big drop is the jini federation - presumably that cost is entirely
attributable to the serialization overhead for RPCs.

Examine in more depth why the embedded federation is slower.  Try a
run on a larger data set and see if this is related to start up costs.

Thesaurus.owl: #terms=586945, #stmts=1,047,647

local, unisolated  : Loaded 1 resources: 1086012 stmts added in  59.609 secs, rate= 18218, commitLatency=312ms
                   : Loaded 1 resources: 1086012 stmts added in  57.765 secs, rate= 18800, commitLatency=328ms
                   : Loaded 1 resources: 1086012 stmts added in  58.313 secs, rate= 18623, commitLatency=312ms
		   : Loaded 1 resources: 1086012 stmts added in  58.687 secs, rate= 18505, commitLatency=312ms (keybuilder refactor)
local,   isolated  : Loaded 1 resources: 1086012 stmts added in  64.562 secs, rate= 16821, commitLatency=156ms
embedded federation: Loaded 1 resources: 1086012 stmts added in  76.969 secs, rate= 14109, commitLatency=31ms
                   : Loaded 1 resources: 1086012 stmts added in  76.938 secs, rate= 14115, commitLatency=16ms
jini federation    : Loaded 1 resources: 1086012 stmts added in 103.734 secs, rate= 10469, commitLatency=0ms
                   : Loaded 1 resources: 1086012 stmts added in 103.859 secs, rate= 10456, commitLatency=31ms

Results for a variety of serialization/compression approaches for the
various Procedures (IndexWriteProc, JustificationWriteProc, etc), but
NOT for serialization changes to the ResultSet (which is really only
used during inference).  In all cases these results are obtained for
the jini federation since that is the only case where we are forced to
serialize the data in a Procedure or a ResultSet for RPC.

NoCompression.  This serializes each key and value as a full length
byte[].

   Loaded 1 resources: 1086012 stmts added in 107.922 secs, rate= 10062, commitLatency=0ms
   Loaded 1 resources: 1086012 stmts added in 105.531 secs, rate= 10290, commitLatency=16ms

NoCompression, but writing on a DataOutputBuffer and then copying the
results to the output stream (see if this case improves if we reuse
the buffer for each request or using a thread-local variable):

   Loaded 1 resources: 1086012 stmts added in 149.484 secs, rate= 7265, commitLatency=16ms

BTreeCompression.  This uses prefix compression on the keys and simple
serialization of the values.

   Loaded 1 resources: 1086012 stmts added in 103.203 secs, rate= 10523, commitLatency=16ms

FastRDFCompression

   Loaded 1 resources: 1086012 stmts added in 102.109 secs, rate= 10635, commitLatency=16ms
   Loaded 1 resources: 1086012 stmts added in  99.75  secs, rate= 10887, commitLatency=16ms (NIO)
   Loaded 1 resources: 1086012 stmts added in  99.313 secs, rate= 10935, commitLatency=15ms (NIO)

The "FastRDF" approach is probably as good as I can make it for the
statement indices.  It performs only marginally better than the no
compression approach.

Perhaps the additional overhead is a mixture of:

 - de-serialization to support RPC;
 - the mechanisms of RPC (client, server, protocol, network)
 - the added burden on the heap

NIO for the RPC protocol appears to help a bit, but it runs out of
memory in the test suite (this shows up as an NPE in ByteBuffer).


Concurrent load rates:

Explore interaction of the group commit policy.  If we check point vs
commit vs do not wait around then how does that effect the
throughput!!!

Note: smaller buffer sizes (1000 statements) makes the total run much
slower.  Try this with more threads, but we will probably have to wait
on the group commit so that won't help with the current policy.

Note: larger buffer sizes will cap out since there is only so much
data in the LUBM files.

U10

embedded data service:

Finished: #loaded=189 files in 96015 ms, #stmts=1272577, rate=13253.0
(#threads=3, largestPoolSize=3, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 73797 ms, #stmts=1272577, rate=17244.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 85625 ms, #stmts=1272577, rate=14862.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 78750 ms, #stmts=1272577, rate=16159.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 73203 ms, #stmts=1272577, rate=17384.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 63172 ms, #stmts=1272577, rate=20144.0
(#threads=20, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=189, #ok=189, #err=0)

All done: #loaded=189 files in 59734 ms, #stmts=1272577, rate=21304.0
(#threads=20, class=LocalTripleStoreWithEmbeddedDataService,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=189,
#ok=189, #err=0)

embedded federation:

Finished: #loaded=189 files in 191828 ms, #stmts=1272577, rate=6633.0
(#threads=1, largestPoolSize=1, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 122343 ms, #stmts=1272577, rate=10401.0
(#threads=3, largestPoolSize=3, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 282140 ms, #stmts=1272577, rate=4510.0
(#threads=3, largestPoolSize=3, bufferCapacity=1000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 90860 ms, #stmts=1272577, rate=14005.0
(#threads=10, largestPoolSize=10, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 85735 ms, #stmts=1272577, rate=14843.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 88453 ms, #stmts=1272577, rate=14387.0
(#threads=20, largestPoolSize=20, bufferCapacity=20000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 87359 ms, #stmts=1272577, rate=14567.0
(#threads=30, largestPoolSize=30, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 105203 ms, #stmts=1272577, rate=12096.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

All done: #loaded=189 files in 106109 ms, #stmts=1272577, rate=11993.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

   disk: 1,230,029,630 {osp,spo,terms} + 51,870,457 {ids,pos}

Alternative index allocation: 

   Note: This case appears to be much more efficient in term and
   space, at least for the embedded federation:

   disk: 80,506,107 {terms,spo} + 90,515,091 {ids,pos,osp}

   All done: #loaded=189 files in 88016 ms, #stmts=1272577,
   rate=14458.0 (#threads=20, class=ScaleOutTripleStore,
   largestPoolSize=20, bufferCapacity=100000, autoFlush=false,
   #done=189, #ok=189, #err=0)

jini federation:

Finished: #loaded=189 files in 392078 ms, #stmts=1272578, rate=3245.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 371297 ms, #stmts=1272582, rate=3427.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 82328 ms, #stmts=1272577, rate=15457.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

    Note: This is an extremely odd result.  It was obtained by running
    immediately after the previous jini federation run.  Overall, jini
    seems very sensitive to initial conditions.  Perhaps this is
    related to memory limits on the laptop platform?  Often the jini
    run appears to be very nearly single threaded.

All done: #loaded=189 files in 241672 ms, #terms=314871,
#stmts=1272577, rate=5265.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=189,
#ok=189, #err=0)

server1: All done: #loaded=190 files in 74049 ms, #terms=314871,
#stmts=1272577, rate=17185.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=190,
#ok=189, #err=1)

server1: All done: #loaded=190 files in 76956 ms, #terms=314871,
#stmts=1272577, rate=16536.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=190,
#ok=189, #err=1)

   disk: 90,926,328 {terms,spo} + 90,926,328 {ids,pos,osp}

server1: All done: #loaded=2008 files in 739904 ms, #terms=3301736,
#stmts=13405383, rate=18117.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false,
#done=2008, #ok=2007, #err=1) (U100 is 13M triples)

   disk: 1,110,058,584 {terms,spo} + 1,071,640,537 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         140.096 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         112.644 s (pause 15.700 s)

server1: #loaded=20022 files in 11419382 ms, #terms=32885169,
#stmts=133573856, rate=11697.0 (#threads=20,
class=ScaleOutTripleStore, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=20022, #ok=20020, #err=2)

   disk: 14,110,887,061 {terms,spo} + 12,039,810,264 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         1319.038 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         693.036 s (pause 103.692 s)

server1: All done: #loaded=20022 files in 11633794 ms, #terms=32885169,
#stmts=133573856, rate=11481.0 (#threads=20,
class=ScaleOutTripleStore, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=20022, #ok=20020, #err=2)

   disk: 14,093,839,353 {terms,spo} + 12,038,914,891 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         1279.318 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         666.388 s (pause 100.395 s)

*** Jini tuning:

    - Server runs:

      - Since jini is so fast on the server, try to get asynchronous
        writes to disk working in DiskOnlyStrategy - it might have a
        big impact since we loose all concurrency when a write to disk
        occurs (however, disk writes will always be at group commits
        if the write cache is large enough so this might have NO
        impact).

      - Try U10000 reading the data from NAS with 2 clients, 10
        threads each and 2 servers.  See if scale-out holds as we
        increase the data size.  The point of comparison is the 1B run
        that we did on server2 (single host, non-scale-out
        architecture, non-concurrent load).

	- I am not seeing the 2nd data service on the current U10000
          run.  That is super weird. 

        - Make sure that yum-updatesd does not run on the servers.  It
          absorbs an entire CPU for quite a while.

        - Try running a thread that reads the file names and building
          up a (blocking) queue of tasks to be started.  Reading all
          the filenames and creating LoadTasks for each requires too
          much startup time and too much heap!

	- Make sure that each process writes on its own nohup_xxx.out
          file.  Hum - there is no way to do that.  Try creating a
          command group using (...) and running that group with nohup
          - probably won't work either.  How about run each in its own
          subdirectory?  Could work, but need to fiddle with the jini
          config and CLASSPATH.

      - Try with dynamically determined index partitions.  We can
        start with either single or double host placement of the
        initial index partitions.  Watch the load balancer and see how
        host utilization and service response time change as the run
        progresses, for different #of client threads, and as index
        splits occur, and as index moves occur.  Delay start of some
        data services, either on each host or on one of the hosts and
        then see how the load changes once we start additional data
        services.  Observe the metadata service response time and
        verify that it does not become a bottleneck since the current
        implementation is NOT caching.

      - Run single client with the metadata service (its lightly
        loaded) and one data service on one server and the other data
        service on another server.

	- Move the client and  metadata server to server3, running the
          data services on server1 and server2.

        - Have a client on each machine connect to the same federation
          (using hash(filename) MOD 2) to select the files to be
          loaded (distributed clients doing a concurrent batch load).
          The source data files will have to reside on NAS or a NSF
          mount or be pre-allocated to the different servers.

      - try TestTripleStoreLoadRateLocalConcurrent on the server to
        get a sense of the performance comparison between jini and an
        embedded data service when both use concurrent data load.

      - we do not appear to be memory capped on the server on U10.  On
        U100 we are using 60% of the RAM on the server (2.2G).

      - examine performance logs to see IO, CPU, etc. rates over time.

      - try larger loads (U100, U1000)

    - Tune indices

      - The ids index should benefit from value compression since the
        values are the serialized terms.  This will require custom
        code to break the values into symbols and then use huffman
        encoding.  Alternatively, simply treat each value as a symbol
        and code from that (assuming that value reuse is common - if
        not then at least URIs can be broken down into common
        symbols).

	Do not store bnodes in the id:term index.

      - The terms (term:id) index is on the order of 5x larger than
        the ids (id:term) index.  Presumably this is because updates
        are distributed more or less randomly across the terms index
        as new terms become defined but are strictly append only for
        the ids index since new ids are always larger than old ids.
	
         - A larger branching factor may benefit the ids index.

	 - A compacting merge of the terms index should greatly reduce
           its size.

	 - Nearly ALL _read_ time between the SPO and TERMS index is
           reading the TERMS index (99%).

	 - Nearly ALL _write_ time between the SPO and the TERMS index
           is writing the SPO index (99%).  Statements are more likely
           to be distinct than terms, so it makes sense that we write
           on the statement index more often.  However, note that this
           is true even though the TERMS index is 3x larger than the
           SPO index.

    - BTree

      - bug sometimes demonstrated by com.bigdata.service.StressTestConcurrent_stressTest1.  The parent
        of the split is being marked as "clean" when the code assumes that it should be dirty. This sort
        of thing tends to involve touches driving evictions resulting in a node asynchronously being made
        persistent (and hence not dirty).  Verify that there are no concurrent readers executing by mistake
        against the live index (e.g., read_committed reads must not read on the live index). This could be
        related to the change in the group commit policy as well since that changes when we record a commit
        point for an index.
      
		Caused by: java.lang.AssertionError
			at com.bigdata.btree.Node.insertChild(Node.java:1515)
			at com.bigdata.btree.Leaf.split(Leaf.java:665)
			at com.bigdata.btree.Leaf.insert(Leaf.java:472)
			at com.bigdata.btree.Node.insert(Node.java:637)
			at com.bigdata.btree.AbstractBTree.insert(AbstractBTree.java:995)
			at com.bigdata.btree.AbstractBTree.insert(AbstractBTree.java:940)
			at com.bigdata.btree.AbstractBTree.rangeCopy(AbstractBTree.java:1477)
			at com.bigdata.resources.SplitIndexPartitionTask$UpdateSplitIndexPartition.doTask(SplitIndexPartitionTask.java:534)
			at com.bigdata.journal.AbstractTask$InnerWriteServiceCallable.call(AbstractTask.java:1035)
			... 9 more
			
     - Support copy in/out of keys and vals in lookup(),
           insert(), remove(), and rangeIterator so that we can (a) be
           more efficient in handling keys and vals by copying; (b)
           handle keys and vals that are byte aligned or bit aligned
           in the node or leaf; (c) reduce GC by converting to a
           compacting record for the node/leaf; and (d) expose the
           version counter and deletion marker for fused views of
           indices with isolation.

	 - done. change version counters to long (commit timestamps).

     - Turn off sendVals for rangeIterators if we recognize the value
       serialized as the NoDataSerializer?

    - Tune serialization:

	 - Test use of FastRDFValueCompression to write the values for
	   the statement indices.

	 - Look at how to do prefix coding using the mg4j and fast
           classes.  That could be used as the default key
           serialization and only overridden in special cases like RDF
           where we can do even better with less effort.
           
	   (old summary, revisit now since significant code changes):
	   the "fast" rdf serializers definately write (and read) less
	   data resulting in a smaller store and less IO.  However,
	   they are also without a doubt SLOWER.

	   It is hard to tell to what extend the throughput drop is
	   due to the work required to compute the compressed codes
	   and to what extent it is due to the additional allocation
	   and copying that we need to do to align the IDataSerializer
	   API with the IKeySerializer API, with the IValueSerializer
	   API, and with the use of the ImmutableKeyBuffer class in
	   the BTree.

	   Those API misalignments need to be reduced before we can
	   figure out whether the "fast" rdf serialization is faster
	   overall.  (There is SOME evidence that the API misalignment
	   is causing problems since only using the "fast" value
	   serializer is slower than using the byte[] value serializer
	   and there is very little "computation" to be done -
	   especially when compared to the "fast" key serializer which
	   computes a code for each long term identifier used in the
	   leaf.)

	   Also, note that most of the time is on the SPO and JUST
	   indices during closure (I would expect that) and on the
	   statement indices (rather than the lexicon) during load
	   (which is NOT what I would have expected for load).

	   I should also try some other data sets, with more memory
	   free on the machine, and on a server platform with more
	   CPUs and memory.

	   nciOncology:

	   fast 1 : Loaded 1 resources: 464841 stmts added in 31.922 secs, rate= 14561, commitLatency=203ms; bytesWritten=?
	   fast 1 : Computed closure in 289703ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=1300; bytesWritten=?

	   fast 2 : Loaded 1 resources: 464841 stmts added in 26.094 secs, rate= 17814, commitLatency=172ms, bytesWritten=95,703,361, bytesRead=42,603,290
	   fast 2 : Computed closure in 135313ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=2785, bytesWritten=581,338,696, bytesRead=742,823,674

	   fast 3 : Loaded 1 resources: 464841 stmts added in 26.265 secs, rate= 17698, commitLatency=172ms
	   fast 3 : Computed closure in 136875ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=2753

	   kbs  1 : Loaded 1 resources: 464841 stmts added in 28.828 secs, rate= 16124, commitLatency=250ms, bytesWritten=109,833,180
	   kbs  1 : Computed closure in 124860ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=3018; bytesWritten=??? (not reported)

	   kbs  2 : Loaded 1 resources: 464841 stmts added in 23.594 secs, rate= 19701, commitLatency=250ms, bytesWritten=109,829,813, bytesRead=44,865,353
	   kbs  2 : Computed closure in 123500ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=3051, bytesWritten=653,911,378, bytesRead=862,234,149


Loaded 1 resources: 464841 stmts added in 25.218 secs, rate= 18432, commitLatency=391ms
Computed closure in 125296ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=3007

    - Distributed file repository

         - handle overflow of blocks to the index segments during MOVE

	 - provide streaming socket api on data service for reading
           blocks (low level in the DiskOnlyStrategy - if in the write cache
           then return directly else return buffered input stream reading on
           the disk file and interrupt if journal is closed).

	 - range delete

	 - logical row scan for headers of documents in a key range.

    - Map/Reduce demo jobs.

      - Download, prepare, extract.

    - Tune network IO

      - AddTerms and AddIds

      - Modify the procedure logic to abstract a 'next key/val'
        iterator using a shared buffer for de-compression in order to
        minimize heap churn on the data server.

      - huffman encoding is appropriate for network IO, but hu-tucker
        is not required since we have to decompress keys to get them
        inserted into the btree.

      - tokenization needs to be specified for RDF Value types for the
        purposes of compression.  In fact, we are guarenteed that
        values are NOT duplicated in a given batch so tokenization
        needs to uncover common symbols.  This is easy for URIs but
        less so for literals and impossible for BNodes (which do not
        really need to be in the lexicon anyway).

    - Try jini federation using only the terms index to assign
      consistent term identifiers, bulk loading into local SPO-only
      indices, and then range partitioning the indices into global
      SPO, POS, and OSP orders and bulk loading the global statement
      indices.  The data loader should be concurrent and a filter
      should be applied such that each "host" loads only the files
      that hash MOD N to that host.  (note that only AddTerms and
      AddIds go across the network API in this case.)

    - The temp triple store supports concurrent read only but not
      concurrent write, so it is not appropriate for a concurrent bulk
      loader.

Short term tasks:

   - (*) Builds and releases.
   
      - Change over to subversion so that the edit trail does not get
        lost.

      - Maven 2.x build
      
         - Start doing snapshot releases.

	 - Start periodic project documentation builds, perhaps on SF.

   - Counters

     - Compute average response time and throughput as 1/average
       response time.  And measure on the client as well so that we
       can compute the RMI overhead.

     - HTTP interface for examining aggregated statistics at the load
       balancer.

     - When aggregating data to the load balancer it will be
       relatively common for some samples to arive "late", especially
       when the reporting period falls right around the minute mark.
       These samples SHOULD be let in if we do not already have a
       sample for that period since otherwise we could see a lot of
       dropped samples.

     - Add a moving average computation to History

     - Add counters for #of index partition split, move (on/off), and
       join operations (OverflowManager).  Also report #of errors
       during asynchronous overflow processing.

     - Add counter to the write service that reports the #of tasks
       which have their locks and are actually doing their work
       concurrently (LockManager defines such a counter but we need
       its moving average not the instantaneous value).  This is the
       real concurrency of the tasks.  The #of active tasks in the
       write service is a red herring since those tasks could be
       waiting for their locks.

       Done, but report the averageQueueLength for both tasks holding
       their locks and for all active tasks in the pool since both are
       interesting data. For example, the averageQueueLength below is
       .25 but the activeTaskSetSize is 20.  For this example this was
       because there were two indices and many operations were forced
       to be sequential.  Even though the commit group size was large
       (committedTaskCount/groupCommitCount) =~ 20, the parallelism
       was poor (average of .25 tasks executing in parallel!)

       averageQueueLength=0.25841155522560005
       (activeCountAverage=0.217179869184,queueSizeAverage=0.04123168604160004),
       nsamples=20

       WriteExecutorService{ paused=false, nrunning=8,
       concurrentTaskCount=1, activeTaskSetSize=20, nwrites=12,
       groupCommitFlag=true, abortFlag=false,
       lockHeldByCurrentThread=false, lockHoldCount=0,
       lockQueueLength=0, activeCount=20, queueSize=0, poolSize=200,
       largestPoolSize=200, maxPoolSize=200, maxRunning=20,
       maxCommitLatency=187, maxLatencyUntilCommit=125,
       groupCommitCount=109, abortCount=0, failedTaskCount=0,
       successTaskCount=2152, committedTaskCount=2140,
       overflowCount=0}

   - LocalTripleStoreWithEmbeddedDataService
     
     - Optimized JOIN that assumes that all indices are local within
       the data service and reads locally on both access paths.  This
       would probably be implemented as a DataServiceIndexProcedure
       and it would need to declare access to the indices being used
       for the left and right hand sides of the join.

   - ScaleOutTripleStore

     - Optimized JOIN for the scale-out triple store.  It needs to
       block up a set of right hand tuples that will be joined against
       data on a given data service and then send those tuples to that
       data service, recieving the results in return. It will also
       have to handle stale locators if the join is running in an
       read-committed mode, but not if it is transactional or a
       historical read.

       Inference is slow due to a large #of small join results.
       Parallel sub-query is probably the way to beat that.  After
       tuning, compare to the purely local unconcurrent line.

       Consider batching a set of rangeQueries together in a single
       operation vs parallel submits.

     - The distinct term scan (really, the prefix scan) needs to be
       optimized for the data service and the federation use cases.
       This also effects the sparse row store.  Also, the sparse row
       store needs to use an extended split handler that always
       chooses a split point which is on a logical row boundary.

     - (*) Need ability to request a rangeIterator that reads in
       reverse key order and the ability to visit the prior or next
       key.  This requirement arises in particular for the bigdata
       repository which is currently using the ILinearList API for the
       AtomicAppend.  This will also help us to replace the
       requirement for the ILinearList API in the metadata index.
       
       The change needs to occur at several levels and should include
       a prior() method on ITupleIterator and the ability to acquire
       an ITupleIterator for ITuple returned from lookup(), insert(),
       etc. so that it can be turned into an iterator for prior/next visitation.

       This is a good time to do efficient prior/next leaf operations
       for the IndexSegment and to make that more efficient for the
       BTree as well.

   - * The consistentRead option used by the
     PartitionedIndexRangeIterator needs to use the most recent commit
     time for the federation.

     Currently it uses the lastCommitTime for the first index
     partition which it scans.  However, it is quite possible that
     there have been writes on other index partitions since which
     would not be reflected in BTree#lastCommitTime.  The
     consistentRead would therefore reflect an older history rather
     than the most recent writes when it moved onto the other index
     partitions.

     In order to fix this the data services MUST discover and use a
     centralized timeservice.  This can be essentially a stripped down
     centralized transaction manager.  The data services will obtain
     their commitTime timestamps from this centralized time service
     and MUST also notify the centralized timeservice once they have
     successfully committed.  The PartitionedRangeIterator will then
     query the centralized time service for the last commit time for
     the federation as a whole and use that as the time for a
     consistentRead operation.

   - OverflowManager

      - Could optionally convert from  a fully-buffered to a disk-only
        store  in  order to  reduce  the  memory  footprint for  fully
        buffered  stores,  but in  that  case  this conversion  should
        happen once asynchronous overflow handling was complete.

   - StoreManager / IndexManager

      - Modify  LRU  to  purge  entries  older than  a  specified  age
        (including an asych  daemon thread to make sure  that they get
        purged even if the LRU is not being touched).  Do this for the
        index segment cache in the IndexManager as well.

      - (*) Better concurrency for openIndex, openStore, getJournal,
        and getIndexOnStore

   - LockManager

       - Use a WeakValueCache to purge unused resources.  The size of
         its internal map from resource name to resource queue will
         grow without bound on a data service as index partitions are
         split and moved around.  There are notes on this issue in the
         LockManager class.

   - DiskOnlyStrategy
   
     - Lazy creation of the backing file.

     - An LRU read cache for records.

       This could be a big win for the DiskOnlyStrategy.  Either make
       this its own layer that can be interposed between the journal
       and the DiskOnlyStrategy or add directly to the
       DiskOnlyStrategy since a read cache is not required for the
       fully buffered modes. Regardless, allow configuration of the
       cache size.

       Also, efficient nextLeaf could improve read performance by
       reducing node reads.

     - CounterSets

       - Add counters designed to give insight into whether the write
         cache tends to full up completely or only partly before the
         next group command and the #of bytes that tend to be written.
         What I want to understand is whether the cache is too large
         and whether an asynchronous of the cache to the disk would be
         a benefit.

	 Note that writes which would exceed the remaining cache size
         cause the existing cache to be flushed while writes that
         exceed the cache capacity are written directly to the disk -
         the cache itself is always dense in terms of the bytes
         written on the address space.

   - Done. Full text indexing for KB.

       - (*) Analyzers are not thread-safe.

       - Try out an mg4j integration for an alternative text indexer
         and search.

   - Try front compression for nodes and leaf keys using
     it.unimi.dsi.fastutil.bytes#ByteArrayFrontCodedList

   - *** Batch API for extractor, allowing runs directly against the KB.

   - ** Change tx timestamps to negative and use positive timestamps for
     historical reads.  Changes to AbstractTask, ITx,
     ITransactionManager, StoreFileManager, IsolationEnum, and the
     post-processing tasks.  This will greatly simplify thinking about
     historical read operations since they will simply use the actual
     commit time while transactions will use a free (-timestamp) value
     selected by the transaction manager.

   - ** Quad store.

    - 3+1 or 4+1 data models.  It IS possible to do statement
      identifiers using bnodes on a quad store, but we lose the
      opportunity to use the context position for virtual graph
      partition (often used to group one or more data sources
      together), which is also a key feature for federation

   - ** Sesame 2 TCL (integration tests)

   - SemTech08 submission?

   - ** correctness testing for scale-out with index partition split,
        move, and join.  See AbstractMRMWTestCase and
        StressTestConcurrent.  The latter should be modified to use
        ground truth indices in a TemporaryStore and we could write
        other classes for performance testing on a cluster.

   - streaming io for block read/write.  note that asynch IO for the
     disk only strategy will not have an impact if we are doing
     commits whose size fits within a single write cache (10M by
     default).

   - write performance test drivers and run on cluster.

      - rdf concurrent data load.

      - rdf concurrent query (rdf lubm is not designed to test with
        concurrent loads).

      - bigdata file system workload

      - the memory demand jumps up during overflow handling.  maybe I
        need to throttle the thread pool more on the data service?

============================================================

- ERROR: 39296     pool-1-thread-142 com.bigdata.resources.PostProcessOldJournalTask.call(PostProcessOldJournalTask.java:1436): java.lang.AssertionError: ndone=8, but #used=9
java.lang.AssertionError: ndone=8, but #used=9
	at com.bigdata.resources.PostProcessOldJournalTask.chooseTasks(PostProcessOldJournalTask.java:1299)
	at com.bigdata.resources.PostProcessOldJournalTask.call(PostProcessOldJournalTask.java:1344)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:123)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)
	at java.lang.Thread.run()V(Unknown Source)
	
============================================================

TestConcurrentJournal#test_concurrentReadersAreOk - might have been write cache flush problem?

java.lang.AssertionError: limit=0, byteCount(addr)=392, addr={nbytes=392,offset=486601}
	at com.bigdata.btree.AbstractBTree.readNodeOrLeaf(AbstractBTree.java:2056)
	at com.bigdata.btree.Node.getChild(Node.java:2110)
	at com.bigdata.btree.ChildIterator.next(ChildIterator.java:163)
	at com.bigdata.btree.ChildIterator.next(ChildIterator.java:1)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:59)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Appenderator.hasNext(Appenderator.java:52)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:56)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Appenderator.hasNext(Appenderator.java:52)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:56)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Appenderator.hasNext(Appenderator.java:52)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:58)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at com.bigdata.btree.AbstractNode$PostOrderEntryIterator.hasNext(AbstractNode.java:669)
	at com.bigdata.btree.ReadOnlyEntryIterator.hasNext(ReadOnlyEntryIterator.java:58)
	at com.bigdata.journal.TestConcurrentJournal$1ReadTask.doTask(TestConcurrentJournal.java:1701)
	at com.bigdata.journal.AbstractTask.call2(AbstractTask.java:684)
	at com.bigdata.journal.AbstractTask.call(AbstractTask.java:603)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:123)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)
	at java.lang.Thread.run(Thread.java:595)


============================================================

Counters error during concurrent data loader run.

WARN : 332347   pool-1-thread-7 com.bigdata.service.DataService$ReportTask.run(DataService.java:1070): Problem in report task?
java.lang.AssertionError: lastModified=0
	at com.bigdata.counters.History.add(History.java:547)
	at com.bigdata.counters.HistoryInstrument.add(HistoryInstrument.java:111)
	at com.bigdata.counters.HistoryInstrument.setValue(HistoryInstrument.java:117)
	at com.bigdata.counters.Counter.setValue(Counter.java:128)
	at com.bigdata.counters.CounterSet$MyHandler.endElement(CounterSet.java:1248)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.endElement(AbstractSAXParser.java:633)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanEndElement(XMLNSDocumentScannerImpl.java:719)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(XMLDocumentFragmentScannerImpl.java:1685)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:368)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:834)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:764)


Fixed concurrency in the text indexer (problem with lucene) and
modified so that it can be used with the LocalTripleStore as well (its
turned on by default).

nciOncology.owl, embedded federation.

INFO : 31844   Main Thread com.bigdata.rdf.rio.BasicRioLoader.loadRdf(BasicRioLoader.java:194): parse complete: elapsed=28187ms, toldTriples=464841, tps=16491

INFO : 882875   Main Thread com.bigdata.rdf.store.DataLoader.loadData(DataLoader.java:517): Loaded 1 resources: 464841 stmts added in 28.265 secs, rate= 528, commitLatency=0ms
rule    	ms	#entms	entms/ms
RuleFastClosure13	15	0	0
RuleOwlEquivalentProperty	16	0	0
RuleRdfs02	5890	395806	67
RuleRdfs03	3297	395806	120
RuleRdfs08	31	41618	1342
RuleRdfs09	12109	41724	3
RuleRdfs10	110	41618	378
RuleRdfs11	720126	5324314	7
totals: elapsed=741594, nadded=376849, numComputed=6241034, added/sec=508, computed/sec=8415

Note: this appears to be incremental TM rather than database at once closure.

========================================

Modified to use database at once closure.

nciOncology.owl, embedded federation.

INFO : 36188   Main Thread com.bigdata.rdf.store.DataLoader.loadData2(DataLoader.java:628): 464841 stmts added in 32.109 secs, rate= 14476, commitLatency=0ms

rule    	ms	#entms	entms/ms
RuleOwlEquivalentProperty	157	0	0
RuleRdf01	110	43	0
RuleRdfs02	8859	395958	44
RuleRdfs03	7125	395958	55
RuleRdfs08	250	41631	166
RuleRdfs09	11406	41759	3
RuleRdfs10	219	41631	190
RuleRdfs11	240719	3951672	16
totals: elapsed=268845, nadded=3951672, numComputed=4868778, added/sec=14698, computed/sec=18109

Computed closure in 301500ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=1249


============================================================


nciOncology.owl, no closure.

ids: #entries(est)=289871
SPO: #entries(est)=464993
POS: #entries(est)=464993
OSP: #entries(est)=464993
just: #entries(est)=0

!!!Note: be careful to choose the line that reports after the commit on the store!!!

local, unisolated:

run 1: Loaded 1 resources: 464841 stmts added in 23.656 secs, rate= 19650, commitLatency=172ms
run 2: Loaded 1 resources: 464841 stmts added in 24.094 secs, rate= 19292, commitLatency=156ms
run 3: Loaded 1 resources: 464841 stmts added in 24.328 secs, rate= 19107, commitLatency=235ms (after refactor for procedures)
(Computed closure in 141047ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=2671)

local, isolated:

run 1: Loaded 1 resources: 464841 stmts added in 26.735 secs, rate= 17386, commitLatency=438ms
run 2: Loaded 1 resources: 464841 stmts added in 25.719 secs, rate= 18073, commitLatency=297ms

embedded data service:
run 1: Loaded 1 resources: 464841 stmts added in 27.532 secs, rate= 16883, commitLatency=0ms (SPOArrayIterator)
(Computed closure in 375953ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=1002)
run 2: Loaded 1 resources: 464841 stmts added in 27.016 secs, rate= 17206, commitLatency=0ms (SPOIterator - no mem cap)
(Computed closure in 482453ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=781)
ren 3: Loaded 1 resources: 464841 stmts added in 27.485 secs, rate= 16912, commitLatency=0ms (SPOIterator - no mem cap)
(Computed closure in 436266ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=863)

embedded federation:

run 1: Loaded 1 resources: 464841 stmts added in 32.313 secs, rate= 14385, commitLatency=31ms

jini federation:

run 1: Loaded 1 resources: 464841 stmts added in 57.204 secs, rate= 8126, commitLatency=16ms
run 2: Loaded 1 resources: 464841 stmts added in 49.172 secs, rate= 9453, commitLatency=16ms

(done) Report more data about the scale-out indices, including the #of
partitions, where each partition is located, and the size on disk on
the partition (the btrees on the journal are conflated so the journal
space needs to be factored out but we can report the #of entries on
the journal and maybe even the bytes written on the journal by the
btree).  Call out the time spent on each index - we need better
counters to report that correctly, or even counters on the data
service.

The embedded federation has a substantial drop in performance when
compared to the local store using isolated indices (the data services
always use isolated indices so that is the point for comparison), but
the big drop is the jini federation - presumably that cost is entirely
attributable to the serialization overhead for RPCs.

Examine in more depth why the embedded federation is slower.  Try a
run on a larger data set and see if this is related to start up costs.

Thesaurus.owl: #terms=586945, #stmts=1,047,647

local, unisolated  : Loaded 1 resources: 1086012 stmts added in  59.609 secs, rate= 18218, commitLatency=312ms
                   : Loaded 1 resources: 1086012 stmts added in  57.765 secs, rate= 18800, commitLatency=328ms
                   : Loaded 1 resources: 1086012 stmts added in  58.313 secs, rate= 18623, commitLatency=312ms
		   : Loaded 1 resources: 1086012 stmts added in  58.687 secs, rate= 18505, commitLatency=312ms (keybuilder refactor)
local,   isolated  : Loaded 1 resources: 1086012 stmts added in  64.562 secs, rate= 16821, commitLatency=156ms
embedded federation: Loaded 1 resources: 1086012 stmts added in  76.969 secs, rate= 14109, commitLatency=31ms
                   : Loaded 1 resources: 1086012 stmts added in  76.938 secs, rate= 14115, commitLatency=16ms
jini federation    : Loaded 1 resources: 1086012 stmts added in 103.734 secs, rate= 10469, commitLatency=0ms
                   : Loaded 1 resources: 1086012 stmts added in 103.859 secs, rate= 10456, commitLatency=31ms

Results for a variety of serialization/compression approaches for the
various Procedures (IndexWriteProc, JustificationWriteProc, etc), but
NOT for serialization changes to the ResultSet (which is really only
used during inference).  In all cases these results are obtained for
the jini federation since that is the only case where we are forced to
serialize the data in a Procedure or a ResultSet for RPC.

NoCompression.  This serializes each key and value as a full length
byte[].

   Loaded 1 resources: 1086012 stmts added in 107.922 secs, rate= 10062, commitLatency=0ms
   Loaded 1 resources: 1086012 stmts added in 105.531 secs, rate= 10290, commitLatency=16ms

NoCompression, but writing on a DataOutputBuffer and then copying the
results to the output stream (see if this case improves if we reuse
the buffer for each request or using a thread-local variable):

   Loaded 1 resources: 1086012 stmts added in 149.484 secs, rate= 7265, commitLatency=16ms

BTreeCompression.  This uses prefix compression on the keys and simple
serialization of the values.

   Loaded 1 resources: 1086012 stmts added in 103.203 secs, rate= 10523, commitLatency=16ms

FastRDFCompression

   Loaded 1 resources: 1086012 stmts added in 102.109 secs, rate= 10635, commitLatency=16ms
   Loaded 1 resources: 1086012 stmts added in  99.75  secs, rate= 10887, commitLatency=16ms (NIO)
   Loaded 1 resources: 1086012 stmts added in  99.313 secs, rate= 10935, commitLatency=15ms (NIO)

The "FastRDF" approach is probably as good as I can make it for the
statement indices.  It performs only marginally better than the no
compression approach.

Perhaps the additional overhead is a mixture of:

 - de-serialization to support RPC;
 - the mechanisms of RPC (client, server, protocol, network)
 - the added burden on the heap

NIO for the RPC protocol appears to help a bit, but it runs out of
memory in the test suite (this shows up as an NPE in ByteBuffer).


Concurrent load rates:

Explore interaction of the group commit policy.  If we check point vs
commit vs do not wait around then how does that effect the
throughput!!!

Note: smaller buffer sizes (1000 statements) makes the total run much
slower.  Try this with more threads, but we will probably have to wait
on the group commit so that won't help with the current policy.

Note: larger buffer sizes will cap out since there is only so much
data in the LUBM files.

U10

embedded data service:

Finished: #loaded=189 files in 96015 ms, #stmts=1272577, rate=13253.0
(#threads=3, largestPoolSize=3, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 73797 ms, #stmts=1272577, rate=17244.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 85625 ms, #stmts=1272577, rate=14862.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 78750 ms, #stmts=1272577, rate=16159.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 73203 ms, #stmts=1272577, rate=17384.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 63172 ms, #stmts=1272577, rate=20144.0
(#threads=20, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=189, #ok=189, #err=0)

All done: #loaded=189 files in 59734 ms, #stmts=1272577, rate=21304.0
(#threads=20, class=LocalTripleStoreWithEmbeddedDataService,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=189,
#ok=189, #err=0)

embedded federation:

Finished: #loaded=189 files in 191828 ms, #stmts=1272577, rate=6633.0
(#threads=1, largestPoolSize=1, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 122343 ms, #stmts=1272577, rate=10401.0
(#threads=3, largestPoolSize=3, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 282140 ms, #stmts=1272577, rate=4510.0
(#threads=3, largestPoolSize=3, bufferCapacity=1000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 90860 ms, #stmts=1272577, rate=14005.0
(#threads=10, largestPoolSize=10, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 85735 ms, #stmts=1272577, rate=14843.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 88453 ms, #stmts=1272577, rate=14387.0
(#threads=20, largestPoolSize=20, bufferCapacity=20000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 87359 ms, #stmts=1272577, rate=14567.0
(#threads=30, largestPoolSize=30, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 105203 ms, #stmts=1272577, rate=12096.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

All done: #loaded=189 files in 106109 ms, #stmts=1272577, rate=11993.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

   disk: 1,230,029,630 {osp,spo,terms} + 51,870,457 {ids,pos}

Alternative index allocation: 

   Note: This case appears to be much more efficient in term and
   space, at least for the embedded federation:

   disk: 80,506,107 {terms,spo} + 90,515,091 {ids,pos,osp}

   All done: #loaded=189 files in 88016 ms, #stmts=1272577,
   rate=14458.0 (#threads=20, class=ScaleOutTripleStore,
   largestPoolSize=20, bufferCapacity=100000, autoFlush=false,
   #done=189, #ok=189, #err=0)

jini federation:

Finished: #loaded=189 files in 392078 ms, #stmts=1272578, rate=3245.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 371297 ms, #stmts=1272582, rate=3427.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 82328 ms, #stmts=1272577, rate=15457.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

    Note: This is an extremely odd result.  It was obtained by running
    immediately after the previous jini federation run.  Overall, jini
    seems very sensitive to initial conditions.  Perhaps this is
    related to memory limits on the laptop platform?  Often the jini
    run appears to be very nearly single threaded.

All done: #loaded=189 files in 241672 ms, #terms=314871,
#stmts=1272577, rate=5265.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=189,
#ok=189, #err=0)

server1: All done: #loaded=190 files in 74049 ms, #terms=314871,
#stmts=1272577, rate=17185.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=190,
#ok=189, #err=1)

server1: All done: #loaded=190 files in 76956 ms, #terms=314871,
#stmts=1272577, rate=16536.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=190,
#ok=189, #err=1)

   disk: 90,926,328 {terms,spo} + 90,926,328 {ids,pos,osp}

server1: All done: #loaded=2008 files in 739904 ms, #terms=3301736,
#stmts=13405383, rate=18117.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false,
#done=2008, #ok=2007, #err=1) (U100 is 13M triples)

   disk: 1,110,058,584 {terms,spo} + 1,071,640,537 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         140.096 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         112.644 s (pause 15.700 s)

server1: #loaded=20022 files in 11419382 ms, #terms=32885169,
#stmts=133573856, rate=11697.0 (#threads=20,
class=ScaleOutTripleStore, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=20022, #ok=20020, #err=2)

   disk: 14,110,887,061 {terms,spo} + 12,039,810,264 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         1319.038 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         693.036 s (pause 103.692 s)

server1: All done: #loaded=20022 files in 11633794 ms, #terms=32885169,
#stmts=133573856, rate=11481.0 (#threads=20,
class=ScaleOutTripleStore, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=20022, #ok=20020, #err=2)

   disk: 14,093,839,353 {terms,spo} + 12,038,914,891 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         1279.318 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         666.388 s (pause 100.395 s)

*** Jini tuning:

    - Server runs:

      - Since jini is so fast on the server, try to get asynchronous
        writes to disk working in DiskOnlyStrategy - it might have a
        big impact since we loose all concurrency when a write to disk
        occurs (however, disk writes will always be at group commits
        if the write cache is large enough so this might have NO
        impact).

      - Try U10000 reading the data from NAS with 2 clients, 10
        threads each and 2 servers.  See if scale-out holds as we
        increase the data size.  The point of comparison is the 1B run
        that we did on server2 (single host, non-scale-out
        architecture, non-concurrent load).

	- I am not seeing the 2nd data service on the current U10000
          run.  That is super weird. 

        - Make sure that yum-updatesd does not run on the servers.  It
          absorbs an entire CPU for quite a while.

	- Make sure that each process writes on its own nohup_xxx.out
          file.  Hum - there is no way to do that.  Try creating a
          command group using (...) and running that group with nohup
          - probably won't work either.  How about run each in its own
          subdirectory?  Could work, but need to fiddle with the jini
          config and CLASSPATH.

      - Try with dynamically determined index partitions.

        Start with either single or double host placement of the
        initial index partitions.

	Watch the load balancer and see how host utilization and
        service response time change as the run progresses, for
        different #of client threads, and as index splits occur, and
        as index moves occur.

	Delay start of some data services, either on each host or on
        one of the hosts and then see how the load changes once we
        start additional data services.

	Observe the metadata service response time and verify that it
        does not become a bottleneck since the current implementation
        is NOT caching.

      - Run single client with the metadata service (its lightly
        loaded) and one data service on one server and the other data
        service on another server.

	- Move the client and metadata server to server3, running the
          data services on server1 and server2.

        - Have a client on each machine connect to the same federation
          (using hash(filename) MOD 2) to select the files to be
          loaded (distributed clients doing a concurrent batch load).
          The source data files will have to reside on NAS or a NSF
          mount or be pre-allocated to the different servers.

    - Tune indices

      - The ids index should benefit from value compression since the
        values are the serialized terms.  This will require custom
        code to break the values into symbols and then use huffman
        encoding.  Alternatively, simply treat each value as a symbol
        and code from that (assuming that value reuse is common - if
        not then at least URIs can be broken down into common
        symbols).

	Done. Do not store bnodes in the id:term index.

      - The terms (term:id) index is on the order of 5x larger than
        the ids (id:term) index.  Presumably this is because updates
        are distributed more or less randomly across the terms index
        as new terms become defined but are strictly append only for
        the ids index since new ids are always larger than old ids.
	
         - A larger branching factor may benefit the ids index.

	 - A compacting merge of the terms index should greatly reduce
           its size.

	 - Nearly ALL _read_ time between the SPO and TERMS index is
           reading the TERMS index (99%).

	 - Nearly ALL _write_ time between the SPO and the TERMS index
           is writing the SPO index (99%).  Statements are more likely
           to be distinct than terms, so it makes sense that we write
           on the statement index more often.  However, note that this
           is true even though the TERMS index is 3x larger than the
           SPO index.

    - BTree

      - bug sometimes demonstrated by com.bigdata.service.StressTestConcurrent_stressTest1.  The parent
        of the split is being marked as "clean" when the code assumes that it should be dirty. This sort
        of thing tends to involve touches driving evictions resulting in a node asynchronously being made
        persistent (and hence not dirty).  Verify that there are no concurrent readers executing by mistake
        against the live index (e.g., read_committed reads must not read on the live index). This could be
        related to the change in the group commit policy as well since that changes when we record a commit
        point for an index.
      
		Caused by: java.lang.AssertionError
			at com.bigdata.btree.Node.insertChild(Node.java:1515)
			at com.bigdata.btree.Leaf.split(Leaf.java:665)
			at com.bigdata.btree.Leaf.insert(Leaf.java:472)
			at com.bigdata.btree.Node.insert(Node.java:637)
			at com.bigdata.btree.AbstractBTree.insert(AbstractBTree.java:995)
			at com.bigdata.btree.AbstractBTree.insert(AbstractBTree.java:940)
			at com.bigdata.btree.AbstractBTree.rangeCopy(AbstractBTree.java:1477)
			at com.bigdata.resources.SplitIndexPartitionTask$UpdateSplitIndexPartition.doTask(SplitIndexPartitionTask.java:534)
			at com.bigdata.journal.AbstractTask$InnerWriteServiceCallable.call(AbstractTask.java:1035)
			... 9 more
			
     - Support copy in/out of keys and vals in lookup(), insert(),
       remove(), and rangeIterator so that we can (a) be more
       efficient in handling keys and vals by copying; (b) handle keys
       and vals that are byte aligned or bit aligned in the node or
       leaf; (c) reduce GC by converting to a compacting record for
       the node/leaf; and (d) expose the version counter and deletion
       marker for fused views of indices with isolation.

     - Turn off sendVals for rangeIterators if we recognize the value
       serialized as the NoDataSerializer?

     - Change checksums to be at the store/record level.  Interpret
       the record length as having 2 additional bytes for read/write
       of the checksum.  Put it at the end of the record.
       Enable/disable at the store level.

       Add an option for read-back validation of writes?
       
       Add an option for a fully synchronized raw store interface on
       the Journal?

    - Distributed file repository

         - handle overflow of blocks to the index segments during MOVE

	 - provide streaming socket api on data service for reading
           blocks (low level in the DiskOnlyStrategy - if in the write cache
           then return directly else return buffered input stream reading on
           the disk file and interrupt if journal is closed).

	 - range delete

	 - logical row scan for headers of documents in a key range.

    - Map/Reduce demo jobs.

      - Download, prepare, extract.

    - Tune network IO

      - Modify the procedure logic to abstract a 'next key/val'
        iterator using a shared buffer for de-compression in order to
        minimize heap churn on the data server.

      - huffman encoding is appropriate for network IO, but hu-tucker
        is not required since we have to decompress keys to get them
        inserted into the btree.

      - tokenization needs to be specified for RDF Value types for the
        purposes of compression.  In fact, we are guarenteed that
        values are NOT duplicated in a given batch so tokenization
        needs to uncover common symbols.  This is easy for URIs but
        less so for literals and impossible for BNodes (which do not
        really need to be in the lexicon anyway).

    - Try jini federation using only the terms index to assign
      consistent term identifiers, bulk loading into local SPO-only
      indices, and then range partitioning the indices into global
      SPO, POS, and OSP orders and bulk loading the global statement
      indices.  The data loader should be concurrent and a filter
      should be applied such that each "host" loads only the files
      that hash MOD N to that host.  (note that only AddTerms and
      AddIds go across the network API in this case.)

    - The temp triple store supports concurrent read only but not
      concurrent write, so it is not appropriate for a concurrent bulk
      loader.

    - An extended transaction model can be used for truth maintenance.
      The focus store is built up within isolated indices (that do not
      actually correspond to persistent indices, they only exist on
      the per-tx per-dataservice TemporaryStore).  The application can
      simply combine sets of assertions or retractions within a single
      transaction.  Either the application or an extension of the
      transaction manager MUST serialize the commits.  Within the
      commit processing, first do retractions then do assertions.

      - Provide for transaction local indices.  The index is dropped
        when the tx completes.

      - Provide for registration of a global index within a
        transaction, but the transaction will fail if the index
        already exists when it commits.

Short term tasks:

   - (*) Builds and releases.
   
      - Change over to subversion so that the edit trail does not get
        lost (complex process).

      - Maven 2.x build
      
         - Start doing snapshot releases.

	 - Start periodic project documentation builds, perhaps on SF.
           Publish on the www.bigdata.com site.

         - Change the dependency to dsiutils.  I tried to do this with
           dsiutils-1.0.4 and ran into problems with
           (de-)serialization when compared to the lgpl-utils versions
           of the same classes.  Try this again and pay close
           attention to the lgpl-utils versions of the classes now
           located in dsiutils and see if I can isolated the problem.
           The problem was demonstrated by the bigdata-rdf test suites
           for both the temp and local triple stores but not for the
           bigdata test suites.

	 - Done. Update the Sesame 2.x dependency.

	 - Put all properties into the com.bigdata namespace.

   - Counters

     - Done. Work the counter path, name, and date(s) into the table
       which shows the counters history values so that it can all get
       copied easily into a worksheet.

     - Done. #commit is not being encoded property and shows up as a
       URL anchor and not as part of the PATH parameter.

     - Done. Counter XML MUST persist the HISTORY in the XML so that
       the log files can be useful for post-mortem.
       
     - Done. Write a final log file ('-final.xml') when the LBS
       terminates.

     - Done. This is now a configuration property.  The load balancer
       is not writing its counters into the correct location (logDir).
       The directory needs to be relative to the service directory, so
       a method needs to expose that directory to the service.

     - Done. (Not quite sure what the problem was here, but I made a
       few changes and it appears to be fixed.)  The concurrent data
       loader was failing to halt once it started the flush tasks.

     - Done.  (Modified to accept samples out of timestamp order and
       to record the #of and total of samples falling within a given
       period.)  Loosing some samples through reporting w/in the same
       period.  Round up to the next period if this period is filled.
       An alternative is to sum the samples in the period and report
       their average by also tracking the #of samples in the period!

     - Done. When writing the path in the table rows, only write the
       path from the selected root.

     - Done. Problem with double-decoding of URL in NanoHTTP.

     - Done. (Can be a bit odd when also using a regex filter.) Add
       depth query parameter to limit the #of levels resolved from the
       path.

     - Done. (Currently using engineering notation, should be query
       parameter).  Set to 6 digits precision, not {3,6} after the
       decimal.  Or right justify decimal value with fixed N digits
       after the decimal (could be query param).

     - Done. (Also added the timestamp itself.) When converting to
       minutes, hours, and days in httpd make sure to have a few
       digits after the decimal -- otherwise false boundaries.

     - Done. (uses wildcards before and after and ORs together.) The
       filter needs to accept regex characters or prefix and post-fix
       with ".*".  Since things are quoted, right now nothing is
       actually matched.

     - Done. Since the log files provide post-mortem, there should be
       a way to view the files through the same httpd tool - a mode
       where it reads a single named counter XML file and then lets
       you browse it.  This will make it easy to find interesting
       views.

     - Done. The IndexManager should report the #of index views open
       concurrently.  Either sample once per second and take a moving
       average track the total number and compute the instanteous
       average per minute.
 
     - Done (reports the #of stores in the weak value
       cache). Likewise, the StoreManager should report the #of open
       journals and index segments.

     - Done. Anything with "%" or "percent" in the name should be
       formatted as a percentage in [0.00:1.00].

     - Done.  The data service should report its configuration
       properties under "Info".

       Done. This should be done for the other services as well.
       Refactor the code in DataService, moving it into the counters
       package.

       Note: Servers should add their Jini configuration information
       as well.  This probably has to be done explicitly for the
       configuration items of interest.

     - Done. Compute average response time.  Throughput is 1/average
       response time.

     - Done. Add counters for #of index partition split, move, and
       join operations (OverflowManager).

       Done. Also report #of errors during asynchronous overflow
       processing.

       Note: There should also be a counter of the #of index
       partitions moved onto a data service.  However there is no
       place in the code to easily note this on the target data
       service since the move is made atomic by an action on the
       metadata service.

     - Done. Add counter to the write service that reports the #of
       tasks which have their locks and are actually doing their work
       concurrently (LockManager defines such a counter but we need
       its moving average not the instantaneous value).  This is the
       real concurrency of the tasks.  The #of active tasks in the
       write service is a red herring since those tasks could be
       waiting for their locks.

     - Done. Add per-process counters for GarbageCollectorMXBeans.

     - Done. (NanoHTTPD was not reporting errors in the serve() method
       anywhere and was failing to send back an error to the client.)
       For some reason a query for the hostname on host3 does not
       return the counters in the browser.  Response is fast both
       beneath that level at at the root.  Maybe the problem is in the
       /host/CPU and /host/Info counter sets - those appear to hang
       while /host/service is fine.... that does not seem to pay out
       either.

     - Done.  (I am assuming that Format was not thread safe - it was
       being used concurrently by the Sar collector and the pidstat
       collector, even with only one service). pidstat : Problem
       parsing [02:08:24 PM] as a time for input string "".  This is
       odd.  I can test this out and it works for the specified
       format.  And it is the only error reported for pidstat parsing.
       Ah.  Format is doubtless not thread-safe.

     - Done. The per-process counters for linux are not being reported
       under "service" but instead directly under the service UUID.

     - Done (also fixed a bug where the LDS was not sending a join
       message to the LBS and modified notify(), warn(), and urgent()
       to invoke join() on the behalf of remote clients). The LBS
       should add counters for the host scores.  This will provide
       transparency in how it interprets the data from the various
       hosts.

     - Done. The client can discover the LBS and report data every so
       often.  This would result in redundent reporting when there is
       more than one client if the counter reflects the database
       state, but there is no harm in that.

     - Done. Could report the #of files read, #of triples processed,
       triples in the db, average throughput rate for that client (or
       all clients), etc

       - (**??) tps appears low as reported by the client to the LBS
         when compared to the final value computed by the client.
         This may be a function of the outstanding writers that have
         not yet completed, in which case the loader clearly needs to
         force the report of the final counter values when a load
         completes.

         Maybe this is reporting the upper bound for statements?  That
         does not make sense though since only deleted entries or
         views with an index segment cause the upper bound to be
         higher than the actual entey count.

       - Compute bytes per statement in the db (requires a db op to
         correlate the journal size with the #of stmts or #of terms)

       - Expose the known triple stores, a view on the global sparse
         row store, etc.

       - (*) report success and errors from the concurrent data loader.

     - (***) Report response time measures on the client, which will
       require a class similar to TaskCounters that is intimate with
       the ClientIndexView.  That will give a client perspective on
       the latency of tasks, which will aggregate across the data
       services that it uses and include the costs of RMI, in contrast
       to a data service perspective, which aggregates across the
       clients using that service and discount RMI.

     - (*****) Debug overflow handling.

       I believe that the problem lies in how new indices are being
       created and the interaction with the index autoCommit,
       checkpoint and group commit mechanisms.

       It appears that indices are becoming available to concurrent
       writers before the commit point (e.g., when the metadata
       service has been notified but data service has not yet
       committed), which breaks the concurrency control mechanisms.

       The primary suspect is when registering or dropping an new
       index since these activities have side-effects via Name2Addr
       which are immediately visible to concurrent unisolated tasks or
       to new read-committed tasks.

       What is required in an atomic change in visibility for one or
       more indices (register/drop) that is coordinated with the
       commit protocol.

       The changes to Name2Addr need to be buffered such that they
       remain visible only to the task which executed those
       operations, inconsistencies in the registered or dropped
       indices across tasks within the same commit group need to be
       eagerly detected, and the changes need to be applied atomically
       with the next commit within Name2Addr's own commit protocol.

       Problem exist with:

       - MetadataService#RegisterScaleOutIndexTask - registers an index

       - JoinIndexPartitionTask - drops the old index partition and
         registers the new index partition.

       - SplitIndexPartitionTask - registers the new index partitions
         and drops the old index partition.

       - AbstractResourceManagerTask - documents a potential problem
         with MDI updates without 2-phase commits.  Look into this
         further and see if this problem can be addressed without
         using a full transaction.  If not, then we will need to use a
         full transaction to avoid this issue.

     - (**) The IndexManager should report the #of open indices.  We
       can have an exact count of that if we use a static atomic
       integer in AbstractBTree.
       
       Likewise, we can get the exact count of the #of open journals
       vs index segment stores and BTree vs IndexSegments using the
       same technique.

       The statistics that are used by the overflow manager to compute
       which indices are move candidates are not being exposed via the
       counters to the LBS.  This includes all of the per-index bytes
       read, bytes written, etc. data.

     - (**) The IndexManager should report the index partitions on the
       data service, at least until this exceeds 100s of indices.
       This should be done not via counters but rather via the httpd
       service itself making an RMI request to the data service and
       listing out the named indices. Present additional information
       when the service is a metadata service, e.g., by listing out
       the tuples of the index, which are in fact the locators for the
       index partitions.  Also, provide some aggregation over the MDI,
       including the total #of partitions and tuples.

     - (**) The per-host physical disk counters for linux are not
       being collected.

       Write the Sar, iostat, or vmstat utility to collect these data.
       This is a bit more complex under Linux since the data are
       reported by device and the relationship of the devices to the
       file system should be explicated.

       Use [df] to report the % free space remaining?  There should be
       one value for the logical disk (perhaps), and there should be a
       report of the % free space remaining on the volume on which the
       dataDir is located and the volume on which the tmpDir is
       located.

     - The counters are overflowing to days before midnight.  Check
       the locale and see when timestamp causes an overflow to the
       next hour and the next day in some unit tests.

     - When restoring the LBS counters from XML, the history on the
       counters is being ignored.  Instead it should be read in using
       history.add(val,timestamp) or historyInst. add(val,timestamp).

     - Make it possible to have more than 60 minutes in the buffer but
       still overflow after 60 minutes onto the hours.  This will
       allow a longer reachback at a given level of aggregation.

     - Add histograms of response time by task (on client and
       service?).  E.g., AddIds, AddTerms, IndexWriteProx, etc.

     - Add UI elements to set the filter(s), depth, decimalFormat,
       etc.  These should be a FORM with a GET action.

     - The services should be translated (or annotated) by the
       hostname and service info.  This will make the information more
       interesting than just staring at UUIDs.

     - I should be able to bookmark interesting counter sets, but the
       service name needs to be there or the bookmarks will depend
       only on filters, such as all unisolated counters.  An xpath
       might be useful here so that the filter reads down to a service
       name or type and then back up to reveal the service.

     - May be loosing some samples by running multiple typeperf's at
       once.  Explore.  If true, then trying combining all w/in same
       JVM using reference counter for process or identifying one
       process in the JVM which will have responsibility for those
       counters.

     - Cache-Control (data are good up to 60 seconds after the
       counters were last updated) and auto-update of page/view?

     - syslogd integration so that I see ERROR and FATAL messages for
       the hosts in the federation.

   - LocalTripleStoreWithEmbeddedDataService

     - Benchmark with owl:sameAs backchainer.

     - (*****) Optimized JOIN that assumes that all indices are local
       within the data service and reads locally on both access paths.
       This would probably be implemented as a AbstractTask and it
       would need to declare access to the indices being used for the
       left and right hand sides of the join.

     - test small and large document sets with and without incremental
       closure:

       -server -Xmx500m -DtestClass=com.bigdata.rdf.store.TestLocalTripleStore -Ddocuments.directory=../rdf-data/metrics/smallDocuments -Ddocuments.ontology=../rdf-data/metrics/metricsOntology_v1.9.rdfs -Dfile=C:/smallDocuments.jnl -DdataLoader.commit=None -DdataLoader.closure=None

   - ScaleOutTripleStore

     - (*****) Optimized JOIN for the scale-out triple store.  It
       needs to block up a set of right hand tuples that will be
       joined against data on a given data service and then send those
       tuples to that data service, recieving the results in
       return. It will also have to handle stale locators if the join
       is running in an read-committed mode, but not if it is
       transactional or a historical read.

       Inference is slow due to a large #of small join results.
       Parallel sub-query is probably the way to beat that.  After
       tuning, compare to the purely local unconcurrent line.

       Consider batching a set of rangeQueries together in a single
       operation vs parallel submits.

     - The distinct term scan (really, the prefix scan) needs to be
       optimized for the data service and the federation use cases.
       This also effects the sparse row store.  Also, the sparse row
       store needs to use an extended split handler that always
       chooses a split point which is on a logical row boundary.

     - (*) Need ability to request a rangeIterator that reads in
       reverse key order and the ability to visit the prior or next
       key.  This requirement arises in particular for the bigdata
       repository which is currently using the ILinearList API for the
       AtomicAppend.  This will also help us to replace the
       requirement for the ILinearList API in the metadata index.
       
       The change needs to occur at several levels and should include
       a prior() method on ITupleIterator and the ability to acquire
       an ITupleIterator for ITuple returned from lookup(), insert(),
       etc. so that it can be turned into an iterator for prior/next visitation.

       This is a good time to do efficient prior/next leaf operations
       for the IndexSegment and to make that more efficient for the
       BTree as well.

     - Done. (get() and find() were running as unisolated tasks.)  I
       am seeing a lot of tasks in the concurrency manager for the
       metadata service, but few commits on the live journal.  What
       the heck is the being reported for the metadata service?

   - (*****) The consistentRead option used by the
     PartitionedIndexRangeIterator needs to use the most recent commit
     time for the federation.

     Currently it uses the lastCommitTime for the first index
     partition which it scans.  However, it is quite possible that
     there have been writes on other index partitions since which
     would not be reflected in BTree#lastCommitTime.  The
     consistentRead would therefore reflect an older history rather
     than the most recent writes when it moved onto the other index
     partitions.

     In order to fix this the data services MUST discover and use a
     centralized timeservice.  This can be essentially a stripped down
     centralized transaction manager.  The data services will obtain
     their commitTime timestamps from this centralized time service
     and MUST also notify the centralized timeservice once they have
     successfully committed.  The PartitionedRangeIterator will then
     query the centralized time service for the last commit time for
     the federation as a whole and use that as the time for a
     consistentRead operation.

     As an alternative, the data services could periodically publish
     their commit times.  I need to see how much of a bottleneck it is
     for clients calling nextTimestamp().

   - OverflowManager

      - Could optionally convert from  a fully-buffered to a disk-only
        store  in  order to  reduce  the  memory  footprint for  fully
        buffered  stores,  but in  that  case  this conversion  should
        happen once asynchronous overflow handling was complete.

   - StoreManager / IndexManager

      - Modify  LRU  to  purge  entries  older than  a  specified  age
        (including an asych  daemon thread to make sure  that they get
        purged even if the LRU is not being touched).  Do this for the
        index segment cache in the IndexManager as well.

      - (*) Better concurrency for openIndex, openStore, getJournal,
        and getIndexOnStore

      - Modify WeakValueCache to use ConcurrentHashMap and support an
        atomic putIfAbsent operation.  This will reduce the latency
        imposed when we need to re-open an index segment from a store.

      - (***) StoreManager#getEarliestDependencyTimestamp() is not
        implemented.  Old store files will not be released as a
        result unless you explicitly set the release time.

      - Should recognize a "disk full" situation and shutdown the data
        service cleanly.

   - LockManager

       - Use a WeakValueCache to purge unused resources.  The size of
         its internal map from resource name to resource queue will
         grow without bound on a data service as index partitions are
         split and moved around.  There are notes on this issue in the
         LockManager class.

   - DiskOnlyStrategy
   
     - Lazy creation of the backing file.

     - Done (No performance change for small stores - I still need to
       review the data for large stores.  Of interest, the write cache
       on a large store is nearly never a hit when trying to read a
       record - this suggests that a read cache will be of no
       benefit).  An LRU read cache for records.

       This could be a big win for the DiskOnlyStrategy.  Either make
       this its own layer that can be interposed between the journal
       and the DiskOnlyStrategy or add directly to the
       DiskOnlyStrategy since a read cache is not required for the
       fully buffered modes. Regardless, allow configuration of the
       cache size.

       Also, efficient nextLeaf could improve read performance by
       reducing node reads.

       Write through to the write cache and flush through to the disk.
       On read, test the read cache.  If not found, read the write
       cache, then the disk.  These are simple layering semantics.

       Use an LRU with a capacity of ~5k records.  The records are
       read-only so we do not need to worry about a canonicalizing
       mapping.

       The read cache is specific to a journal.  Each journal gets its
       own read cache.  Historical journals might have a smaller read
       cache capacity, or maybe 2k records is enough for any journal.
       Experiment and find out.

       (***) Look at the effect [host3] on readSecs, on the ratio of
       readSecs to writeSecs, and on IOWait, especially as the size of
       the journal grows.  If the LRU is not paying its way then
       disable it by default.

     - CounterSets

       - Add counters designed to give insight into whether the write
         cache tends to full up completely or only partly before the
         next group command and the #of bytes that tend to be written.
         What I want to understand is whether the cache is too large
         and whether an asynchronous of the cache to the disk would be
         a benefit.

	 *** #bytes/commit (measured delta in offset from commit to
              commit).

	      Also, #flushes / commit - when ~ 1:1 the write cache is
	      at least large enough.

	 Note that writes which would exceed the remaining cache size
         cause the existing cache to be flushed while writes that
         exceed the cache capacity are written directly to the disk -
         the cache itself is always dense in terms of the bytes
         written on the address space.

   - Done. Full text indexing for KB.

       - Done. Analyzers are not thread-safe.

       - Try out an mg4j integration for an alternative text indexer
         and search.

   - Consider thread pool size defaults, especially for the temporary
     stores such as the temp triple store.  What is a good policy?

   - Provide option to pass in a write cache buffer for a temporary
     store and use that buffer as the in-memory store before it
     overflows onto the disk.

   - *** Batch API for extractor, allowing runs directly against the
     KB.  Form a single prefix-scan query from a sort of all simple
     terms in the document and then piece together the phrases from
     the result.

   - ** Change tx timestamps to negative and use positive timestamps for
     historical reads.  Changes to AbstractTask, ITx,
     ITransactionManager, StoreFileManager, IsolationEnum, and the
     post-processing tasks.  This will greatly simplify thinking about
     historical read operations since they will simply use the actual
     commit time while transactions will use a free (-timestamp) value
     selected by the transaction manager.

   - Modify to have an observable event or callback that assigns the
     service UUID and that indicates when the resource manager is
     running and refactor the LDS and DS startup logic to use that to
     configure the reporting of counters and an optional httpd service
     (at least for the LDS). The relevant Jini method is
     ServiceIDNotify().  For the moment I have disabled the httpd for
     the LDS.

   - Consider dropping the BasicRioLoader, PresortRioLoader, etc.  All
     of the benefit is in the use of the StatementBuffer.  These
     loaders just obscure the RIO mechanism and make them harder to
     configure.

     The DataLoader might be a utility class.

     The ConcurrentDataLoader is certainly a useful utility class.

   - Quad store.

    - ** Sesame 2 TCK (integration tests)

         (temp fix) You should add the following URL as a maven
	 repository to your maven settings.xml file:

	    http://repo.aduna-software.org/maven2/releases/

    - ***** Two database modes: named-graph mode (quads with all 6
      indices) and provenance mode (3+1 where the context position
      holding a bnode with a 1:1 relationship to the triple and
      therefore serving as a statement identifier and is stored as the
      value associated to the triple in the index; The source
      extension for RDF/XML needs to be supported such that the given
      BNode or URI for the source is correlated to the use of that
      same Resource elsewhere in the same RDF/XML document - we need
      to extend RIO for this).

      1. 3+1

	 - Done. value serializer must be different when using sids

	 - Note: any partition of the term:id index may be used to
           assign term identifiers for bnodes since the bnode ID is
           only required to be distinct, but not stable.

	 - Done. TMStatementBuffer needs to recursively wipe out
           statements using a statement identifier.  this should be
           part of truth maintenance.  when we get the original set of
           explicit statements to be removed we collect their
           statement identifiers and then collect all statements using
           those statement identifiers in either the subject or object
           position and add them to the original set of statements to
           be removed.

	   Done. AccessPath#removeAll() we also need to collect the
	   set of statements using a statement identifier and delete
	   them as well.  this will wind up being a double test for
	   the original set of statements if TM is being used, but it
	   is required when TM is not in use.

	   Done. Do not generate statement identifiers if the SPO is
	   marked as an inference (an optimization).
	   
	   Done. Modify the procedure that actually writes on the
	   statement index to write a ZERO (0L) statement identifier
	   if the statement is (in fact at the time that we examine
	   the statement index) an inference or an axiom.  If an
	   explicit statement is later asserted for the same SPO, then
	   we need to overwrite that 0L with the assigned statement
	   identifier.

	   Done. override ISPOIterator#close() when returning an
	   iterator backed by a temporary store.

	   Done. infinite loop test case fails when sids NOT used. I'm
	   not sure how the test was succeeding before, but the
	   problem was failing to verify that a statement was in the
	   database before adding it to the focusStore.

	   Done. TestTripleStore#removeStatements() has problem with
	   sids.  The problem is that the sid is not getting placed
	   onto the SPO by the unit test, but it highlights the fact
	   that with sids you need to either have the SID on hand
	   before calling removeStatements() or I need to modify the
	   code to resolve the sids as a first step when I compute
	   their fixed point (at which point I could also discard any
	   statements that were not actually in the database).
	   (addStatements already resolves sids so as to always make
	   them consistent).

	   Done. write unit tests for TM cases when using statement
	   identifiers to make metadata statements.

	 - Work through an import scenario from an application that is
           using URIs generated from the {s,p,o} to represent the
           statement identifier.

	 - Survey all of the ways in which reportStatement/3 gets
           called in RDFXMLParser and decide whether or not "context"
           (the variable set based on bigdata:sid) is always correct
           (either "" or the sid) or if there are some uses, such as
           reification, where "context" should be ignored and update
           the calls to reportStatement/3 or reportStatement/4 as
           appropriate.
      
      2. Done. RDF/XML w/ statement identifiers in/out.

         - @todo reduce to only an "explicit" flag rather than
           {explicit, axiom, inferred} since we can not differentiate
           between constructed statements and inferences.

      3. High level query for reading variable bindings out.

	 - Verify a CONSTRUCT query.

	 - Done. Verify a SELECT query using statement identifiers.

      4. (****) JOIN optimization.

	 - (*) direct term scan.

	 - Map the first triple pattern over its index, and for each
           mapped key-range of the index collect intermediate results
           and map them over the next index.  This must use
           read-historical or read-committed access to avoid locking
           up the unisolated index, but that's going to be automatic
           since the join operator itself does not write on an index.
           the buffer in which we accumulate the join results needs to
           write somewhere, and the choices either back to the client,
           onto the focusStore, or onto the database.  All of those
           can be handled since the read-only procedure will be either
           returning a result or submitting a write procedure.  A join
           variable buffer should accumulate those results and then
           write only the selected variables into any of the
           appropriate locations on overflow (via subclassing or a
           ctor parameter for the writer).

	 - Any SPARQL query which can be directly mapped onto a series
           of JOINS can be directly translated into a rule and run by
           the existing rule engine.  If it has filters that need to
           be applied then they should be handled by a filter applied
           to the buffered join results.  That filter can even handle
	   batch filtering by inspection of datatyped literals.

	 - A further optimization is possible for the local data
           service since all indices are known to be local and the
           index lookups do not need to be batched since they will
           always be continuous unbuffered local reads.

	 - ***** Replace Sesame 2 JOINs by re-writes into our rule
                 engine.
      
      6. Publish on statement level provenance and truth maintenance
         for SPARQL end points.

      7. Defer "named-graph" style quad store for now.

      8. (***) implement prefix compression and apply to the lexicon.
         test it out also on the statement indices and see how it
         stacks up against the "fast rdf key" compression and compare
         with huffman encoding of the decoded long identifiers as
         well.

	 See it.unimi.dsi.fastutil.bytes#ByteArrayFrontCodedList.

	 Note: We should use front compression by default for the
	 nodes of the BTree.  Since the separator keys in the nodes
	 are dynamically determined prefixes, application specified
	 compression generally can't be used.

   - SemTech08 submission?

   - Done.  Correctness testing for scale-out with index partition
     split, move, and join.  See services/StressTestConcurrent. It can
     be parameterized for this purpose.

   - streaming io for block read/write.  note that asynch IO for the
     disk only strategy will not have an impact if we are doing
     commits whose size fits within a single write cache (10M by
     default).

   - write performance test drivers and run on cluster.

      - rdf concurrent query (rdf lubm is not designed to test with
        concurrent loads).

      - bigdata file system workload (must provide reverse traversal
        iterator to handle atomic append for the key-range partitioned
        indices).

      - (****) Write script to allocate services to nodes.

        - N data services; 1 MDS; 1 LBS; 1 TS, etc.

	- The script needs to start the services on a LOCAL disk on
          each machine (I am currently setup on NAS so that means the
          DISK is REMOTE).  This means replicating the environment
          onto the local host (at least the configuration) and then
          starting the service.  The classpath could be resolved on
          NAS or replicated onto the local host and resolve there. (I
          just need to copy [policy.all,
          bigdata-rdf/src/resources/logging/log4j.properties, and
          bigdata-rdf/src/resources/config/standalone ->
          .../src/resources/config/standalone {create the directory
          path first}].  I could also copy the classpath resources,
          but presumably they will be fetched quickly enough and
          become stable - or maybe not?

	- Need to touch up launch-all after the jini install as well
          as installing from a pre-touched version fixing the
          LD_ASSUME_KERNEL_VERSION bug.

	- Get clusterondemand account.

	- Support downloadable code in the configuration, including a
          an optional security model.

      - (*******) rdf concurrent data load.

	- Still an annoying problem with the service names as
          displayed by the jini browser....  This may well be an issue
          with failing to expose the interfaces and service classes
          via an http service as downloadable code.

	- Done. Verify that there is an atomic commit point for the
          IndexSegment so that partial index segment writes can be
          recognized.

	- (***) Add post-load validation to make sure that the
	  concurrent data load is really correct and run that
	  validation after a db restart.  This will probably require
	  that I write the bulk term scan and get the validation unit
	  tests running for scale-out modes.
 
	  ============================================================

	- Results:

	  Note: writing on local disk but reading the classpath and
          data from NAS.

	  Note: test on host1/host2 and get a sense of the platform
	  differences.

	  **** Try with overflow processing enabled.

	       - (Verify again.) The memory demand jumps up during
	         overflow handling.  maybe I need to throttle the
	         thread pool more on the data service?

	  **** Try with services running in distinct JVMs and on more
               than one host.

	  **** Try w/o disabling flush so that each document is parsed
               and written in a single pass.  Also try w/ different
               combinations of the statement buffer size and the #of
               threads in the client.

	  **** Measure the effect of the WriteCache on U100 and U1000.
               Measure again with the onboard disk cache disabled.

	  http://192.168.20.28:8080/

	- U10, host3, LDS, 20 threads: 11517 tps <== verify again

	- U10, host3, LDS, 10 threads: 16837/16389/16942 tps.

	- U100, host3, LDS, 10 threads, writeCache: 15490 tps.

	- U1000, host3, LDS, 10 threads, writeCache: _____  tps; no
          problems until it runs out of disk space.
	
	- U10, host3, Jini, 20 threads: fails to progress.  (*) Why is
          the system so sensitive to the #of client threads?

	- U10, host3, Jini, 10 threads: 9692/9611/9406 tps

	- U100, host3, Jini, 10 threads: 11693 tps - saved copy of the
          log output.

	- U1000, host3, Jini, 10 threads, writeCache, no overflow:
          ______ tps. (Should run until its out of disk now that the
          write cache memory leak is fixed.  A run without the write
          cache progressed fine until it ran out of disk after 6 hours
          (U1000-host3-disk-full).  At that point it had loaded 17349
          out of ~20000 files and consumed ~59G.)

	  ----------------- overflow enabled --------------

	- U10, host3, Jini, 10 threads: ______ tps.

============================================================

	Looks like virtuoso is running a clustered triple store!

	http://virtuoso.openlinksw.com/wiki/main/Main/VOSArticleLUBMBenchmark

	http://www.openlinksw.com/weblog/oerling/?id=1336

	http://www.openlinksw.com/weblog/oerling/?id=1335

	http://docs.openlinksw.com/virtuoso/clusterprogrammingsqlexmod.html

============================================================

**** Write logic to validate the KB load and run it against the
     result.  I will have to stop before I run out of space on the
     disk, so maybe U500 rather than U1000 - or maybe U1000 will be Ok
     with periodic index segment builds.

**** Compare performance with 1M and 10M write cache capacity.

**** Add option to NOT run typeperf and use for the unit tests of the
     services when performance counters are not required.

Issues to be resolved in this branch.

1. Re-design of the node/leaf API to support processing of serialized
   nodes and leaves without materializing their contents as java
   objects.  Nodes and leaves are now expanded into Java objects iff a
   mutation needs to be performed on the node or leaf. 

   Note: Since search may have more overhead, operations per second
   (lookup, insert, etc) may actually be slower until caching effects
   trump IO Wait.

2. Global record or B+Tree node/leaf record buffering for a DS.  This
   allows us to efficiently use all available RAM to buffer the
   indices.

3. Store level record compression.

   See com.bigdata.io.compression as well as
   com.bigdata.btree.raba.codec.TestHuffmanEncoder, NodeSerializer,
   and IndexSegmentBuilder.

4. Dependency refactor (fastutil, dsiutils, lgpl-utils).

5. Performance optimizations for the B+Tree refactor change set,
   including the performance tuning for the rabas, the LRU buffer,
   configurations, etc.

--------------------

Query optimization

x. Parallel range iterator flag.

x. Resolve RDF Values using JOINs.

x. Distributed join optimization (performance tuning, hot spot
   elimination, configuration parameter tuning, etc).

--------------------

Index partition splits.

   - Proactive scheduling

   - Faster.  The best solution is to split after a compacting merge
     since we have exact knowledge of the tuple counts and exact
     knowledge of the on disk storage requirements at that time.

   - More robust (transactional)

   - Based on #of bytes in post-merge file (no need to tweak).

--------------------
Sync of disconnected clients:

Offhand, I see three ways of approaching this.  One is to simply log
the deltas and replay the log to the client.  Another is to store
version timestamps on the tuples and use a scan to identify the change
set, or alternatively to scan the two B+Tree revisions in parallel,
reporting only the delta (tuples found in one index but not the
other), where a tuple would correspond to an SPO(C).  If we go with
the per-tuple revision timestamp, but also store the most current
tuple revision timestamp on the ancestor nodes (these are always
touched when we update a descendent), then we can prune the search on
any node/leaf whose largest revision timestamp is LT then starting
revision required to sync the client.

The revision timestamp is LT the commit timestamp since revision
timestamps are assigned at the start of the commit protocol, while the
commit timestamp is assigned once the write sets have been reconciled,
the indices have been checkpointed, and all is good for the commit,
which just updates the root block.  Revision timestamps on a given
B+Tree will be totally ordered for concurrent transactions since the
commit protocol has an exclusive lock on the unisolated B+Tree view.
Since we would be working with revision timestamps rather than commit
timestamps, I could either extend the transaction service to allow
reporting of the assigned revision timestamp, or it could be obtained
by an after action read on the root of the B+Tree using the commit
time of the transaction.  Tuple revision timestamps and delete markers
are automatically maintained by the transaction commit protocol.

This approach would do much less work since we would basically skip
any node (and all dominated leaves) which had not been updated since
the starting revision for the sync.  New statements would be
associated with tuples having a revision timestamp GT the starting
revision time for the sync.  Deleted statements would be identified by
the presence of a tuple with a delete marker having a revision
timestamp GT the starting revision for the sync.

If it sounds like that would do what you want, I can think about how
to raise that information up into a filter on the B+Tree iterators and
from there to the KB layer.  If we did this for the SPO(C) index, then
you could resolve those tuples to RDF Statements and push them to the
client.  The application would need a log of the revision timestamps
associated with the commit timestamps so it could use the former to
sync clients and the latter to read from a historical commit point.
Probably the transaction service could be extended to retain that
information for a temporal store.

Scale-out for a temporal store will require the ability to patch the
MDI (or maybe just the logical to physical DS mapping) with the new
locations of index partitions which are moved due to machine failure.

--------------------

Asynchronous notification of RDF change sets (deltas).

There is some interest in a feature which would report on the change
sets for the RDF database.  E.g., triples added or removed.  In the
case of truth maintenance, this might also include the state change
for triples between "explicit" and "inference".

For the current SAIL, we could extract bit flags for each triple
written on (or retracted from) the database indicating for that triple
whether or not the database was modified (e.g., whether it was already
present when added or whether it was not present when retracted).  We
would do this for just one of the statement indices -- probably the
SPO index.  This information could be reported via an asynchronous
listener mechanism each time a chunk of RDF statements is flushed to
the database.  This might be a few days effort.

When performing truth maintenance, we fix point a temporary triple
store until it contains the statements to be added to (or removed
from) the database.  This can include explicit statements which become
inferences, or inferences which are no longer supported and are fully
retracted.  In this case, it would be possible to report statements
which were added, removed, or whose status as inferences or explicit
statements was changed by the operations.  This is a bit more work.

Doing this for transactions would require an extension to the commit
protocol since the events would occur during the "merge down" of the
transaction write set onto the unisolated indices.  If the "statement
change set" events are to be ACID, then we will have to log the events
and play them back to the listener as an after action on the commit.
That way the listener would have a guarantee that the persistent state
change was in fact committed.  That log might be a restart safe queue,
so a listener could ensure that they observed all state changes. If we
mark the commit time associated with a set of logged changes, then a
listener could survive restart and request a reply of the change sets
since some historical commit time.  Those events would be in terms of
the internal keys of the statement index, which are essentially
long[3] and correspond to the Subject, Predicate, and Object positions
of a triple.  Those keys could be resolved to an RDF Statement, and
the application would set the change set as RDF Statements.  For truth
maintenance, we would also log the change in the statement type
(explicit, inference).  Handling change set notification for
transactions is clearly more complex.

By doing this at the database layer we should be able to report events
even when concurrent operations insert the same triples without
duplicate events.  Change set detection would also work for scale-out,
but a proxy for the listener would have to be registered with each
data service.

The more complex approach would be if you were using native (bigdata)
transactions, wanted the notifications to have the same ACID semantics
as the transaction. E.g., the application would never see a change set
unless the transaction was successfully committed to disk, and
required a guarantee that you eventually saw all change sets.

You could still do the "bit flag" route with the clustered backend,
but you would need to register proxies for listeners at each bigdata
node on the cluster.

-------------------- 
Examples:

 - Example showing use of the {@link KeyBuilder} (and DDL?)
  
 - Example showing how to use the scale-out indices efficiently with
   {@link IIndexProcedure}s. this approach is necessary when you need
   to do complex logic which would otherwise result in a large volume
   of point tests against the scale-out index. Instead you map a
   procedure against the index and it is executed locally on each
   index partition within some key-range.

------------------------------------------------------------
Other:

- Examine opportunities for "rack aware" selection of the CS or DS
  when integrating with HDFS.

-- merge to branch -- 

from: https://bigdata.svn.sourceforge.net/svnroot/bigdata/trunk

Revision: 2004 (this is the first revision of the branch - the one
	        where it was created)

to: https://bigdata.svn.sourceforge.net/svnroot/bigdata/branches/BTREE_BUFFER_BRANCH

Revision: Head

Commit message: Merging trunk into branch BTREE_BUFFER_BRANCH [2004]:[2022].

See http://openmrs.org/wiki/Subversion_Branching_and_Merging_Techniques

------------------------------------------------------------
-------------- Store level record compression --------------
------------------------------------------------------------

- Support gzip, zip, bmz and other compression protocols in the store
  at the record level.

       - bmz implementation in C available from hypertable at
         http://www.hypertable.org/doxygen/dir_636d9f0bc773a215f705e2de9f182c4e.html
         (GPL).  The author was Luke Lu.  I don't know if there is a
         JNI wrapper for it or not.  Luke Lu <hypertable@vicaya.com>
         and probably Luke (vicaya@gmail.com).

	 There is a open source implementation of bmdiff/zippy like
         library in Hypertable, called bmz. It’s written in pure ANSI
         C, and can be easily embedded in any project (that was a goal
         I had in mind, so it’s written in C instead of C++ like the
         rest of the Hypertable). The performance is similar to
         google’s published numbers for small blocks (the size of
         sstable block from 64KB-128KB. The speed to will go down
         dramatically (to about 40MB/s), when the block size is big
         enough to thrash the processor cache, as the algorithm uses a
         hashtable.)  I wrote it (the bmdiff part, lzo is used for the
         optional final pass) mostly for experiments without much
         optimization (so there are rooms for improvement). The
         Rabin-Karp like hash functions can be easily plugged in for
         experiments. Feel free to give it try and ping me on how to
         use the library (documentation is in bmz.h.)

Goals:

  - Compression: two-level compression.

    1. Record compression (e.g., deflate)

       Store as record header + compressed (deflate) record body.

       Using deflate (record level compression) is new.  The thing
       which makes it difficult is the double-linked leaves in the
       index segment.  The record size must be known when we obtain
       the address of the record from the store.  This means that the
       priorAddr/nextAddr are outside of the compressed region.  Thus
       generalized record compression at the store level requires us
       to mark the #of header bytes.  Otherwise we can handle record
       compression at the B+Tree level.

    2.

     Header:

	  -- header --
	  headerDataLength(byte) // node=1;leaf=1;linkedLeaf=17(1+sizeof(long)*2)) 
	  [ // This field exists iff compression was used.
	   bodyDataLength(int)
	  ]

       The [headerDataLength] gives the #of bytes of application data
       in the header exclusive of the header metadata. If there is no
       application header data then this field is zero.  The header
       data is intended used to represent data which can not be
       compressed.  For example, the priorAddr and nextAddr fields of
       an IndexSegment leaf can not be compressed because we need to
       know the total size of the record before they can be
       assigned. If the byte count is negative, then it is interpreted
       as the #of 32-bit words containing (uncompressed) header data.

       Either all records in the store are compressed by a common
       technique or none are.  This is indicated when the store is
       created.  The compression method should be something fast for
       the journal when used to absorb writes for a data service and
       could even be "none".  The compression method can also be set
       for each index and will be applied to all IndexSegmentStore
       files generated for that scale-out index.

       When the records for a store are compressed, [bodyDataLength]
       gives the #of bytes in the decompressed record body and is used
       to pre-allocate a right sized ByteBuffer when compressing the
       record.  The actual size of that ByteBuffer is the decoded
       headerDataLength plus the bodyDataLength.

       IRawStore#write(ByteBuffer) always writes a record with zero
       header data bytes.  IRawStore#write(int,ByteBuffer) writes a
       record whose first N bytes are stored in the header data
       section (outside of the compressed record body).  On read,
       the returned ByteBuffer always has exactly the same data.

       There will need to be alternative IRawStore#write() methods
       which accept the #of header data bytes.  The header data itself
       appears in the ByteBuffer to be written just as it would appear
       when the record was deserialized.  Compression may be applied
       to all records on the store.  The root blocks are fixed length
       regions which lie outside of the user space for the store and
       are not compressed. 

       Compression and decompression will need to pay attention to the
       header.  When we ready from the backing file we will get a
       ByteBuffer.  If the record in that ByteBuffer is compressed,
       then we automatically decompress the record.  The origin of the
       ByteBuffer is automatically adjusted to the start of the header
       data area.  The body data bytes follow the header data bytes so
       the ByteBuffer provides an uninterrupted view of the header
       data + body data to the application.

	    @todo Write a compression service.  It can have a queue
		  and use a small thread pool to (de-)compress
		  records.  Each worker thread will have its own
		  compression buffer, which should grow until "right
		  sized".

            FIXME review the use of byteCount as decoded from the addr
            in application level data structures.  If we allow
            optional compression then this will break code which
            assumes that the byteCount reflects the pre-compression
            record size. 
	    
	    @todo The IndexSegmentBuilder will have to explicitly
	          apply the selected record compression and we will
	          then have to patch the priorAddr and nextAddr on the
	          record.

       Checksums are a store-level option.  The checksum is computed
       for the entire record, including the record header and any
       application data stored in the header data bytes.  If both
       compression and checksums are enabled, then the checksum is
       taken of the compressed record.  When enabled, the checksum is
       written into the last 4 bytes of the record, and the size of
       the record is automatically adjusted to store those additional
       bytes.  The checksum itself is be visible at the {@link
       IRawStore} API layer (the ByteBuffer view is adjusted so as to
       hide the checksum).

	    @todo Write a checksum service.  It can have a queue and
		  use a small thread pool to compute or verify
		  checksums for records.  Notification should be
		  synchronous to avoid use of records whose checksums
		  indicate a media error.  This could be layered over
		  the compression service or the same worker thread
		  could be assigned both tasks.  Each worker thread
		  should have a thread local
		  com.bigdata.util.ChecksumUtility instance.

	    @todo The key and value compression APIs may need to be
		  changed since they were written with the assumption
		  that we were encoding onto an OutputStream.  

		  Can we apply record level compression to keys and
		  values serialized for RMI?

		  Can we serialize key/value data as ByteBuffer's
		  wrapped as INodeData for RMI?

	    @todo Move various interfaces and tests related to the
	          node and leaf data records into their own package.

       Node:

	  -- header data --
          type(byte) // 0=node
	  -- body (compressed) --
	  version(short)
	  flags(short) // unused.
    	  nkeys(int)
	  nentries(int)
	  keysSize(int)
	  childAddr[]:long[]
	  childEntryCounts:int[]
	  keys:byte[][] // encoded as dictionary plus byte aligned code strings.

       new Node(ByteBuffer) // wrap ByteBuffer as NodeData, initialize
			    // nkeys and nentries from the ByteBuffer
			    // and set a ref to the NodeData

       ReadOnlyNodeData(ByteBuffer) // wrap and decode.

       ReadOnlyNodeData(INodeData) // allocate ByteBuffer, encode on
				   // ByteBuffer as [header+body], and
				   // wrap.

       Leaf:

	  -- header data --
          type(byte) // 1=leaf; 2=linkedLeaf
	  [ // iff linked leaf.
	   priorAddr(long)
	   nextAddr(long)
	  ]
	  -- body (compressed) --
	  version(short)
	  flags(short) // VERSION_TIMESTAMPS | DELETE_MARKERS
    	  nkeys(int)
	  keysSize(int)
	  valuesSize(int)
	  timestamp:long[] // optional, based on B+Tree provisioning.
	  deleteMarker:bit[] // optional, based on B+Tree provisioning.
	  keys:byte[][] // encoded as dictionary plus byte aligned code strings.
	  values:byte[][] // encoded as dictionary plus byte aligned code strings.

       new Leaf(ByteBuffer) // wrap ByteBuffer as LeafData, initialize
			    // nkeys from the ByteBuffer and set a ref
			    // to the LeafData.  priorAddr and nextAddr
			    // are available iff type==2.

       ReadOnlyLeafData(ByteBuffer) // wrap and decode.

       ReadOnlyLeafData(ILeafData) // allocate ByteBuffer, encode on
				   // ByteBuffer as [header+body], and
				   // wrap.

       These record layouts order the fixed length fields and arrays
       with known dimensions to the front of the record.  The keys and
       values are ordered to the back of the record.

       The node/leaf type byte is in the header because we must
       inspect it before we can interpret the bytes which could
       correspond to the priorAddr and nextAddr fields and those
       fields can not be compressed, which is why they are stored in
       the header data.  This means that the type field itself must be
       in the header data as well.  This means that writing a B+Tree
       node or leaf will always use IRawStore#write(int,ByteBuffer).

       - Done. Create mutable INodeData and ILeafData implementations
         based on the existing code in AbstractNode, Node, Leaf, and
         MutableKeys.

	 - Done (this is now checked by the various methods which copy
	   data out of a leaf). ILeafData#getValue(int) and possibly
	   some other methods must also test whether or not the tuple
	   has been deleted.  That information is NOT available on the
	   IRaba.

	 - Done. nkeys refactor has broken the code.

	 - Done. toString() for Leaf, Node, LeafData impls and NodeData impls.

	 - Done. Finish isolation @ ILeafData and INodeData.

	   - PO:

	     - identity, dirty, deleted

	   - AbstractNode:

	     - self:Reference<AbstractNode<T>>
	     - parent:Reference<Node>
	     - keys:IRaba
	     - referenceCount:int

	   - Node:

	     - childRefs[]
	     - childLocks[]
	     - data:INodeData (new!)

	   - MutableNodeData:

	     - keys:IRaba
	     - nentries:int
	     - childAddr:long[]
	     - childEntryCount:int[]

	   - Leaf

	     - data:ILeafData (new!)

	   - MutableLeafData

	     - keys:IRaba
	     - values:IRaba
	     - deleteMarkers[] (BitVector w/ long[] impl?)
	     - versionTimestamps:long[]

	   - Done (passing in the branchingFactor). Either the
             IRabaCoder must round-trip the capacity or we must have
             appropriate ctors for MutableKeyBuffer and
             MutableValueBuffer so that they are provisioned to the
             correct capacity (rather than just the #of keys or values
             currently in the node or leaf).

	   - Done. verify clearing (in delete()) and stealing (in ctors).

	   - Done. verify copyOnWrite().

	   - Done. Verify use of Node:Node(final Node src, final long
             triggeredByChildId).  The javadoc indicates that this is
             a copy ctor and that the src is always an immutable Node.

	   - Done. Roll prior/next addr into ILeafData?

	 - Done. Review cases where we explicitly tunnel through and
           obtain the mutable version of the keys or values for a node
           or leaf.  I believe that in all cases we are about to
           discard the values in a sibling during a merge, in which
           case it is Ok to decode them first and we do not lose any
           efficiency since we will be storing the keys/values in a
           mutable node/leaf.

1. Done. encode(IRaba,ByteArrayBuffer):byte[]

   This allows buffer reuse, but gives us the coded byte[].  We need
   that to write the #of bytes followed by the coded data for RMI.

2. Done. decode(IRawRecord):IRaba(Decoder)

   This allows decoding a slice of a byte[], which we need for leaves
   and nodes.  Make sure that we decode in place for those cases.

3. Done. IRabaDecoder data():IRawRecord

   probably the latter for now as data() might still be useful.

x. Done. Verify all RabaCoder and leaf/node data records.  

         - Done. Modify IRabaCoder to reduce byte[] allocation:
	   
	 - Done. Optimize the CustomFrontCodedByteArray for IRawRecord
           and optimize the IRabaCoder for that case.

	 - Done. Verify that O_xxx offsets are NOT presumed to be
           absolute when coding.  E.g., that we do not presume that
           the buffer was rewound before we begin to code the
           data. However, they ARE absolute during decoding.

	 - Done. Leaf and node should code seqeuentially and the patch
           the byte length of the keys and values to avoid additional
           byte[] allocations.

	 - Done. Abstract the leaf and node coders into an interface
           parallel in design to the IRabaCoder.
           encode(ILeafData,flags):slice and decode(slice):ILeafData.
           This will let me directly replace the coded representation
           of the leaves.

	 - Done. Unit tests should verify that leaf data and node data
           are correctly coded and decoded when there is a non-zero
           offset.  The IRabaCoder unit tests already handle this.

	 - Done. Unit tests should verify search for random prefix and
           random suffix of known keys.  Again, the IRabaCoder unit
           tests already handle this.

	 - Done. Bug fix for search on front-coded raba.

4. (***) Get rid of all uses of IDataSerializer, replacing them with
   IRabaCoder.

	   For RMI, we need to pass along the byte length of the coded
	   rabas for that to work since we can not just read to the
	   end of the input stream if there is more data following the
	   coded raba.  This looks like:

                 final ByteArrayBuffer buf = new ByteArrayBuffer();
                 tupleSerializer.getLeafKeyCoder().encode(keys, buf);
                 final int nbytes = buf.limit();
                 out.writeInt(nbytes);
                 out.write(buf.array(), 0/*off*/, nbytes);

	   This also means that we will no longer have to worry about
	   whether RMI output streams are buffered.
		
	   What we lose is the ability to stream coded data onto the
	   network without first fully buffering it in memory.  What
	   we gain is the ability to access the coded representation.
	   However, for RMI we MUST buffer the coded representation
	   when sending or receiving.

	 - (***) Clean up (de-)serialization to avoid decoding keys
	         and values.

	   - IRabaDecoder should probably be discarded.  The caller
	     can obtain the backing {byte[],off,len} tuple from their
	     argument to IRabaCoder#encode(raba,out).

 	   - Harmonize IDataSerializer and IRabaCoder.

	   - Examine use of rabas in index procedures.

	   - Optimize out the use of the ByteBuffer.  This should be
             easy to hack w/ record level compression if we use a
             ByteArrayBuffer as the (de-)compression target.

	   - Record level compression for the store.

	   - (*) Verify that transient B+Trees convert data records to
             immutable data records when there is an eviction event.
             This will help us bound their in-memory footprint.

	     In fact, they do not, not even for a B+Tree backed by a
	     persistence store.  This needs to be changed.  The node
	     or leaf data record should be coded, the reference to the
	     data record on the owning node or leaf replaced, and the
	     coded record then written onto the store.  [This last
	     step can be skipped when the B+Tree is transient.]

	   - (*) Should run all of the persistence tests against each
             of the IRabaCoder combinations.

         - IndexMetadata will need to be updated to reflect the codec
	   interface change and the reconcilation of the
	   IDataSerializer with the IRabaCoder.
       
         - NodeSerializer and IAddressSerializer are deprecated and
           will be deleted. IKeyBuffer is deprecated and should be
           deleted - it is only used by the test suites.  The
           ImmutableKeyBuffer is deprecated and should be deleted.

	 - ITupleSerializer - update (IDataSerializer).

	 - (***) ResultSet must operate on the coded representation
           rather than de-serializing the data.

	 - (***) The IIndexProcedure implementations MUST NOT
           deserialize the coded rabas.  Just wrap them.
	   
	   Make sure that we are reusing buffers for RMI. E.g., the
	   index write pipeline code might have a small pool of those
	   buffers.  The buffers could be linked to a "coding" task.

	   getKeys():IRaba and getValues():IRaba SHOULD be raised into
	   the IKeyArrayIndexProcedure API.  This will make it easier
	   to use efficient access to the coded data.

5. (***) Replace data record in AbstractBTree on eviction with coded
   data record.

         - Must replace the [data] with the read-only record.  This
           will change the NodeSerializer API since we need the
           ReadOnlyLeafData or ReadOnlyNodeData object back, not just
           the ByteBuffer (this is available from those classes).

	     AbstractBTree#writeNodeOrLeaf(final AbstractNode node)

	 - Verify that we do this for transient B+Tree instances as
           well.

         - Each mutable B+Tree should use a single backing buffer
           instance for key and value compression (NodeSerializer did
           it this way, so there might be a reason to keep an object
           around which has those buffers)

	 - The persistence tests should use the default codec for keys
           and values.  Either front-coded or canonical huffman for
           keys and canonical huffman for values.

6. Tighter coding & versioning.

   - Make sure all records and Externalizable interfaces are versioned.

	 - (*) ReadOnlyNode/Leaf Coding.

	     Note: Any of these coding schemes could wind up with a
	     bit boundary which needs to be byte aligned.  See how
	     this is done in the CanonicalHuffmanRabaCoder.

	   - Code the min/max of the version timestamp (add to the
             API).
	   
  	   - Code the version timestamp[].

	     A good coding scheme might be the minimum timestamp plus
             a minimum width coding for the remainder for each
             timestamp.

	   - Code the childAddr[].

	     A good scheme might be identical to the version timestamp
	     scheme since the childAddr[] will tend to have a shared
	     prefix.

	     For an index segment, the childAddr[] values will also be
	     strictly increasing.

	   - Code the childEntryCount[].

	     A similar scheme might apply here.  Since the B+Tree is
	     balanced the #of spanned tuples for each child will be
	     roughly comparable, hence substracting out the minimum
	     could save space.

       - (***) Performance tuning:

	 AbstractRabaCoderTestCase#main(String[])

	     - There are definately optimizations which could be made
	       in the front-coded byte[], mainly dealing with
	       recomputing the length of a byte[] entry, but also with
	       copying the data efficiently onto an OutputStream.

	     - Memory analyzer.  How is the in-memory representation
               now as compared to the trunk?

	 (***) The canonical huffman raba coder appears to be VERY
	       slow when compared to the other implementations.  The
	       problem appears to be InputBitStream.

	       Modify InputBitStream to natively use the backing
	       (byte[], off, len), which should be much faster.

	       This would be a new ctor.  position := 0.  pos := off.
	       avail := len.

	       AbstractFixedByteArrayBuffer would use that ctor for
	       its getInputBitStream() method.

nops=400000, size=256, ncoders=5
com.bigdata.btree.raba.codec.SimpleRabaCoder@ee6ad6 : elapsed=17231, recordLength=3032
com.bigdata.btree.raba.codec.FrontCodedRabaCoder@3ca56f{ratio=2} : elapsed=18219, recordLength=2061
com.bigdata.btree.raba.codec.FrontCodedRabaCoder@ca2076{ratio=8} : elapsed=18583, recordLength=1910
com.bigdata.btree.raba.codec.FrontCodedRabaCoder@1d451d2{ratio=32} : elapsed=24614, recordLength=1889
com.bigdata.btree.raba.codec.CanonicalHuffmanRabaCoder@19c4844 : elapsed=75892, recordLength=1886

nops=200000, size=256, ncoders=5
op	count	nanos	%time	ops/ms
length	1375	2514	0.19%	546,937.152
get   	55160	1676	0.12%	32,911,694.511
copy  	27425	1676	0.12%	16,363,365.155
search	82880	2794	0.21%	29,663,564.782
itr   	27556	124038	9.16%	222,157.726
recode	5604	1221664	90.20%	4,587.186
com.bigdata.btree.raba.codec.SimpleRabaCoder@1cc0a7f : elapsed=11796, recordLength=3053
op	count	nanos	%time	ops/ms
length	1396	2515	0.17%	555,069.583
get   	55093	1676	0.12%	32,871,718.377
copy  	27413	1676	0.12%	16,356,205.251
search	82862	3073	0.21%	26,964,529.775
itr   	27648	122641	8.43%	225,438.475
recode	5587	1322793	90.95%	4,223.639
com.bigdata.btree.raba.codec.FrontCodedRabaCoder@169df00{ratio=2} : elapsed=12925, recordLength=2101
op	count	nanos	%time	ops/ms
length	1453	2514	0.17%	577,963.405
get   	55108	2514	0.17%	21,920,445.505
copy  	27435	1676	0.11%	16,369,331.742
search	83059	3073	0.21%	27,028,636.512
itr   	27330	123759	8.31%	220,832.424
recode	5615	1354921	91.03%	4,144.153
com.bigdata.btree.raba.codec.FrontCodedRabaCoder@52744{ratio=8} : elapsed=12544, recordLength=1927
op	count	nanos	%time	ops/ms
length	1417	2794	0.14%	507,158.196
get   	55557	3631	0.18%	15,300,743.597
copy  	27557	3073	0.15%	8,967,458.51
search	82257	3073	0.15%	26,767,653.759
itr   	27801	122641	6.01%	226,686.019
recode	5411	1903873	93.37%	2,842.101
com.bigdata.btree.raba.codec.FrontCodedRabaCoder@773a1{ratio=32} : elapsed=15283, recordLength=1902
op	count	nanos	%time	ops/ms
length	1399	2235	0.03%	625,950.783
get   	55129	2514	0.04%	21,928,798.727
copy  	27774	1956	0.03%	14,199,386.503
search	82998	10057	0.15%	8,252,759.272
itr   	27091	503974	7.68%	53,754.757
recode	5609	6044344	92.07%	927.975
com.bigdata.btree.raba.codec.CanonicalHuffmanRabaCoder@9b04ac : elapsed=50066, recordLength=1916


       - Canonical Huffman.

          - Add unit tests for keys NOT present in the raba.

	  - Done. Special case when nsymbols==1 (dsiutils bug).

       - Done. Refactor the FastRDFValueSerializer (no test suite yet).

	   (***) FastRDFValueCompression MAY be broken due to the
		 changes to BytesUtil to use the same big endian
		 format for bit flags as OutputBitStream.

       - RDF Front Coding or Canonical Huffman Leaf Key Coders.

         This would apply only to the leaves, where we always have
         full length keys.  Either front-coding or canonical huffman
         coding would work as long as they operating on long[] rather
         than byte[].

         - Front-coding should already be pretty efficient in space
           and time and we can always use the byte[] based huffman
           coder for RDF keys.

         - Generalize the CanonicalHuffmanRabaCoder to allow for
           non-byte values (char[], int[], long[]).  This should be
           very compact when the alphabet corresponds to the natural
           data type size.  E.g., RDF long[3] or long[4] keys.

       - ByteBuffer is too slow.
	   
	     There are two very frequent batch operations which we use
	     where we are forced to use byte-at-a-time processing with
	     ByteBuffer.  These are (1) compareBytes(), which is an
	     unsigned byte[] compare; and (2) writeOn(OS), which is
	     used to copy byte[]s out of a raba.

	     DO NOT use ByteBuffer#asReadOnlyBuffer() since it hides
	     the backing byte[].

	     DO tunnel through to the backing {byte[], off, len} for a
	     heap ByteBuffer and use that data in place of the
	     ByteBuffer within IRabaCoder#decode()
	     implementations. Possibly extend the BackingBuffer
	     interface and implementations for this purpose.

	     Note: Test suites will have to exercise both with and w/o
	     the byte[] tunnel.  The easy way to do that is to use
	     ByteBuffer#asReadOnlyBuffer() since that will hide the
	     array().

	     Note: (*****) Methods which interact with a ByteBuffer
	     MUST NOT change its position or limit. If they do then
	     ALL methods which touch the buffer need to be
	     synchronized so NONE of them can have a concurrent read
	     during which the position/limit has been transiently
	     modified.  The culprits here are the bulk byte transfer
	     methods ByteBuffer#get() and ByteBuffer#put().  This is
	     really a huge limitation on the use of a ByteBuffer for
	     concurrent access to a read-only data structure!!!!

	     Note: We will still have a C heap ByteBuffer for the
             IndexSegment nodes region, but that might be the only
             place they show up.

	     Note: If we are doing record level compression then be
             sure to decompress to an expandable per-worker task
             byte[].  Once decompressed, create an exact fit byte[]
             and wrap it as a ByteBuffer.

	     Note: If the InputBitStream backed by a ByteBuffer is
	     buffered, then make sure that the buffer is a reasonable
	     size.  We could do this ourselves with a
	     BufferedInputStream wrapping the ByteBufferInputStream.

	     This could be part of how we reconcile the DataSerializer
	     and the IRabaCoder APIs.

       - (***) Add global read retention queue and weak value hash map
         based lookup per store of the retained nodes.

       - Examine GC problems with lots of B+Tree nodes buffered.  Try
         parallel old GC and the G1 collector.

	 Tony Printezis [Antonios.Printezis@sun.com] 10G+ heaps will
	 most likely create unpleasant Full GC pauses!  :-) But, yes,
	 evaluate Parallel Old to first see how it works. And we can
	 take it from there...

       - Done. Converge on the standard fastutil and dsi dependencies,
         removing their variants from the lgpl-utils module and move
         the lgpl-utils module into the bigdata project.

	 - added dsiutils-1.0.10 (369k), replacing lgpl-utils.

	 - updated fastutils-5.1.4 to fastutils-5.1.5.

	 - colt-1.2.0 is unchanged.

	 - Package change for InputBitStream and OutputBitStream from
	   it.unimi.dsi.mg4j.io to it.unimi.dsi.io.

	 - Package change for Coder, Decoder, and HuffmanCodec from
           it.unimi.dsi.mg4j.compression to it.unimi.dsi.compression.

	 - Method change for Codec.getDecoder() to Codec.decoder().

	 - Method change for PrefixCodec.getCoder() to
           PrefixCodec.coder().

	 - (*) Verify use of the correct (new) ctor for
           OutputBitStream.

	   Verify new uses of FastByteArrayOutputStream in compression
	   code and of the OutputBitStream.  Make sure that we are not
	   allocating huge buffers when compared to the actual
	   requirements.

	 - (*) Verify requirement for additional dsiutil dependencies.

	 - Feedback to dsiutils on changes introduced in the
           CustomByteArrayFrontCodedList.  These changes are probably
           too far reaching to expect that they will make it back into
           dsiutils.

	 - (*) Replace it.unimi.dsi.mg4j.util.BloomFilter2 with
	   it.unimi.dsi.util.BloomFilter?

	   Request changes in dsi-utils so we can use that BloomFilter
	   class directly.

         - (*) Use autojar to extract only the necessary classes from
           fastutils.  Possibly do this as part of the lgpl-utils
           build.

	 - (*) Make sure that the ant script builds the lgpl-utils jar
           first and then links with it to reference the classes
           inside of that jar.

Access:

  - Efficient search on compressed keys (prefix, canonical huffman, or
    hu-tucker).

  - Random access to tuples in a leaf (for keyAt(), etc).  This does
    not have to be as efficient as the key search which is much more
    heavily used.

  - Fast extraction of the value, timestamp, and delete marker for a
    tuple.

  - Fast scan of tuples in either direction, materializing keys,
    values, timestamps, and/or delete flags.

  - Efficient merged iterator on two or more leaves with optional
    filter (used for storing asynchronous writes in a compressed
    form).

  - Parallel iterator scan at the ClientIndexView level (process the
    index partitions in parallel).  This should be implemented using a
    BlockingBuffer on the client.  The iterator is distributed in
    parallel to the DS.  Chunks are added to the client's blocking
    buffer as they arrive.  The max parallelism of the client index
    view should be respected for this operation.  The only real catch
    will be handling redirects for the unisolated index view.
    
    This flag should be used in the high-level query and turned on
    only for purposes where we can tolerate the partial delivery order
    of the parallel iterator.  In fact, we can probably tolerate that
    form most use cases but some care still needs to be excercised
    when enabling this.

    Compare performance with this flag against performance without the
    flag and against differing degrees of limited parallelism.

  - Fix the sparse row store splitter (must respect the logical row
    boundaries).

  - Raise AbstractBTreeTupleCursor::rangeCheck(final L leaf, final int
    index) into the ILeafData API?  The method signature would have to
    be changed to accept the optional fromKey and toKey constraints
    and the test would then be answered by the ILeafData
    implementation.

  - Strongly type the AbstractBTree's IAutoBox interface for the
    key/value types and link that with the ITupleSerializer types.

B+Tree node buffering.

       - set of large native buffers.

       - hash map <UUID,addr> : <buffer#,offset>

       - strict unbounded LRU for eviction.

       - guard for node/leaf data access (latch) protect critical
         regions using a nested inc/dec approach and ensures that the
         data mapped to a given buffer and offset is neither moved nor
         evicted during access.  avoid guards across recursive methods
         since that can lead to deadlock with compact.

       - compact buffers - must not deadlock with record access as
         mediated by guards.

       - bitmap might help to write compact logic.

       - use an allocation unit for the slots in the buffer which is a
         multiple of some power of two, e.g., 128 or 256 bytes per
         slot.  all slots for a given record are contiguous.

       - Could use the same buffers for the index segment nodes
         region, but this raises the opportunity for fragmentation
         significantly.  This is a kind of double-buffering.  If the
         nodes region is not compressed but only the individual nodes,
         then the uncompressed nodes would also be buffered on a
         strict LRU basis.

       - Compact should maintain a free region at the end of each
         buffer, prefer to fill a buffer to capacity, and perform the
         minimum movement possible to fill gaps (incremental) or
         simply compact onto a different buffer (batch).  With buffer
         and buffer to buffer copy should be DMA.  The challenge is to
         hold all necessary locks so that we can relocate the records
         within or across buffers.

	 You know, this variable length record compacting thing is
	 exactly the problem that Java memory management is already
	 solving.  What I need to do is use normal java memory (since
	 it leaks native byte buffers) and impose a soft maximum on
	 the desired #of bytes allocated to B+Tree nodes, clear
	 references for nodes in an LRU pattern until the weakly
	 referenced byte count is LTE the soft maximum, and also track
	 the bytes recovered when weak references are cleared so I
	 know the min/max allocation.  There is some overhead for both
	 byte[] and a ByteBuffer, but not that much when compared to
	 the overhead of the byte[][] keys and values and the other
	 node/leaf metadata.

       - Reading from the store already returns a ByteBuffer on the
         heap.

       - The DiskOnlyStrategy already defines a read cache.  That
         could be refactored as an IIndexStore scope B+Tree node
         cache.  The records would be read from the underlying store
         and the obtained ByteBuffer would be entered into the cache.
         Since the cache is global, the key would have to be
         <UUID,addr> or <File,addr>.  The records should remain
         available until purged by the LRU policy even if the backing
         store is closed (in order words, do not attempt to remove
         records from the cache when the store is closed).  Purging
         records on delete of the backing store could be considered,
         but it is unlikely to be much benefit.  One way to do that is
         to scan from the LRU position backwards an arbitrary distance
         clearing references for any nodes associated with a deleted
         store.
	 
         The cache could also support write through so an newly
         persisted B+Tree node would either be buffered (if write
         through was enabled) or discarded (if it was not enabled).
	 
         While newly persisted nodes are unlikely to be re-read within
         the context of an ACID operation, they are relatively likely
         to be re-read within the context of following operation.

	 We could also install records into the cache when building an
	 index segment, but again that might not be a benefit.

 * <pre>

   version:uint6     The version identifier for this record format (6 bits,
		     which allows for 64 format revisions).

   bitCodedSymbols:1 A bit flag whose value is 1 iff the symbols are given as
		     a packed symbol[] and 0 if they are given as a 256 bit
		     vector (32 bytes).

   nsymbols:uint9    There are at most 256 distinct symbols, which are the
                     distinct possible byte values (9 bits, which allows
                     for an empty leaf or node with no byte values used as
		     well as a leaf or node with all 256 byte values used).

   -- note: at this point the record is byte aligned --

   symbol:byte[]     The packed symbol array -or- an alternative coding which
   symbol:bit[256]   is used if there are GT 32 distinct symbols to be coded
                     since it a more compact representation.

		     O_symbols := 16 bits.

   -- note: at this point the record is byte aligned --

   bitLengthBits:uint8
		     The width in bits of the integers used to code the 
		     bitLength[].

		     The bit offset to this field is either ((2+nsymbols)*8) (if
		     the symbols are coded as a packed byte[]) or ((2+32)*8) (if
		     the symbols are coded as a 256 bit vector).

   codeOffsetBits:uint8
	             The width in bits of the integers used to code the
	             codeOffset[].  If this is ZERO (0), then the codeOffset[]
		     was not stored and the coded values must be decoded with
		     a sequential scan of the codeWord[].

		     O_codeOffsetBits := O_bitLengths + 8;

   sumCodedValueBitLengths:uint32
		     The sum of the bit lengths of the coded values.  This is
		     a 32bit unsigned integer, which is sufficient to code up
		     bit lengths of up to 512MB.  This field IS NOT present 
		     if the codeOffsetBits is ZERO (0) since the field is only
		     used to compute the bit offset of the codeOffset[].

		     O_sumCodedValueBitLengths := O_bitLengths + 16;

   -- note: at this point the record is byte aligned --
   
   nulls[]	     A vector of [nvalues] bit flags.  A flag is a ONE (1) iff
		     the corresponding byte[] in the logical byte[][] was a 
		     null.  A null is coded as a sequence of ZERO (0) code 
		     words.  However, empty byte[]s are also permitted for
		     B+Tree values (but not for B+Tree keys).  Therefore you
		     MUST test the nulls[] to distinguish between a null byte[]
		     and an empty byte[].

		     O_nulls :=  O_bitLengthBits + 8 + 8 + (codeOffsetBits==0?0:32);
  
   bitLength[]	     The delta in the bit length of each code word from the
		     previous code word.  There is one code word pre symbol.  
		     The code words MUST be a canonical huffman code, which
		     implies that the code words are arranged in a
		     non-decreasing order.  This is used to reconstruct the
		     decoder when processing the record.  The combination of
		     a canonical huffman code and the bitLength[] allows us 
		     to generate the decoder without any additional information.

		     O_bitLength[] := O_nulls + nvalues

   codedValue:bit[]  The coded values given as a sequence of code words.  The
		     offsets of each coded value in this array are given
		     directly by the codeOffset[].

		     O_codedValue[] := O_bitLength[] + (nsymbols*bitLengthBits)

   codeOffset[]      Offset to the start of each code from the start of the
                     codedValue[].  While the delta in the offsets could be
		     represented more efficiently, the offsets are represented
		     directly so that we may avoid reading the entire codeOffset[]
		     into memory.  This array is present iff codeOffsetBits is GT 
		     ZERO.

		     O_codeOffset[] := O_codedValue[] + sumCodedValueBitLengths

 </pre>

codeWord[]=[1101, 000, 001, 010, 1110, 1111, 011, 100, 101, 1100]
longCodeWord[]=[13, 0, 1, 2, 14, 15, 3, 4, 5, 12] (base10 version of the codeWords)

length[]=[3, 3, 3, 3, 3, 3, 4, 4, 4, 4]
symbol[]=[1, 2, 3, 6, 7, 8, 9, 0, 4, 5]

Reordering of the symbol2byte[] to match the code book:
      0    1    2    3    4    5    6    7    8    9
in: [99, 101, 105, 107, 109, 110, 111, 112, 114, 115]
out:[101, 105, 107, 111, 112, 114, 115, 99, 109, 110]

codeWord: 000, symbol=0, value=99 (c)
codeWord: 001, symbol=1, value=101 (e)
codeWord: 010, symbol=2, value=105 (i)
codeWord: 011, symbol=3, value=107 (k)
codeWord: 100, symbol=4, value=109 (m)
codeWord: 101, symbol=5, value=110 (n)
codeWord: 1101, symbol=6, value=111 (o)
codeWord: 1110, symbol=7, value=112 (p)
codeWord: 1111, symbol=8, value=114 (r)
codeWord: 1100, symbol=9, value=115 (s)


Note: When sending this record format, you must also transmit both the
#of values in the logical byte[][] coded by the record and the #of
bits in the record itself.

- The Huffman decoder ctor requires the code words arranged into a
  non-decreasing order by code length paired with the symbols arranged
  in a correlated order.  This can be done either in the serialized
  format or when read build the decoder from the record.  We do this
  when we build the decoder because that allows us to use a more
  compact representation of the symbols in the serialized record and
  because the code word lengths are implicit in the code words due to
  the prefix property.

  - The codeWordLength[] might be more compact with run length
    encoding.  The article suggests [min, max, #of codes having each
    length between min and max].  E.g., a variant of runLength
    encoding.

    Try fibonacci codes for the dictionary entries when N LT 64?

  - dsiutils

    1. Modified HuffmanCodec to expose the symbol[].
    
    2. HuffmanCodec will fail if it is given a zero length
       frequency[].  The exception is thrown out of the
       CanonicalFast64CodeWordDecoder ctor.

    3. Done. The ByteArrayFrontCodedList can not directly use the data
       in place in a ByteBuffer or IBS.  Its serialization will have
       to be modified to support that application.

    4. Build lgpl-utils as a separate JAR and link it as a dependency.

  - Implement at least:

    None w/ search.  The byte[][] is written out w/o coding.

       Write and test.  We should be able to do search() on this as
       well as random lookup.  [This could be implemented using
       FrontCodedCoder if it were modified such that the ratio is
       dynamically set to the #of items in the record.]

    Prefix coding w/ search

       Done.  Continue the refactor so we can use search() on a raba
       and test it in this context.  See if I can make some progress.

       Still need to reconcile PrefixSerializer (IDataSerializer,
       streams) with FrontCodedDataCoder (IDataCoder, records).

    Canonical huffman coding w/ search.

       This requires us to code zero frequency count byte values are
       used so any byte[] may be coded.

    Canonical huffman coding w/o search.

       Only the byte values having non-zero frequency counts are
       coded.
    
	Note: I am still having troubles with the canonical huffman
	code.  It is possible to rebuild the Decoder from the
	frequency[].  While that is quite a hack, it might be better
	than banging my head on the wall further.  One problem with
	that approach (at least as taken by MikeP) is it is NOT a
	canonical code which results.  Another is that different data
	are written onto the persistent record so the serialized
	formats would not be compatible after a fix.

    Hu-Tucker w/ search.

        I am disincluded to implement this coding since the algorithm
	implemented to find the code is quadratic in the size of the
	code space and we would need to code all 256 byte values each
	time in order to use this for search.

    FastRDFValueCompression

	Modify implement IDataCoder so as to permit in place access to
	the coded values.

  - Try patch to HuffmanSerializer in the trunk w/o StringBuilder and
    w/o freq[].  Instead, send the code word bit lengths and either
    use canonical huffman codes or also send the symbols/symbol
    indices.

- extend IRandomAccessByteArray interface for keys, adding a search()
  method.  

  Refactor to declare optional operations which throw an
  UnsupportedOperationException and then clean up the interface
  hierarchy.

- Both Leaf and Node should provide access methods to wrap their keys
  as a Raba and their values as a Raba.  We may want caching for those
  wrapped views.

  - Implement MutableLeafData by refactoring Leaf.

  - Implement MutableNodeData by refactoring Node.

- Align IDataSerializer with IRabaEncoder and IRabaDecoder.  Modify
  various things to take advantage of random access to the coded
  byte[]s rather than decoding everything first.

- IBS over ByteBufferInputStream might be slow.  Look into that.

------------------------------------------------------------
-- Global B+Tree node/leaf buffering, or maybe record ------
-- buffer across all stores for a DS instance --------------
------------------------------------------------------------

Paul,

We are looking to tune the use of RAM to buffer B+Tree nodes and
leaves for the scale-out database.  I would expect that at least 50%
of the RAM should be dedicated to this task on each JVM instance.  I
am trying to decide on the approach to managing this memory.  I am
considering either (A) a large ByteBuffer on the native heap and
managing the memory myself; or (B) allocating a large number of
perfect fit ByteBuffers on the Java heap and letting the JVM manage
them.

As someone with roots in C, my first inclination is to allocate a
large ByteBuffer on the native heap and then manage the memory within
that buffer myself.  We made the decision to impose a constraint only
on the B+Tree branching factor, but not on the size of the B+Tree node
or leaf representation on the disk. This gives us perfect fit records
on the disk so the on disk image corresponds to perfect utilization
within the B+Tree even if nodes and leaves are not 100% full. As a
consequence, the B+Tree nodes and leaves use variable length records
rather than fixed size pages.  Given that, the buffer memory
management problem would probably rely on mechanisms such as those
already used by the JVM to handle Java allocations.  However, Java
already has great garbage collectors.

So it occurs to me that we might be better off allocating ByteBuffer's
on the Java heap and then letting the JVM manage the memory on our
behalf.  I believe that we could achieve the desired degree of
buffering using a hash map with weak reference values combined with a
hard reference retention cache using an LRU eviction policy.  If we
cleared hard references from the LRU position whenever the total of
the buffered byte[]s exceeded the desired percentage of the JVM heap,
then those byte[]s would be automatically discarded once they were
only weakly reachable.

While I would like to use native ByteBuffers for these data, but it is
my understanding that Java "leaks" ByteBuffer's whose backing storage
is on the native heap.  Also, it is my expectation that a large part
of the memory savings will come from operating on a binary image of
the B+Tree node/leaf rather than de-serializing the (de-compressed)
image read from the disk.  This will allow us to get rid of the
(de-)serialization time and the entailed memory demand on the Java
data structures used to model the nodes and leaves.  Running with the
expanded (deserialized Java objects) version of the B+Tree nodes and
leaves we typically buffer .3G of data on disk in RAM on a server with
a 12G JVM process.  I expect that we could increase that to 6G with
either of the proposed approaches, which would of course drammatically
improve the database performance by reducing IO Wait and
(de-)serialization costs.

I would appreciate it if you have any insight on this question which
you could share.

Thanks,

-bryan

import net.jini.jeri.BasicILFactory;
import net.jini.jeri.BasicJeriExporter;
import net.jini.jeri.tcp.TcpServerEndpoint;

import net.jini.discovery.LookupDiscovery;
import net.jini.core.discovery.LookupLocator;
import net.jini.core.entry.Entry;
import net.jini.lookup.entry.Name;
import net.jini.lookup.entry.Comment;
import net.jini.lookup.entry.Address;
import net.jini.lookup.entry.Location;
import net.jini.lookup.entry.ServiceInfo;
import net.jini.core.lookup.ServiceTemplate;

import java.io.File;

import com.bigdata.util.NV;
import com.bigdata.journal.BufferMode;
import com.bigdata.jini.lookup.entry.*;
import com.bigdata.service.IBigdataClient;
import com.bigdata.service.jini.*;
import com.bigdata.service.jini.lookup.DataServiceFilter;
import com.bigdata.service.jini.master.ServicesTemplate;
import com.bigdata.jini.start.config.*;
import com.bigdata.jini.util.ConfigMath;

import org.apache.zookeeper.ZooDefs;
import org.apache.zookeeper.data.ACL;
import org.apache.zookeeper.data.Id;

// imports for various options.
import com.bigdata.btree.IndexMetadata;
import com.bigdata.rdf.sail.BigdataSail;
import com.bigdata.rdf.spo.SPORelation;
import com.bigdata.rdf.spo.SPOKeyOrder;
import com.bigdata.rdf.lexicon.LexiconRelation;
import com.bigdata.rdf.lexicon.LexiconKeyOrder;
import com.bigdata.rawstore.Bytes;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeUnit.*;

/*
 * This is a sample configuration file for a bigdata federation.
 * 
 * Note: The original file is a template.  The template contains parameters
 * of the form @XXX@.  The values for those template parameters are specified
 * in the build.properties file when you use ant to install bigdata.
 * 
 * Note: This file uses the jini configuration mechanism.  The syntax
 * is a subset of Java.  The properties for each component are grouped
 * within the namespace for that component.
 *
 * See the net.jini.config.ConfigurationFile javadoc for more
 * information.
 */

/*
 * A namespace use for static entries referenced elsewhere in this
 * ConfigurationFile.
 */
bigdata {

    /**
     * The name for this federation.
     *
     * Note: This is used to form the [zroot] (root node in zookeeper
     * for the federation) and the [serviceDir] (path in the file
     * system for persistent state for the federation).
     *
     * Note: If you will be running more than one federation, then you
     * MUST use unicast discovery and specify the federation name in
     * the [groups].
     */
    static private fedname = "@FED@";

    /**
     * Where to put all the persistent state.
     */
    static private serviceDir = new File("@LAS@");

    /**
     * A common point to set the Zookeeper client's requested
     * sessionTimeout and the jini lease timeout.  The default lease
     * renewal period for jini is 5 minutes while for zookeeper it is
     * more like 5 seconds.  This puts the two systems onto a similar
     * timeout period so that a disconnected client is more likely to
     * be noticed in roughly the same period of time for either
     * system.  A value larger than the zookeeper default helps to
     * prevent client disconnects under sustained heavy load.
     */
    // jini
    static private leaseTimeout = ConfigMath.m2ms(60);// 20s=20000; 5m=300000; 
    // zookeeper
    static private sessionTimeout = (int)ConfigMath.m2ms(10);// was 5m 20s=20000; 5m=300000; 

    /*
     * Example cluster configuration.
     *
     * Data services are load balanced.  Index partitions will be
     * moved around as necessary to ensure hosts running data
     * service(s) are neither under nor over utilized.  Data services
     * can be very resource intensive processes.  They heavily buffer
     * both reads and writes, and they use RAM to do so.  They also
     * support high concurrency and can use up to one thread per index
     * partition.  How many cores they will consume is very much a
     * function of the application.
     *
     * Zookeeper services use a quorum model.  Always allocate an odd
     * number.  3 gives you one failure.  5 gives you two failures.
     * Zookeeper will sync the disk almost continuously while it is
     * running.  It really deserves its own local disk.  Zookeeper
     * also runs in memory.  Since all operations are serialized, if
     * it starts swapping then peformance will drop through the floor.
     *
     * Jini uses a peer model.  Each service registers with each
     * registrar that it discovers.  Each client listeners to each
     * registrar that it discovers.  The default jini core services
     * installation runs entirely in memory (no disk operations, at
     * least not for service registration). A second instance of the
     * jini core services provides a safety net.  If you are using
     * multicast then you can always add another instance.
     */

	/* Declare the hosts.  This provides indirection for planning
	 * purposes.
	 *
	 * The summary notation is: cores@GHZ/cache x RAM x DISK
	 */
	static private h0 = "192.168.20.26"; // 4@3ghz/1kb x 4GB x 263G
	static private h1 = "192.168.20.27"; // 4@3ghz/2kb x 4GB x 263G 
	static private h2 = "192.168.20.28"; // 4@3ghz/1kb x 4GB x 64G
	
	/* Note: this configuration puts things that are not disk intensive
	 * on the host with the least disk space and zookeeper.
	 */
    static private lbs = h2; // specify as @LOAD_BALANCER_HOST@ ?
    static private txs = h2;
    static private mds = h2;

    // 1+ jini servers
    static private jini1 = h2;
    //static private jini2 = h1;
    static private jini = new String[]{ jini1 }; //,jini2};

    // Either 1 or 3 zookeeper machines (one instance per).
    // See the QuorumPeerMain and ZooKeeper configurations below.
    static private zoo1 = h2;
    //static private zoo2 = h1;
    //static private zoo3 = h2;
    static private zoo = new String[] { zoo1 }; // ,zoo2,zoo3};

    // 1+ client service machines (1+ instance per host).
    static private cs0  = h2;

    // 1+ data service machines (1+ instance per host).
    static private ds0  = h0;
    static private ds1  = h1;

    // client servers
    static private cs = new String[] {
    	cs0 //, ...
    };

    // The target #of client servers.
    static private clientServiceCount = 2; // was 8
    static private maxClientServicePerHost = 2; // was 2

    // data servers
    static private ds = new String[]{
	   ds0, ds1 //, ...
 	   };

    // The target #of data services.
    static private dataServiceCount = 4;// was 20

    // Maximum #of data services per host (remember to disable Remote JMX)
    static private maxDataServicesPerHost = 2;//1;//2
    // @todo also specify k (replicationCount)

    /* Sets the initial and maximum journal extents.
     * 
     * Note: 200M is good for 32bit machines.  Higher end server machines with
     * more on-disk cache work well with 1000M (1GB).
     */
    static private journalExtent = ConfigMath.multiply(200, Bytes.megabyte);

    /**
     * A String[] whose values are the group(s) to be used for discovery
     * (no default). Note that multicast discovery is always used if
     * LookupDiscovery.ALL_GROUPS (a <code>null</code>) is specified.
     */

    // one federation, multicast discovery.
    //static private groups = LookupDiscovery.ALL_GROUPS;

    // unicast discovery or multiple federations, MUST specify groups.
    static private groups = new String[]{bigdata.fedname};

    /**
     * One or more unicast URIs of the form <code>jini://host/</code>
     * or <code>jini://host:port/</code> (no default).
     *
     * This MAY be an empty array if you want to use multicast
     * discovery <strong>and</strong> you have specified the groups as
     * LookupDiscovery.ALL_GROUPS (a <code>null</code>).
     */
    static private locators = new LookupLocator[] {

	// runs jini on the localhost using unicast locators.
	//new LookupLocator("jini://localhost/")
	
	// runs jini on two hosts using unicast locators.
	new LookupLocator("jini://"+jini1),
	//new LookupLocator("jini://"+jini2),

    };

    /**
     * The policy file that will be used to start services.
     */
    private static policy = "@POLICY_FILE@";

    /**
     * log4j configuration file (applies to bigdata and zookeeper).
     *
     * Note: The value is URI!
     *
     * Note: You should aggregate all of the log output to a single
     * host.  For example, using the log4j SocketAppender and the
     * SimpleNodeServer.
     */
    log4j = "@LOG4J_CONFIG@";

    /**
     * java.util.logging configuration file (applies to jini as used
     * within bigdata).
     *
     * Note: The value is a file path!
     */
    logging = "@LOGGING_CONFIG@";

    /*
    private static host = ConfigUtil.getHostName();
    private static port = "8081";
    private static jskdl = " http://" + host + ":" + port + "/jsk-dl.jar";
    */

}

/*
 * Service configuration defaults.  These can also be specified on a
 * per service-type basis.  When the property is an array type, the
 * value here is concatenated with the optional array value on the per
 * service-type configuration.  Otherwise it is used iff no value is
 * specified for the service-type configuration.
 */
com.bigdata.jini.start.config.ServiceConfiguration {

    /* 
     * Default java command line arguments that will be used for all
     * java-based services
     *
     * Note: [-Dcom.sun.jini.jeri.tcp.useNIO=true] enables NIO in
     * combination with the [exporter] configured below.
     */
    defaultJavaArgs = new String[]{
	"-server",
	"-ea",
	//"-Xmx2G",
	"-Dcom.sun.jini.jeri.tcp.useNIO=@USE_NIO@",
	"-Djava.security.policy="+bigdata.policy,
	"-Djava.util.logging.config.file="+bigdata.logging,
	"-Dcom.bigdata.counters.linux.sysstat.path=@SYSSTAT_HOME@"
    };

    /* Default path for service instances and their persistent
     * data. This may be overriden on a per service-type basis. 
     *
     * Note: For logical services that support failover, the concrete
     * service directory is assigned dynamically when a physical
     * service instance is created.
     */
    serviceDir = bigdata.serviceDir;

    /* The bigdata services default logging configuration (a URI!)
     */
    log4j = bigdata.log4j;

    /*
     * Set up some default properties values that will be inherited
     * (copy by value) by all clients and services started using this
     * configuration file.
     */
    properties = new NV[] {

    /* 
     * Each JiniClient (and hence all bigdata services) can run an
     * httpd that will expose performance counters for the service and
     * the host on which it is running.  This property specifies the
     * port for that httpd service.  Valid values are port number,
     * zero (0) for a random open port, MINUS ONE (-1) to disable the
     * httpd service.
     */
    //new NV(IBigdataClient.Options.HTTPD_PORT, "-1"),

    /*
     * Option to disable collection of performance counters for the
     * host on which the client or service is running.
     *
     * Note: The load balancer relies on this information!
     */
    //new NV(IBigdataClient.Options.COLLECT_PLATFORM_STATISTICS,"false"),

    /* Option to disable collection of performance counters on the
     * queues used internally by the client or service.
     *
     * Note: The load balancer relies on this information!
     */
    //new NV(IBigdataClient.Options.COLLECT_QUEUE_STATISTICS,"false"),

    /* Option controls how many times a client request will be
     * reissued on receiving notice that an index partition locator is
     * stale.  Stale locators arise when an index partition is split,
     * moved, or joined.
     *
     * Note: This option needs to be larger if we are aggressively
     * driving journal overflows and index partitions splits during
     * the "young" phase of a data service or scale-out index since a
     * LOT of redirects will result.
     */
    new NV(IBigdataClient.Options.CLIENT_MAX_STALE_LOCATOR_RETRIES,"1000"),

    };

}

/**
 * JoinManager options.
 *
 * Note: These options must be copied into the service.config (to
 * specify the service lease timeout) as well as used by the client
 * (which uses this file directly).
 */
net.jini.lookup.JoinManager {

    // The lease timeout for jini joins.
    maxLeaseDuration = bigdata.leaseTimeout;

}

/**
 * Jini service configuration.
 */
jini {

    args = new String[] {

        "-Djava.security.policy="+bigdata.policy,
        "-Djava.util.logging.config.file="+bigdata.logging,

    };

    /**
     * The main jini configuration file.  This file contains a
     * NonActivatableServiceDescriptor[]. The elements of that array
     * describe how to start each of the jini services.
     */
    configFile = new File("@JINI_CONFIG@");

    /**
     * The #of instances to run.
     *
     * Note: A jini service instance may be started on a host if it is
     * declared in [locators].  If locators is empty, then you are
     * using multicast discovery.  In this case an instance may be
     * started on any host, unless [constraints] are imposed.  In any
     * case, no more than [serviceCount] jini services will be started
     * at any given time.  This is checked against the #of discovered
     * instances.
     */
    serviceCount = 1;

}

/**
 * Zookeeper server configuration.
 */
org.apache.zookeeper.server.quorum.QuorumPeerMain {

    /* Directory for zookeeper's persistent state.  The [id] will be
     * appended as another path component automatically to keep
     * instances separate.
     */
    dataDir = new File(bigdata.serviceDir,"zookeeper");

    /* Optional directory for the zookeeper log files.  The [id] will
     * be appended as another path component automatically to keep
     * instances separate.
     * 
     * Note: A dedicated local storage device is highly recommended
     * for the zookeeper transaction logs!
     */
    //dataLogDir=new File("/var/zookeeper-log");

    // required.
    clientPort=2181;

    tickTime=2000;

    initLimit=5;

    syncLimit=2;

    /* A comma delimited list of the known zookeeper servers together
     * with their assigned "myid": {myid=host:port(:port)}+
     *
     * Note: You SHOULD specify the full list of servers that are
     * available to the federation. An instance of zookeeper will be
     * started automatically on each host running ServicesManager that
     * is present in the [servers] list IF no instance is found
     * running on that host at the specified [clientPort].
     * 
     * Note: zookeeper interprets NO entries as the localhost with
     * default peer and leader ports. This will work as long as the
     * localhost is already running zookeeper.  However, zookeeper
     * WILL NOT automatically start zookeeper if you do not specify
     * the [servers] property.  You can also explicitly specify
     * "localhost" as the hostname, but that only works for a single
     * machine.
     */
    // standalone
    //servers="1=localhost:2888:3888";
    // ensemble
    /**/
    servers =  "1="+bigdata.zoo1+":2888:3888"
//            + ",2="+bigdata.zoo2+":2888:3888"
//	    + ",3="+bigdata.zoo3+":2888:3888"
	    ;

    // This is all you need to run zookeeper.
    classpath = new String[] {
    	"@LIB_DIR@/apache/zookeeper-3.1.0.jar",
        "@LIB_DIR@/apache/log4j-1.2.15.jar"
    };

    /* Optional command line arguments for the JVM used to execute
     * zookeeper.
     *
     * Note: swapping for zookeeper is especially bad since the
     * operations are serialized, so if anything hits then disk then
     * all operations in the queue will have that latency as well.
     */
    args = new String[]{
	"-Xmx1G",
	/*
	 * Enable JXM remote management.
	 *
	"-Dcom.sun.management.jmxremote.port=9997",
	"-Dcom.sun.management.jmxremote.authenticate=false",
	"-Dcom.sun.management.jmxremote.ssl=false",
	 */
};

    // zookeeper server logging configuration (value is a URI!)
    log4j = bigdata.log4j;

}

/*
 * Zookeeper client configuration.
 */
org.apache.zookeeper.ZooKeeper {

    /* Root znode for the federation instance. */
    zroot = "/"+bigdata.fedname;

    /* A comma separated list of host:port pairs, where the port is
     * the CLIENT port for the zookeeper server instance.
     */
    // standalone.
    // servers = "localhost:2181";
    // ensemble
    servers =   bigdata.zoo1+":2181" // @TODO enable other instances.
//             + ","+bigdata.zoo2+":2181"
// 	    + ","+bigdata.zoo3+":2181"
	    ;

    /* Session timeout (optional). */
    sessionTimeout = bigdata.sessionTimeout;

    /* 
     * ACL for the zookeeper nodes created by the bigdata federation.
     *
     * Note: zookeeper ACLs are not transmitted over secure channels
     * and are placed into plain text Configuration files by the
     * ServicesManagerServer.
     */
    acl = new ACL[] {

	new ACL(ZooDefs.Perms.ALL, new Id("world", "anyone"))

    };

}

/*
 * Jini client configuration
 */
com.bigdata.service.jini.JiniClient {

    /* Default Entry[] for jini services.  Also used by the
     * ServicesManagerService as is.
     *
     * Note: A Name attribute will be added automatically using the
     * service type and the znode of the service instance.  That Name
     * will be canonical.  It is best if additional service names are
     * NOT specified as that might confuse somethings :-)
     *
     * Note: A Hostname attribute will be added dynamically.
     */
    entries = new Entry[] {
	// Purely informative.
	new Comment(bigdata.fedname),
    };

    groups = bigdata.groups;

    locators = bigdata.locators;

    // optional JiniClient properties.
    // properties = new NV[] {};

    /*
     * Overrides for jini SERVICES (things which are started
     * automatically) BUT NOT CLIENTs (things which you start by hand
     * and which read this file directly).
     *
     * The difference here is whether or not a service.config file is
     * being generated.  When it is, the jiniOptions[] will be
     * included in how that service is invoked and will operate as
     * overrides for the parameters specified in the generated
     * service.config file.  However, normal clients directly consume
     * this config file rather than the generated one and therefore
     * you must either specify their overrides directly on the command
     * line when you start the client or specify them explicitly in
     * the appropriate component section within this configuration
     * file.
     *
     * In practice, this means that you must specify some parameters
     * both here and in the appropriate component configuration. E.g.,
     * see the component section for "net.jini.lookup.JoinManager"
     * elsewhere in this file.
     */
    jiniOptions = new String[] {

	// The lease timeout for jini joins.
	"net.jini.lookup.JoinManager.maxLeaseDuration="+bigdata.leaseTimeout,

    };

}

/**
 * Options for the bigdata services manager.
 */
com.bigdata.jini.start.ServicesManagerServer {

    /*
     * This object is used to export the service proxy.  The choice
     * here effects the protocol that will be used for communications
     * between the clients and the service.
     */
    exporter = new BasicJeriExporter(TcpServerEndpoint.getInstance(0),
                                     new BasicILFactory()); 

    /*                                          
     * The data directory and the file on which the serviceID will be
     * written.
     *
     * Note: These properties MUST be specified explicitly for the
     * ServicesManager since it uses this as its Configuration file.
     * For other services, it generates the Configuration file and
     * will generate this property as well.
     */

    serviceDir = new File(bigdata.serviceDir,"ServicesManager");

    serviceIdFile = new File(serviceDir,"service.id");
    
    /* The services that will be started.  For each service, there
     * must be a corresponding component defined within this
     * configuration file.  For each "ManagedServiceConfiguration", an
     * entry will be made in zookeeper and logical and physical
     * service instances will be managed automatically.  For unmanaged
     * services, such as jini and zookeeper itself, instances will be
     * started iff necessary by the services manager when it starts
     * up.
	 */
    services = new String[] {
	
      	"jini",
      	"org.apache.log4j.net.SimpleSocketServer",
 		"org.apache.zookeeper.server.quorum.QuorumPeerMain",
  		"com.bigdata.service.jini.TransactionServer",
    	"com.bigdata.service.jini.MetadataServer",
    	"com.bigdata.service.jini.DataServer",
    	"com.bigdata.service.jini.LoadBalancerServer",
    	"com.bigdata.service.jini.ClientServer"
	
    };

    /*
     * Additional properties passed through to the JiniClient or the
     * service.
     *
     * Note: The services manager is used to collect statistics from the
     * OS for each host so we have performance counters for hosts which
	 * are only running non-bigdata services, such as jini or zookeeper.
     */
    properties = new NV[]{

    };

    /* The services manager MUDT be run on every host so that it may
     * start both bigdata and non-bigdata services (jini, zookeeper).
     * This is also used to report per-host performance counters to
     * the load balancer for hosts that are not running bigdata 
     * services.
     */
    constraints = new IServiceConstraint[] {

    };

}

/**
 * Used to aggregate log messages.
 */
org.apache.log4j.net.SimpleSocketServer {

    constraints = new IServiceConstraint[] {

		// constrain start to the configured host.
		new HostAllowConstraint("@LOG4J_SOCKET_LOGGER_HOST@")

    };
    
    options = new String[] {
    
    	// the port the logger service is listening on.
    	"@LOG4J_SOCKET_LOGGER_PORT@",
    	
    	// the logger service configuration file (a URL)
		"@LOG4J_SOCKET_LOGGER_CONFIG@"
    
    };

}

/**
 * Initial configuration for new instances of the transaction server.
 */
com.bigdata.service.jini.TransactionServer {

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),

	new HostAllowConstraint(bigdata.txs)

    };

	properties = new NV[] {
	
	/* The #of milliseconds that the database will retain history no
	 * longer required to support the earliest active transaction.
	 *
	 * A value of ZERO means that only the last commit point will
	 * be retained.  The larger the value the more history will be
	 * retained.  You can use a really big number if you never want
	 * to release history and you have lots of disk space :-)
	 *
	 * Note: The most recent committed state of the database is
	 * NEVER released.
	 */
	new NV(TransactionServer.Options.MIN_RELEASE_AGE, "0"),

	};
	
}

com.bigdata.service.jini.MetadataServer {

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),
	new TXRunningConstraint(),

	new HostAllowConstraint(bigdata.mds),

    };

    properties = new NV[]{

        /*
	 * The MDS does not support overflow at this time so
	 * overflow MUST be disabled for this service.
	 */
        new NV(MetadataServer.Options.OVERFLOW_ENABLED,"false")    

    };

}

com.bigdata.service.jini.DataServer {

    args = new String[]{
	/* 
	 * Grant lots of memory.
	 *
	 * Note: 32-bit JVMs have a 2G limit on the heap.  64-bit JVMs
	 * can use much more RAM.
	 */
	"-Xmx2G",
	/*
	 * FIXME This might not be required, so that should be tested.
	 * However, you don't want the JVM to just die if it is being
	 * heavily loaded by GC so this is probably a good idea
	 * anyway.
	 */
	//"-XX:-UseGCOverheadLimit",
	/* Configure GC for higher throughput.  Together these options
	 * request parallel old generation collection using N threads.
	 * The application will be paused when this occurs, but GC will
	 * be faster.  Hence throughput will be higher.
	"-XX:+UseParallelOldGC",
	"-XX:ParallelGCThreads=8",
	 */
	/*
	 * Enable JXM remote management for the data service.
	 *
	 * Note: This will not work if you have two data services on a host 
	 * because it will assign the same port to each service.  In order
	 * to work around that the argument would have to be specified by
	 * the service starter and then published in the Entry[] attributes.
	 *
	 * However, you can use ssh -X to open a tunnel with X
	 * forwarding and then run jconsole locally on the target host
	 * and bring up these data services without enabling remote
	 * JMX.
	 *
	"-Dcom.sun.management.jmxremote.port=9999",
	"-Dcom.sun.management.jmxremote.authenticate=false",
	"-Dcom.sun.management.jmxremote.ssl=false",
	 */
    };

    serviceCount = bigdata.dataServiceCount;

    // restrict where the data services can run.
    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),
	new TXRunningConstraint(),

	new HostAllowConstraint(bigdata.ds),

	new MaxDataServicesPerHostConstraint(bigdata.maxDataServicesPerHost),

    };

    /*
     * Note: the [dataDir] will be filled in when a new service
     * instance is created based on the [servicesDir], so don't set it
     * here yourself.
     */
    properties = new NV[]{

	new NV(DataServer.Options.BUFFER_MODE,
	    //""+com.bigdata.journal.BufferMode.Direct
	    ""+com.bigdata.journal.BufferMode.Disk
	    ),

	/* Option disables synchronous overflow after N times and
	 * configures the offset bits for the journal for a scale-up
	 * configuration so we may use very large journals.
	 */
	//new NV(DataServer.Options.OVERFLOW_MAX_COUNT,"5"),
	//new NV(DataServer.Options.OFFSET_BITS,""+com.bigdata.rawstore.WormAddressManager.SCALE_UP_OFFSET_BITS),

	/* Synchronous overflow is triggered when the live journal is
	 * this full (the value is a percentage, expressed as a
	 * floating point number in [0:1]).
	 */
	//new NV(DataServer.Options.OVERFLOW_THRESHOLD,".9"),

	/* Override the initial and maximum extent so that they are more
	 * more suited to large data sets.  Overflow will be triggered as
	 * the size of the journal approaches the maximum extent.  The
	 * initial and maximum extent are configured up above.
	 */

	new NV(DataServer.Options.INITIAL_EXTENT, "" + bigdata.journalExtent),
	new NV(DataServer.Options.MAXIMUM_EXTENT, "" + bigdata.journalExtent),

	/* Specify the queue capacity for the write service (unisolated
	 * write operations).
	 * 
	 * 0 := SynchronousQueue.
	 * N := bounded queue of capacity N
	 * Integer.MAX_VALUE := unbounded queue. 
	 *
	 * Note: The corePoolSize will never increase for an unbounded
	 * queue so the value specified for maximumPoolSize will
	 * essentially be ignored in this case. 
	 *
	 * Note: A SynchronousQueue is a good choice here since it allows
	 * the #of threads to change in response to demand.  The pool
	 * size should be unbounded when using a SynchronousQueue.
	 */
	new NV(DataServer.Options.WRITE_SERVICE_QUEUE_CAPACITY,"0"), // synchronous queue.
	new NV(DataServer.Options.WRITE_SERVICE_CORE_POOL_SIZE,"50"), //
	new NV(DataServer.Options.WRITE_SERVICE_MAXIMUM_POOL_SIZE,""+Integer.MAX_VALUE),
	new NV(DataServer.Options.WRITE_SERVICE_PRESTART_ALL_CORE_THREADS,"true"),

	/*
	 * Options turns off overflow processing (debugging only).
	 * All writes will go onto the live journal, no index segments
	 * will be built, and indices will not be split, moved,
	 * joined, etc.
	 */
	//new NV(DataServer.Options.OVERFLOW_ENABLED,"false"),

	/* Index partition must be this percent of a nominal index
	 * partition in size before a tail split may be triggered.
	 */
	new NV(DataServer.Options.PERCENT_OF_SPLIT_THRESHOLD,".9"),// default

	/* Index partition must be this percent of a nominal index
	 * partition in size before a hot split may be triggered.
	 *
	 * @todo reduce to 1.0 once tailSplit counters are being
	 * collected again (actually, I think that they are) and once
	 * I can verify that indices are not simply being re-split
	 * until they reach the hot split threshold (which would be a
	 * waste).
	 */
	new NV(DataServer.Options.HOT_SPLIT_THRESHOLD,"2.0"),// was .4

	/* Maximum #of index partition moves per overflow.
	 */
	new NV(DataServer.Options.MAXIMUM_MOVES,"1"),

	/* Option controls how many index partitions may be moved onto
	 * any given target data service in a single overflow cycle
	 * and may be used to disable index partition moves (for
	 * debugging purposes).
	 */
	new NV(DataServer.Options.MAXIMUM_MOVES_PER_TARGET,"1"),

	/* The minimum CPU activity on a host before it will consider moving an
	* index partition to shed some load.
	*/
	new NV(DataServer.Options.MOVE_PERCENT_CPU_TIME_THRESHOLD,".85"),//was .7

	/* Option limits the #of index segments in a view before a
	 * compacting merge is forced.
	 */
	new NV(DataServer.Options.MAXIMUM_SEGMENTS_PER_VIEW,"5"), // default 6

	/* Option limits the #of optional merges that are performed in each
	 * overflow cycle.
	 */
	new NV(DataServer.Options.MAXIMUM_OPTIONAL_MERGES_PER_OVERFLOW,"1"),

	/* Option effects how much splits are emphasized for a young
	 * scale-out index.  If the index has fewer than this many
	 * partitions, then there will be a linear reduction in the
	 * target index partition size which will increase the likelyhood
	 * of an index split under heavy writes. This helps to distribute
	 * the index early in its life cycle.
	 */
	new NV(DataServer.Options.ACCELERATE_SPLIT_THRESHOLD,"20"),//20//50

	/* Options accelerates overflow for data services have fewer than
	 * the threshold #of bytes under management.  Acceleration is
	 * accomplished by reducing the maximum extent of the live journal
	 * linearly, but with a minimum of a 10M maximum extent.  When the
	 * maximum extent is reduced by this option, the initial and the
	 * maximum extent will always be set to the same value for that
	 * journal.
	 */
	new NV(DataServer.Options.ACCELERATE_OVERFLOW_THRESHOLD,
	    //"0"
	    //""+com.bigdata.rawstore.Bytes.gigabyte
    	    "2147483648" // 2G
	    ),

	// Zero is full parallelism; otherwise #of threads in the pool.
	new NV(DataServer.Options.OVERFLOW_TASKS_CONCURRENT,"5"),

	/* Use Long.MAX_VALUE to always run overflow processing to
	 * completion (until no more data remains on the old journal).
	 */
	new NV(DataServer.Options.OVERFLOW_TIMEOUT,""+Long.MAX_VALUE),

	new NV(DataServer.Options.OVERFLOW_CANCELLED_WHEN_JOURNAL_FULL,"false"),

    new NV(DataServer.Options.LIVE_INDEX_CACHE_CAPACITY,"10"), // was 60

    new NV(DataServer.Options.HISTORICAL_INDEX_CACHE_CAPACITY,"10"), // was 60

        /* The maximum #of clean indices that will be retained on the
         * hard reference queue (default 20).
         */
        new NV(DataServer.Options.INDEX_CACHE_CAPACITY,"10"), // was 50

        /* The timeout for unused index references before they are
         * cleared from the hard reference queue (default is 1m).
         * After this timeout the index reference is cleared from the
         * queue and the index will be closed unless a hard reference
         * exists to the index.
         */
//       new NV(DataServer.Options.INDEX_CACHE_TIMEOUT,"1200000"), // 20m vs 1m

        /* The maximum #of clean index segments that will be retained
         * on the hard reference queue (default 60).  Note that ALL
         * index segments are clean (they are read-only).
         */
        new NV(DataServer.Options.INDEX_SEGMENT_CACHE_CAPACITY,"20"), // was 100

        /* The timeout for unused index segment references before they
         * are cleared from the hard reference queue (default is 1m).
         * After this timeout the index segment reference is cleared
         * from the queue and the index segment will be closed unless
         * a hard reference exists to the index segment.
         */
//       new NV(DataServer.Options.INDEX_SEGMENT_CACHE_TIMEOUT,"60000000"), // 10m vs 1m

        /* The #of store files (journals and index segment stores)
         * whose hard references will be maintained on a queue.  The
         * value should be slightly more than the index segment cache
         * capacity since some journals also used by the views, but
         * same journals are shared by all views so adding 3 is plenty..
         */
        new NV(DataServer.Options.STORE_CACHE_CAPACITY,"23"),// was 110

//       new NV(DataServer.Options.STORE_CACHE_TIMEOUT,"1200000"),//20m vs 1m. 
	
    };

}

/**
 * Configuration options for the containers used to distribute application tasks
 * across a federation.
 *
 * @todo There should be a means to tag certain client servers for one purpose
 * or another.  This could be handled by subclassing, but it really should be
 * declarative.
 */
com.bigdata.service.jini.ClientServer {

    args = new String[]{
	/* 
	 * Grant lots of memory.
	 */
	"-Xmx800M", /* was 1G, but could be starving the host since other processes
				   are sharing the host in this config. */
	/*
	 * Note: This might not be required, so that should be tested.
	 * However, you don't want the JVM to just die if it is being
	 * heavily loaded by GC so this is probably a good idea
	 * anyway.
	 */
	//"-XX:-UseGCOverheadLimit",
	/* Configure GC for higher throughput.  Together these options
	 * request parallel old generation collection using N threads.
	 * The application will be paused when this occurs, but GC will
	 * be faster.  Hence throughput will be higher.
	"-XX:+UseParallelOldGC",
	"-XX:ParallelGCThreads=8",
	 */
	/*
	 * Enable JXM remote management for the data service.
	 *
	 * Note: This will not work if you have two such services on a host 
	 * because it will assign the same port to each service.  In order
	 * to work around that the argument would have to be specified by
	 * the service starter and then published in the Entry[] attributes.
	 *
	 * However, you can use ssh -X to open a tunnel with X
	 * forwarding and then run jconsole locally on the target host
	 * and bring up these data services without enabling remote
	 * JMX.
	 *
	"-Dcom.sun.management.jmxremote.port=9996",
	"-Dcom.sun.management.jmxremote.authenticate=false",
	"-Dcom.sun.management.jmxremote.ssl=false",
	 */
    };

    serviceCount = bigdata.clientServiceCount;

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),

	new HostAllowConstraint(bigdata.cs),

	new MaxClientServicesPerHostConstraint(bigdata.maxClientServicePerHost),

    };

    properties = new NV[] {
	
    };

}

com.bigdata.service.jini.LoadBalancerServer {

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),

	new HostAllowConstraint(bigdata.lbs)

    };

    args = new String[]{
    /*
     * FIXME The load balancer is a big piggish on long runs because it
     * keeps the performance counter histories in RAM.  While those histories
     * are bounded, it still uses more RAM than it should.
     */
	"-Xmx1G",
	/*
	 * Enable JXM remote management for the data service.
	 *
	 * Note: This will not work if you have two data services on a host 
	 * because it will assign the same port to each service.  In order
	 * to work around that the argument would have to be specified by
	 * the service starter and then published in the Entry[] attributes.
	 *
	"-Dcom.sun.management.jmxremote.port=9998",
	"-Dcom.sun.management.jmxremote.authenticate=false",
	"-Dcom.sun.management.jmxremote.ssl=false",
	 */
    };

    /*
     * Override some properties.
     */
    properties = new NV[] {

    /* 
     * Each JiniClient (and hence all bigdata services) can run an
     * httpd that will expose performance counters for the service and
     * the host on which it is running.  This property specifies the
     * port for that httpd service.  Valid values are port number,
     * zero (0) for a random open port, MINUS ONE (-1) to disable the
     * httpd service.
     *
     * Note: The load balancer httpd normally uses a known port so
     * that it is easy to find.  This is where you will find all of
     * the performance counters aggregated for the entire federation,
     * including their history.
     */
    new NV(IBigdataClient.Options.HTTPD_PORT, "@LOAD_BALANCER_PORT@"),

    /*
     * Note: The load balancer SHOULD NOT collect platform statistics
     * itself since that interfers with its ability to aggregate
     * statistics about the host on which it is running.  Instead it
     * should rely on the presence of at least one other service
     * running on the same host to report those statistics to the load
     * balancer.
     */
    new NV(IBigdataClient.Options.COLLECT_PLATFORM_STATISTICS,"false"),
		
    /* 
     * The directory where the aggregated statistics will be logged.
     * The load balancer will write snapshots of the historical
     * counters into this directory.  See LoadBalancerService javadoc
     * for configuration options which effect how frequently it will
     * log its counters and how many snapshots will be preserved.
     *
     * Note: You only need to specify this option if you want to put
     * the files into a well known location, e.g, on a shared volume.
     */
    //new NV(LoadBalancerServer.Options.LOG_DIR,"/opt2/var/log/bigdata"),

    /* Option essentially turns off the load-based decision making for
     * this many minutes and substitutes a round-robin policy for
     * recommending the least utilized data services.  The main reason
     * to this is to force the initial allocation to be distributed as
     * evenly as possible across the data services in the cluster.
     */
    new NV(LoadBalancerServer.Options.INITIAL_ROUND_ROBIN_UPDATE_COUNT,"10"),

    };

}

/**
 * Configuration options for the distributed LUBM data generator.
 *
 * @todo For query, we can run the LUBM test harness.  However, it
 * should take the kb namespace from the label in the file.
 */
lubm {

    // #of clients to run.
    static private nclients = bigdata.clientServiceCount; // was 14

    // The #of universities to generate. 
    // U8000   is 1.2B told triples
    // U25000  is 3.4B told triples.
    // U50000  is 6.7B told triples.
    // U100000 is ~12B told triples.
    static private univNum = 100000;//8000

    // the job name (based on the #of universities by default).
    static private jobName = "U"+univNum+"";

    // the kb namespace (same as job by default).
    static private namespace = jobName;

    // data directory.  may be NAS or local file system.
    static private dataDir = "@LAS@/lubm/"+jobName+"-data";

    // log directory.  may be NAS or local file system.
    static private logDir = "@LAS@/lubm/"+jobName+"-log";

    // The directory for scheduled index partition dumps for runs.
    static private indexDumpDir = "@NAS@/lubm/"+jobName+"-indexDumps";

    // the ontology to be loaded.
    static private ontology = new File("/nas/metrics/lehigh/univ-bench.owl");

    // minimum #of data services to run.
    static private minDataServices = bigdata.dataServiceCount;

    // How long the master will wait to discover the minimum #of data
    // services that you specified (ms).
    static private awaitDataServicesTimeout = 8000;

    /* Multiplier for the scatter effect.
     *
     * Note: TERM2ID tends to grow more slowly than the other indices for two
     * reasons.  First, there are many more distinct RDF Statements than RDF
     * Values for nearly any data set (except if statement identifiers are enabled,
     * in which case there are more terms than statements).  Second, the keys of
     * the TERM2ID index compress nicely since long prefixes are very common.
     * Therefore it makes sense to use a smaller scatter factor for this index.
     */
    static private scatterFactor = 2;
    static private scatterFactor_term2id = 1;

    /* The #of index partitions to allocate on a scatter split.  ZERO
     * (0) means that 2 index partitions will be allocated per
     * data service which partiticpates in the scatter split.
     * Non-zero values directly give the #of index partitions to
     * create.  
     */
    static private scatterSplitIndexPartitionCount = ConfigMath.multiply
	( scatterFactor,
	  bigdata.dataServiceCount
	  );
    static private scatterSplitIndexPartitionCount_term2id = ConfigMath.multiply
	( scatterFactor_term2id,
	  bigdata.dataServiceCount
	  );

    // Use all discovered data services when scattering an index.
    static private scatterSplitDataServiceCount = 0;

    /* Scatter split trigger point.  The scatter split will not be
     * triggered until the initial index partition has reached
     * this percentage of a nominal index partition in size.
     */
    static private scatterSplitPercentOfSplitThreshold = 0.5;//was .5

    /* This is an integer scaling factor for multiples of the nominal
     * 200M index partition size.  The use of a value other than 1
     * will correspondingly increase the cost of MOVEs.  I am
     * exploring this as a means to reduce the RAM demand as the #of
     * index partitions grows. 
     *
     * Note: You can not use a large multiplier here - it will cause
     * the index segment build times and the asynchronous overflow
     * times to increase exponentially!
     */
    static private partitionSizeMultiplier = 1;

	/* 
	 * Multipliers that compensate for the consumer/producer ratio for
	 * the asynchronous index write API.  These are empirical factors
	 * based on observing the ratio (chunkWritingTime/chunkWaitingTime).
	 * Assuming a constant chunk writing time, if the chunk size for each
	 * index is adjusted by its multiplier then this ratio would be 1:1.
	 * In practice, the chunk writing time is not a linear function of
	 * the chunk size, which is one reason why we prefer larger chunks
	 * and why the asynchronous write API is a win.
	 *
	 * Note: These factors were set relative to TERM2ID.  However, when
	 * I reduced the scatterFactor for TERM2ID by 1/2, I doubled its
	 * chunk size to keep up the same throughput so it is now at 2.00
	 * rather than 1.00.
	 */
	static private chunkSizeFactor_id2term =  1.79;
	static private chunkSizeFactor_term2id =  2.00;
	static private chunkSizeFactor_spo     =  3.89;
	static private chunkSizeFactor_pos     = 13.37;
	static private chunkSizeFactor_osp     = 27.35;
	
	/* The nominal sink chunk size.  For each index, this is adjusted
	 * by the factor specified above.
	 */
	static private sinkChunkSize = 10000;
	 
    /*
     * Specify / override some triple store properties.
     *
     * Note: You must reference this object in the section for the
     * component which will actually create the KB instance, e.g.,
     * either the SplitFinder or the LubmGeneratorMaster.
     */
    static private properties = new NV[] {
	
        /*
         * When "true", the store will perform incremental closure as
         * the data are loaded. When "false", the closure will be
         * computed after all data are loaded. (Actually, since we are
         * not loading through the SAIL making this true does not
         * cause incremental TM but it does disable closure, so
         * "false" is what you need here).
         */
        new NV(BigdataSail.Options.TRUTH_MAINTENANCE, "false" ),

        /*
         * Enable rewrites of high-level queries into native rules (native JOIN
         * execution). (Can be changed without re-loading the data to compare
         * the performance of the Sesame query evaluation against using the
         * native rules to perform query evaluation.)
         */
        new NV(BigdataSail.Options.NATIVE_JOINS, "true"),

        /*
         * May be used to turn off inference during query, but will
	 * cause ALL inferences to be filtered out when reading on the
	 * database.
         */
        // new NV(BigdataSail.Options.INCLUDE_INFERRED, "false"),

        /*
         * May be used to turn off query-time expansion of entailments such as
         * (x rdf:type rdfs:Resource) and owl:sameAs even through those
         * entailments were not materialized during forward closure (this
	 * disables the backchainer!)
         */
        new NV(BigdataSail.Options.QUERY_TIME_EXPANDER, "false"),

        /*
         * Option to restrict ourselves to RDFS only inference. This
         * condition may be compared readily to many other stores.
         * 
         * Note: While we can turn on some kinds of owl processing
         * (e.g., TransitiveProperty, see below), we can not compute
         * all the necessary entailments (only queries 11 and 13
         * benefit).
         * 
         * Note: There are no owl:sameAs assertions in LUBM.
         * 
         * Note: lubm query does not benefit from owl:inverseOf.
         * 
         * Note: lubm query does benefit from owl:TransitiveProperty
         * (queries 11 and 13).
         * 
         * Note: owl:Restriction (which we can not compute) plus
         * owl:TransitiveProperty is required to get all the answers
         * for LUBM.
         */
        new NV(BigdataSail.Options.AXIOMS_CLASS, "com.bigdata.rdf.axioms.RdfsAxioms"),
        // new NV(BigdataSail.Options.AXIOMS_CLASS,"com.bigdata.rdf.axioms.NoAxioms"),

        /*
         * Produce a full closure (all entailments) so that the
         * backward chainer is always a NOP. Note that the
         * configuration properties are stored in the database (in the
         * global row store) so you always get exactly the same
         * configuration that you created when reopening a triple
         * store.
         */
        // new NV(BigdataSail.Options.FORWARD_CHAIN_RDF_TYPE_RDFS_RESOURCE, "true"),
        // new NV(BigdataSail.Options.FORWARD_CHAIN_OWL_SAMEAS_PROPERTIES, "true"),

        /*
         * Additional owl inferences. LUBM only both inverseOf and
         * TransitiveProperty of those that we support (owl:sameAs,
         * owl:inverseOf, owl:TransitiveProperty), but not owl:sameAs.
         */
        new NV(BigdataSail.Options.FORWARD_CHAIN_OWL_INVERSE_OF, "true"),
        new NV(BigdataSail.Options.FORWARD_CHAIN_OWL_TRANSITIVE_PROPERY, "true"),

        // Note: FastClosure is the default.
        // new NV(BigdataSail.Options.CLOSURE_CLASS, "com.bigdata.rdf.rules.FullClosure"),

        /*
         * Various things that effect native rule execution.
         */
        // new NV(BigdataSail.Options.FORCE_SERIAL_EXECUTION, "false"),
        // new NV(BigdataSail.Options.MUTATION_BUFFER_CAPACITY, "20000"),
        new NV(BigdataSail.Options.CHUNK_CAPACITY, "100"),
        // new NV(BigdataSail.QUERY_BUFFER_CAPACITY, "10000"),
        // new NV(BigdataSail.FULLY_BUFFERED_READ_THRESHOLD, "10000"),

        /*
         * Turn off incremental closure in the DataLoader object.
         */
        new NV(com.bigdata.rdf.store.DataLoader.Options.CLOSURE, "None"),
	//com.bigdata.rdf.store.DataLoader.ClosureEnum.None.toString()),

        /*
         * Turn off commit in the DataLoader object. We do not need to commit
         * anything until we have loaded all the data and computed the closure
         * over the database.
         */
        new NV(com.bigdata.rdf.store.DataLoader.Options.COMMIT,"None"),
	//com.bigdata.rdf.store.DataLoader.CommitEnum.None.toString()),

        /*
         * Turn off Unicode support for index keys. This is a big win
         * for load rates if the data set does not use Unicode data,
         * but it has very little effect on query rates since the only
         * time we generate Unicode sort keys is when resolving the
         * Values in the queries to term identifiers in the database.
	 *
	 * @see com.bigdata.btree.keys.CollatorEnum
         */
        new NV(BigdataSail.Options.COLLATOR,"ASCII"),

        /*
         * Turn off the full text index (search for literals by keyword).
         */
	new NV(BigdataSail.Options.TEXT_INDEX, "false"),

        /*
         * Turn on bloom filter for the SPO index (good up to ~2M
         * index entries for scale-up -or- for any size index for
         * scale-out).  This is a big win for some queries on
         * scale-out indices since we can avoid touching the disk if
         * the bloom filter reports "false" for a key.
         */
        new NV(BigdataSail.Options.BLOOM_FILTER, "true"),

	/* The #of low order bits from the TERM2ID index partition
	 * local counter that will be reversed and written into the
	 * high-order bits of the term identifier.  This has a strong
	 * effect on the distribution of bulk index read/write
	 * operations for the triple store.  For a given value of N, a
	 * bulk write will tend to touch 2^N index partitions.
	 * Therefore if this is is even roughly on the order of the number
	 * of index partitions, each bulk write will tend to be scattered
	 * to all index partitions.
	 *
	 * Note: If this value is too large then the writes WITHIN the
	 * index partitions will become uniformly distributed, which
	 * will negatively impact index performance.
	 *
	 * Note: The SplitFinder may only be used when this value is
	 * ZERO (0) due to how it figures out the split points.
	 */
        new NV(BigdataSail.Options.TERMID_BITS_TO_REVERSE,"6"),//was 8, was 6

        /*
         * Turn off statement identifiers (support for statements
         * about statements).
         */
        new NV(BigdataSail.Options.STATEMENT_IDENTIFIERS, "false"),

        /*
	 * Option may be enabled to store blank nodes such that they
	 * are stable (they are not stored by default).
	 *
         * @todo LUBM uses blank nodes. Therefore re-loading LUBM will
         * always cause new statements to be asserted and result in
         * the closure being updated if it is recomputed. Presumably
         * you can tell bigdata to store the blank nodes and RIO to
         * preserve them, but it does not seem to work (RIO creates
         * new blank nodes on reparse). Maybe this is a RIO bug?
         */
        // new NV(BigdataSail.Options.STORE_BLANK_NODES,"true");

        /*
         * Turn off justification chains.  This impacts only the load
         * performance, but it is a big impact and only required if
         * you will be doing truth maintenance (TM). Also, truth
         * maintenance based on the justification chains does not
         * scale-out.  (We are planning a magic sets integration to
         * take care of that).
         */
        new NV(BigdataSail.Options.JUSTIFY, "false"),

        /*
         * Choice of the join algorithm.
         * 
         * false is pipeline, which scales-out.
         * 
         * true is nested, which is also the default right now but
         * does not scale-out.
         */
        new NV(BigdataSail.Options.NESTED_SUBQUERY, "false"),

        /*
         * Maximum #of subqueries to evaluate concurrently for the 1st
         * join dimension for native rules. Zero disables the use of
         * an executor service. One forces a single thread, but runs
         * the subquery on the executor service. N>1 is concurrent
         * subquery evaluation.
	 *
	 * Note: parallel subquery does not work for pipeline joins at
	 * this time so this option is only safe for nested subquery
	 * join, and nested subquery joins do not scale-out.
         */
        // new NV(BigdataSail.Options.MAX_PARALLEL_SUBQUERIES, "5"),
        new NV(BigdataSail.Options.MAX_PARALLEL_SUBQUERIES, "0"),

	/*
         * This controls how much data the mutable BTree on the live
         * journal and the read-historical BTrees will buffer.  The
         * default should work well unless you are resource
         * constrained. There is no point making this value too large
         * since periodic overflow keeps down the #of tuples in these
         * BTree objects.  There are separate parameters which control
         * the buffering for the IndexSegments.  A value of ZERO (0) 
	 * will disable the read-retention queue, but the write retention
	 * queue will continue to provide buffering sufficient for most
	 * purposes (this covers both mutable and immutable BTrees).
         */
	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.BTREE_READ_RETENTION_QUEUE_CAPACITY
		 ), "0"),// default 10000

	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.WRITE_RETENTION_QUEUE_CAPACITY
		 ), "500"),// default 500

	/*
	 * Turn on direct buffering for index segment nodes.
	 */
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.INDEX_SEGMENT_BUFFER_NODES
		  ), "true"),

	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.INDEX_SEGMENT_BRANCHING_FACTOR
		 ), "512"),// default 512

	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.INDEX_SEGMENT_LEAF_CACHE_CAPACITY
		 ), "20"),// default 100

	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.INDEX_SEGMENT_LEAF_CACHE_TIMEOUT
		 ), ""+ConfigMath.s2ns(10)),// default 30s

	// Disable per-child locking.
	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.CHILD_LOCKS
		 ), "false"), 

	/*
	 * Tweak the asynchronous write API parameters.
	 */

	// dial down the master/sink queue capacities to rein in RAM.
	// default is 5k.  The sink is the big RAM consumer since there
	// is one since per index partition (and one thread per sink).
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.MASTER_QUEUE_CAPACITY
		  ), "5"), // was 1000; was 5000
	// default is 5k
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SINK_QUEUE_CAPACITY
		  ), "50"), // was 500, was 5000

 	// Override master/sink queue capacity for TERM2ID since uses KVOLatch
 	// and requires all results to be in before writes on the other indices
 	// may proceed.
// 	new NV(com.bigdata.config.Configuration.getOverrideProperty
// 	       ( namespace + "."
// 				    + LexiconRelation.NAME_LEXICON_RELATION + "."
// 				    + LexiconKeyOrder.TERM2ID,
// 		 IndexMetadata.Options.MASTER_QUEUE_CAPACITY
// 		 ), "1000"),// default 5000
// 	new NV(com.bigdata.config.Configuration.getOverrideProperty
// 	       ( namespace + "."
// 				    + LexiconRelation.NAME_LEXICON_RELATION + "."
// 				    + LexiconKeyOrder.TERM2ID,
// 		 IndexMetadata.Options.SINK_QUEUE_CAPACITY
//		 ), "1000"),// default 5000

	// default is 10k (global adjustment)
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SINK_CHUNK_SIZE
		  ), "10000"), // was 20000

	/* per index adjustment. */
 	new NV(com.bigdata.config.Configuration.getOverrideProperty
 	       ( namespace + "."
 				    + LexiconRelation.NAME_LEXICON_RELATION + "."
 				    + LexiconKeyOrder.TERM2ID,
 		 IndexMetadata.Options.SINK_CHUNK_SIZE
		 ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_term2id)),
 	new NV(com.bigdata.config.Configuration.getOverrideProperty
 	       ( namespace + "."
 				    + LexiconRelation.NAME_LEXICON_RELATION + "."
 				    + LexiconKeyOrder.ID2TERM,
 		 IndexMetadata.Options.SINK_CHUNK_SIZE
		 ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_id2term)),
    new NV(com.bigdata.config.Configuration.getOverrideProperty
           ( namespace + "."    + SPORelation.NAME_SPO_RELATION + "."
                                + SPOKeyOrder.SPO,
             IndexMetadata.Options.SINK_CHUNK_SIZE
             ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_spo)),
    new NV(com.bigdata.config.Configuration.getOverrideProperty
       ( namespace + "."        + SPORelation.NAME_SPO_RELATION + "."
                                + SPOKeyOrder.POS,
             IndexMetadata.Options.SINK_CHUNK_SIZE
             ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_pos)),
    new NV(com.bigdata.config.Configuration.getOverrideProperty
           ( namespace + "."    + SPORelation.NAME_SPO_RELATION + "."
                                + SPOKeyOrder.OSP,
             IndexMetadata.Options.SINK_CHUNK_SIZE
             ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_osp)),

// 	// default is infinite
// 	new NV( com.bigdata.config.Configuration.getOverrideProperty
// 		( namespace,
// 		  IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
// 		  ), ""+Long.MAX_VALUE),//ConfigMath.s2ns(120)),

// 	// default is infinite.
// 	new NV( com.bigdata.config.Configuration.getOverrideProperty
// 		( namespace,
// 		  IndexMetadata.Options.SINK_IDLE_TIMEOUT_NANOS
//		  ), ""+Long.MAX_VALUE),//ConfigMath.s2ns(320)),

	/* The TERM2ID index uses idle timeout to preserve liveness for async writes.
	 * This is required in order for the async loader to not wait forever for the
	 * last full chunk when bulk loading with async writes on the TERM2ID index.
	 * The value of this timeout effects both how large the chunks will be and
	 * how long after the last document is parsed it will be until the TERM2ID
	 * async write buffers decide that they are idle and evict the last of their
	 * chunks.  This also effects the throughput since the larger chunk sizes
	 * correlates very well with higher throughput.
	 */
         new NV(com.bigdata.config.Configuration.getOverrideProperty
 	       ( namespace + "." + LexiconRelation.NAME_LEXICON_RELATION + "." + LexiconKeyOrder.TERM2ID,
 	         IndexMetadata.Options.SINK_IDLE_TIMEOUT_NANOS
 	         ), ""+ConfigMath.s2ns(20)),

        /*
         * Tweak parameters designed to result in good split points
         * for the various indices to achieve 100-200M segments (based
         * on U50 data set).
         * 
         * @todo These changes should be applied to the triple store,
         * but in a manner that permits explicit override. They result
         * in good sizes for the index partitions, at least for the
         * LUBM data. [Actually, I may rework the split logic to be
         * based purely on the size of the index partition after a
         * compacting merge in which case I won't need to deal with
         * tweaking these parameters any more.]
         */

	//
	// scatter split overrides
	//

	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SCATTER_SPLIT_PERCENT_OF_SPLIT_THRESHOLD
		  ), ""+scatterSplitPercentOfSplitThreshold),

	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SCATTER_SPLIT_DATA_SERVICE_COUNT
		  ), ""+scatterSplitDataServiceCount),

	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SCATTER_SPLIT_INDEX_PARTITION_COUNT
		  ), ""+scatterSplitIndexPartitionCount),
    new NV(com.bigdata.config.Configuration
       .getOverrideProperty(namespace + "."
			    + LexiconRelation.NAME_LEXICON_RELATION + "."
			    + LexiconKeyOrder.TERM2ID,
			    IndexMetadata.Options.SCATTER_SPLIT_INDEX_PARTITION_COUNT),
       "" +scatterSplitIndexPartitionCount_term2id),

	//
	// normal split overrides
	//

        // SPO statement index (20x the default)
        new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace + "."
				    + SPORelation.NAME_SPO_RELATION + "."
				    + SPOKeyOrder.SPO,
				    IndexMetadata.Options.SPLIT_HANDLER_MIN_ENTRY_COUNT),
	       "" + ConfigMath.multiply(partitionSizeMultiplier,
					ConfigMath.multiply( 10000/*was 5000*/, Bytes.kilobyte32))),
	new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace + "."
				    + SPORelation.NAME_SPO_RELATION + "."
				    + SPOKeyOrder.SPO,
				    IndexMetadata.Options.SPLIT_HANDLER_ENTRY_COUNT_PER_SPLIT),
	       "" + ConfigMath.multiply(partitionSizeMultiplier,
					ConfigMath.multiply(20/*was 10*/, Bytes.megabyte32))),
	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace + "."
				    + SPORelation.NAME_SPO_RELATION + "."
				    + SPOKeyOrder.SPO,
		 IndexMetadata.Options.INDEX_SEGMENT_BRANCHING_FACTOR
		 ), "512"),// default 512 (3x is 1536)

        // OSP statement index (20x the default)
        new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace + "."
				    + SPORelation.NAME_SPO_RELATION + "."
				    + SPOKeyOrder.OSP,
				    IndexMetadata.Options.SPLIT_HANDLER_MIN_ENTRY_COUNT),
	       "" + ConfigMath.multiply(partitionSizeMultiplier,
					ConfigMath.multiply( 10000/*was 5000*/, Bytes.kilobyte32))),
	new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace + "."
				    + SPORelation.NAME_SPO_RELATION + "."
				    + SPOKeyOrder.OSP,
				    IndexMetadata.Options.SPLIT_HANDLER_ENTRY_COUNT_PER_SPLIT),
	       "" + ConfigMath.multiply(partitionSizeMultiplier,
					ConfigMath.multiply(20/*was 10*/, Bytes.megabyte32))),
	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace + "."
				    + SPORelation.NAME_SPO_RELATION + "."
				    + SPOKeyOrder.OSP,
		 IndexMetadata.Options.INDEX_SEGMENT_BRANCHING_FACTOR
		 ), "512"),// default 512 (3x is 1536)

        // POS statement index (30x the default)
        new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace + "."
				    + SPORelation.NAME_SPO_RELATION + "."
				    + SPOKeyOrder.POS,
				    IndexMetadata.Options.SPLIT_HANDLER_MIN_ENTRY_COUNT),
	       "" + ConfigMath.multiply(partitionSizeMultiplier,
					ConfigMath.multiply( 15000/*was 5000*/, Bytes.kilobyte32))),
	new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace + "."
				    + SPORelation.NAME_SPO_RELATION + "."
				    + SPOKeyOrder.POS,
				    IndexMetadata.Options.SPLIT_HANDLER_ENTRY_COUNT_PER_SPLIT),
	       "" + ConfigMath.multiply(partitionSizeMultiplier,
					ConfigMath.multiply(30/*was 10*/, Bytes.megabyte32))),
	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace + "."
				    + SPORelation.NAME_SPO_RELATION + "."
				    + SPOKeyOrder.POS,
		 IndexMetadata.Options.INDEX_SEGMENT_BRANCHING_FACTOR
		 ), "512"),// default 512 (5x is 2560)

        // term2id index (20x the default).
        new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace + "."
				    + LexiconRelation.NAME_LEXICON_RELATION + "."
				    + LexiconKeyOrder.TERM2ID,
				    IndexMetadata.Options.SPLIT_HANDLER_MIN_ENTRY_COUNT),
	       "" + ConfigMath.multiply(partitionSizeMultiplier,
					ConfigMath.multiply(10000/*was 2500*/, Bytes.kilobyte32))),
        new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace + "."
				    + LexiconRelation.NAME_LEXICON_RELATION + "."
				    + LexiconKeyOrder.TERM2ID,
				    IndexMetadata.Options.SPLIT_HANDLER_ENTRY_COUNT_PER_SPLIT),
	       "" + ConfigMath.multiply(partitionSizeMultiplier,
					ConfigMath.multiply(20/*was 5*/, Bytes.megabyte32))),
	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace + "."
				    + LexiconRelation.NAME_LEXICON_RELATION + "."
				    + LexiconKeyOrder.TERM2ID,
		 IndexMetadata.Options.INDEX_SEGMENT_BRANCHING_FACTOR
		 ), "512"),// default 512 (3x is 1536)

        // id2term index (4x the default)
        new NV(com.bigdata.config.Configuration
                .getOverrideProperty(namespace + "."
                        + LexiconRelation.NAME_LEXICON_RELATION + "."
                        + LexiconKeyOrder.ID2TERM,
                        IndexMetadata.Options.SPLIT_HANDLER_MIN_ENTRY_COUNT),
	       "" + ConfigMath.multiply(partitionSizeMultiplier,
					ConfigMath.multiply(2000/*was 1000*/, Bytes.kilobyte32))),
        new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace + "."
				    + LexiconRelation.NAME_LEXICON_RELATION + "."
				    + LexiconKeyOrder.ID2TERM,
				    IndexMetadata.Options.SPLIT_HANDLER_ENTRY_COUNT_PER_SPLIT),
	       "" + ConfigMath.multiply(partitionSizeMultiplier,
					ConfigMath.multiply(4/*was 2*/, Bytes.megabyte32))),
	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace + "."
				    + LexiconRelation.NAME_LEXICON_RELATION + "."
				    + LexiconKeyOrder.ID2TERM,
		 IndexMetadata.Options.INDEX_SEGMENT_BRANCHING_FACTOR
		 ), "512"),// default 512 (.5x is 256)

    };

}

/**
 * Distributed bulk loader configuration.
 *
 * Note: This may be enabled by changing the name of the component
 * which is being configured here.  When enabled, it will load files
 * written by a previous run of the LUBM generator. Or you can change
 * the dataDir to load files from some other location.
 */
//com.bigdata.rdf.load.RDFDataLoadMaster {

/**
 * LUBM distributed bulk loader configuration.  This is designed to
 * dynamically generate and load the files so you can load very large
 * data sets without having to pre-generate and store the data on a
 * shared file system.
 */
edu.lehigh.swat.bench.ubt.bigdata.LubmGeneratorMaster {

	/*
	 * General options for a master executing distributed tasks.
	 */

    // The job name.
    jobName = lubm.jobName;

	// The #of client tasks to execute.
    nclients = lubm.nclients;

	// Properties to be made visible to JiniFederation#getProperties()
	properties = lubm.properties;

	// where to write scheduled dumps of the index partition metadata.
	indexDumpDir = new File(lubm.indexDumpDir,jobName);

	// must be specified for indexDumpDir to work.
	indexDumpNamespace = lubm.namespace;
	
	// force overflow of all data services after the job ends.
	forceOverflow = true;

    /* How long the master will wait in milliseconds to discover the services
     * that you specify for [servicesTemplates] and [clientsTemplate].
     */
    awaitServicesTimeout = 10000;
    
	/* The minimum set of services which must be discovered before the master
	 * can start.
	 */
	servicesTemplates = new ServicesTemplate[] {

		// the metadata service
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IMetadataService.class
				},
				null/*attributes*/),
			null/*filter*/
			),

		// the data services (filter is required to exclude metadata services)
		new ServicesTemplate(
			bigdata.dataServiceCount/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IDataService.class
				},
				null/*attributes*/),
			DataServiceFilter.INSTANCE/*filter*/
			),

		// the load balancer
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[] {
					com.bigdata.service.ILoadBalancerService.class
				},
				null/*attributes*/),
			null/*filter*/
			)
		
	};

    /* Template for matching the services to which the clients will be
     * distributed for execution.  Normally you will specify
     * IClientService as the interface to be discovered.  While it is
     * possible to run tasks on an IDataService or even an
     * IMetadataService since they both implement IRemoteExecutor, it
     * is generally discouraged unless the tasks require explicit
     * access to the local index partitions for their execution.
     */
	clientsTemplate = new ServicesTemplate(
			bigdata.clientServiceCount, // minMatches
			new ServiceTemplate(
				null, //serviceID
				new Class[]{
					com.bigdata.service.IClientService.class
				},
				null // attributes
			    ),
			null // filter
			);

	/*
	 * RDF distributed data loader options.
	 */

    // The KB name.
    namespace = lubm.namespace;
    
    /* @todo This is unused since we are reading file names from a queue and
       the files are written into the outDir specified for the LUBM generator.
       The option will be migrated into a class which is specific to a
       distributed-file loader.
    */
    dataDir = new File(lubm.dataDir);

    // The ontology to load (file or directory)
    ontology = lubm.ontology;

    // #of concurrent data loader threads per client.
    nthreads = 100; // (was 30) Fixed size thread pool.
    //nthreads = Integer.MAX_VALUE; // Unbounded thread pool with synchronous queue.

	/* The capacity of the task queue feeding those clients (ignored when using
	 * an unbounded thread pool).
	 */
	//queueCapacity = 0; // SynchronousQueue 
	queueCapacity = nthreads; // Bounded queue capacity.
	//queueCapacity = Integer.MAX_VALUE; // Unbounded queue capacity.

    // The StatementBuffer capacity for sync writes (unused for async writes).
    bufferCapacity = 100000;

	// chunk size for async writes (2x perf). when zero, using sync RPC instead.
	asynchronousWrites = true;
	syncRPCForTERM2ID = false; // enable async writes on TERM2ID also.
	asynchronousWriteProducerChunkSize = 20000;
	valuesInitialCapacity = 20000;
	bnodesInitialCapacity = 16;

    // create the KB if not found.
    create = true; // Note: specify [false] here if you are using SplitFinder.

    // when true, deletes each source file once loaded successfully.
    deleteAfter = true;

    // when true, loads data.
    loadData = true;

    // when true, computes closure.
    computeClosure = false;

	/*
	 * When true, requests a compacting merge of the data services in
	 * the federation before computing the closure.
	 */
	forceOverflowBeforeClosure = true;

	/*
	 * LUBM Generator integration options.
	 */

    /* You have a choice of generate only or generate and load using
     * queues to couple the generator to the data loader.
     *
     * Note: If you just generate the data and write it into local
     * files on each host, then you must place a data loader task on
     * each host in order to have the full data set loaded.
     */
    generateOnly = false;

    // The #of universities to generate. 
    univNum = lubm.univNum;
    
    /* Specify where to put the generated data files and the generated
       log files.  The big choice is whether to put the data onto a
       shared volume or to write the data onto a local volume, in
       which case it must be consumed by the same host which generated
       the data. */
    outDir = new File(lubm.dataDir);

    // log directory is always NAS so we can see what is happening.
    logDir = new File(lubm.logDir,jobName);

	/* The #of files that can be queued up by the generator before it
	   will block. */
//	generatorQueueCapacity = 0; // SynchronousQueue
     generatorQueueCapacity = (int)ConfigMath.multiply(2,nthreads); // blocking
	// generatorQueueCapacity = Integer.MAX_VALUE; // unbounded 

    /* More lubm options (not very likely to change).
     */
    
    startIndex = 0;
    
    ontologyURL = "http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl";

}

edu.lehigh.swat.bench.ubt.bigdata.LubmGeneratorMasterOld {

    /* You have a choice of generate only or generate and load using
     * queues to couple the generator to the data loader.
     *
     * Note: If you just generate the data and write it into local
     * files on each host, then you must place a data loader task on
     * each host in order to have the full data set loaded.
     */
    generateOnly = false;

    /* Force overflow and compacting merge of the data services before
     * computing the database at once closure.
     */
    forceOverflowBeforeClosure=true;

    // the job name.
    jobName = lubm.jobName;

    // The #of universities to generate. 
    univNum = lubm.univNum;
    
    // There can be more clients than data services, but not more
    // clients than universities to be generated.
    nclients = lubm.nclients;

    // Set to [nclients] unless you have fewer data services available
    // in which case set to the #of data services.  You can use a
    // smaller value if you are not sure if all data services will be
    // running when you start the lubm master.
    minDataServices = lubm.minDataServices;

    // How long the master will wait to discover the minimum #of data
    // services that you specified.
    awaitDataServicesTimeout = lubm.awaitDataServicesTimeout;

    /* Specify where to put the generated data files and the generated
       log files.  The big choice is whether to put the data onto a
       shared volume or to write the data onto a local volume, in
       which case it must be consumed by the same host which generated
       the data. */
    outDir = new File(lubm.dataDir);

    // log directory is always NAS so we can see what is happening.
    logDir = new File(lubm.logDir,jobName);

    // where to put the scheduled index partition metadata dumps.
    indexDumpDir = new File(lubm.indexDumpDir,jobName);

    // non-blocking queue.
    //queueCapacity = Integer.MAX_VALUE;
    // blocking queue capacity (#of files in the queue per client).
    queueCapacity = 100; // was 50;

    /* Not very likely to change.  */

    startIndex = 0;
    
    // ontology URL
    ontologyURL = "http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl";

    /* Overrides KB properties when the LUBM master is used to create
     * the KB.
     */
    properties = lubm.properties;

}

com.bigdata.service.jini.DumpFederation {

	namespace = lubm.namespace;

}

com.bigdata.service.jini.BroadcastSighup {

	pushConfig = false;
	
	restartServices = true;

}

/**
 * Configuration for a component that pre-parses a specified data set
 * in order to find good split points and the allocates a scale-out
 * triple store on the federation using those split points.
 *
 * @deprecated This component has be replaced by the use of scatter splits
 *             together with TERMID_BITS_TO_REVERSE.
 */
com.bigdata.rdf.load.SplitFinder {

    // create iff true, otherwise just report what the splits should be.
    create = true;
    
    // the namespace of the triple store to be created.
    namespace = lubm.namespace;
    
    // #of splits per scale-out index.
    nsplits = ConfigMath.multiply(2, bigdata.dataServiceCount);
    
    // #of services on which the index partitions will be registered.
    nservices = lubm.minDataServices;

	// data used to generate the split points.
    dataDir = new File("/opt2/data/lehigh/U10");

	// ontology used to generate the split points.
    ontology = lubm.ontology;

    // ontology to be loaded into the created triple store.    
    postCreateOntology = lubm.ontology;

    /* Override some triple store properties when the SplitFinder is
     * used to create the KB.
     */
    properties = lubm.properties;

	/* Disable reporting to the LBS for the SplitFinder so that it
	 * can run standalone without issuing warnings about not
	 * finding the LBS.
	 *
	 * @todo needs to be concatenated with the other properties.
	 */
	//new NV(IBigdataClient.Options.REPORT_DELAY,""+Long.MAX_VALUE),

}

/**
 * Configuration for a throughput testing utility.
 */
com.bigdata.service.jini.benchmark.ThroughputMaster {

	//forceOverflow=true;

	jobName = "test_1";
	
	namespace = "test.throughputTest.test_1";

    /* How long the master will wait in milliseconds to discover the services
     * that you specify
     */
    awaitServicesTimeout = 5000;
    
	/* The minimum set of services which must be discovered before the master
	 * can start.
	 */
	servicesTemplates = new ServicesTemplate[] {

		// the metadata service
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IMetadataService.class
				},
				null/*attributes*/),
			null/*filter*/
			),

		// the data services (filter is required to exclude metadata services)
		new ServicesTemplate(
			bigdata.dataServiceCount/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IDataService.class
				},
				null/*attributes*/),
			DataServiceFilter.INSTANCE/*filter*/
			),

		// the load balancer
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[] {
					com.bigdata.service.ILoadBalancerService.class
				},
				null/*attributes*/),
			null/*filter*/
			)
		
	};

    /* Template for matching the services to which the clients will be
     * distributed for execution.  Normally you will specify
     * IClientService as the interface to be discovered.  While it is
     * possible to run tasks on an IDataService or even an
     * IMetadataService since they both implement IRemoteExecutor, it
     * is generally discouraged unless the tasks require explicit
     * access to the local index partitions for their execution.
     */
	clientsTemplate = new ServicesTemplate(
			bigdata.clientServiceCount, // minMatches
			new ServiceTemplate(
				null, //serviceID
				new Class[]{
					com.bigdata.service.IClientService.class
				},
				null), // attributes
			null // filter
			);

	/* The initial #of index partitions for the scale-out index
	 * (computed as #partitions per data service). Choose at least
	 * one per core if you want to fully load your machines.
	 */
	npartitions = ConfigMath.multiply(10, bigdata.dataServiceCount);

	/* #of clients to start across the federation.
	 * 
	 * Note: If the clients choose the index partition for each
	 * operation randomly then this must be at least N times the
	 * #of index partitions to expect an average queue length of N
	 * for that index partition.  If index partitions are split,
	 * then you will need more clients to achieve the same queue
	 * length. Also, since client operations have RMI latency, you
	 * will need more than N to achieve a queue length of N.
	 * 
	 * Note: If the clients choose the key range on which they
	 * will write statically [startKeyPartitions := true] then it
	 * makes the most sense to directly specify the #of clients or
	 * to specify the #of clients per data service.
	 */
	//nclients = ConfigMath.multiply(200, npartitions);
	nclients = ConfigMath.multiply(10, bigdata.dataServiceCount);

	startKeyPartitions = true;
	
	operationCount =    1000000; // 1M
//	operationCount =   10000000; // 10M
//	operationCount =  100000000; // 100M
//	operationCount = 1000000000; // 1B

	maxKeysPerOp = 1000;
	
//	incRange = 100; // very tight
	//incRange = 1000; // tight
	incRange = 10000; // loose
	
    /* When true, uses the asynchronous write API.  Otherwise use the
     * synchronous RPC API.  The asynchronous write API buffers writes
     * destined for each index partition and can substantially increase
     * the throughput of distributed index writes, especially when the
     * the application performs a large #of small writes and those writes
     * are scattered across a large #of index partitions.
     */
    asynchronous = true;

	// disable zookeeper updates by the test clients.
	zookeeperUpdateInterval = 0;
	//zookeeperUpdateInterval = 100000;

}

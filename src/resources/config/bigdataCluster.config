import net.jini.jeri.BasicILFactory;
import net.jini.jeri.BasicJeriExporter;
import net.jini.jeri.tcp.TcpServerEndpoint;

import net.jini.discovery.LookupDiscovery;
import net.jini.core.discovery.LookupLocator;
import net.jini.core.entry.Entry;
import net.jini.lookup.entry.Name;
import net.jini.lookup.entry.Comment;
import net.jini.lookup.entry.Address;
import net.jini.lookup.entry.Location;
import net.jini.lookup.entry.ServiceInfo;
import net.jini.core.lookup.ServiceTemplate;

import java.io.File;

import com.bigdata.util.NV;
import com.bigdata.journal.BufferMode;
import com.bigdata.jini.lookup.entry.*;
import com.bigdata.service.IBigdataClient;
import com.bigdata.service.jini.*;
import com.bigdata.service.jini.lookup.DataServiceFilter;
import com.bigdata.service.jini.master.ServicesTemplate;
import com.bigdata.jini.start.config.*;
import com.bigdata.jini.util.ConfigMath;

import org.apache.zookeeper.ZooDefs;
import org.apache.zookeeper.data.ACL;
import org.apache.zookeeper.data.Id;

// imports for various options.
import com.bigdata.btree.IndexMetadata;
import com.bigdata.btree.keys.KeyBuilder;
import com.bigdata.rdf.sail.BigdataSail;
import com.bigdata.rdf.spo.SPORelation;
import com.bigdata.rdf.spo.SPOKeyOrder;
import com.bigdata.rdf.lexicon.LexiconRelation;
import com.bigdata.rdf.lexicon.LexiconKeyOrder;
import com.bigdata.rawstore.Bytes;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeUnit.*;

/*
 * This is a sample configuration file for a bigdata federation.
 * 
 * Note: The original file is a template.  The template contains parameters
 * of the form @XXX@.  The values for those template parameters are specified
 * in the build.properties file when you use ant to install bigdata.
 * 
 * Note: This file uses the jini configuration mechanism.  The syntax
 * is a subset of Java.  The properties for each component are grouped
 * within the namespace for that component.
 *
 * See the net.jini.config.ConfigurationFile javadoc for more
 * information.
 */

/*
 * A namespace use for static entries referenced elsewhere in this
 * ConfigurationFile.
 */
bigdata {

    /**
     * The name for this federation.
     *
     * Note: This is used to form the [zroot] (root node in zookeeper
     * for the federation) and the [serviceDir] (path in the file
     * system for persistent state for the federation).
     *
     * Note: If you will be running more than one federation, then you
     * MUST use unicast discovery and specify the federation name in
     * the [groups].
     */
    static private fedname = "@FED@";

    /**
     * Where to put all the persistent state.
     */
    static private serviceDir = new File("@LAS@");

	/**
	 * Which JDK to use.
	 */
	static private javaHome = new File("@JAVA_HOME@");
	
    /**
     * A common point to set the Zookeeper client's requested
     * sessionTimeout and the jini lease timeout.  The default lease
     * renewal period for jini is 5 minutes while for zookeeper it is
     * more like 5 seconds.  This puts the two systems onto a similar
     * timeout period so that a disconnected client is more likely to
     * be noticed in roughly the same period of time for either
     * system.  A value larger than the zookeeper default helps to
     * prevent client disconnects under sustained heavy load.
     */
    // jini
    static private leaseTimeout = ConfigMath.m2ms(60);// 20s=20000; 5m=300000; 
    // zookeeper
    static private sessionTimeout = (int)ConfigMath.m2ms(10);// was 5m 20s=20000; 5m=300000; 

    /*
     * Example cluster configuration.
     *
     * Data services are load balanced.  Index partitions will be
     * moved around as necessary to ensure hosts running data
     * service(s) are neither under nor over utilized.  Data services
     * can be very resource intensive processes.  They heavily buffer
     * both reads and writes, and they use RAM to do so.  They also
     * support high concurrency and can use up to one thread per index
     * partition.  How many cores they will consume is very much a
     * function of the application.
     *
     * Zookeeper services use a quorum model.  Always allocate an odd
     * number.  3 gives you one failure.  5 gives you two failures.
     * Zookeeper will sync the disk almost continuously while it is
     * running.  It really deserves its own local disk.  Zookeeper
     * also runs in memory.  Since all operations are serialized, if
     * it starts swapping then peformance will drop through the floor.
     *
     * Jini uses a peer model.  Each service registers with each
     * registrar that it discovers.  Each client listeners to each
     * registrar that it discovers.  The default jini core services
     * installation runs entirely in memory (no disk operations, at
     * least not for service registration). A second instance of the
     * jini core services provides a safety net.  If you are using
     * multicast then you can always add another instance.
     */

	/* Declare the hosts.  This provides indirection for planning
	 * purposes.
	 *
	 * The summary notation is: cores@GHZ/cache x RAM x DISK
	 */
	static private h0 = "192.168.20.26"; // 4@3ghz/1kb x 4GB x 263G
	static private h1 = "192.168.20.27"; // 4@3ghz/2kb x 4GB x 263G 
	static private h2 = "192.168.20.28"; // 4@3ghz/1kb x 4GB x 64G
	
	/* Note: this configuration puts things that are not disk intensive
	 * on the host with the least disk space and zookeeper.
	 */
    static private lbs = h2; // specify as @LOAD_BALANCER_HOST@ ?
    static private txs = h2;
    static private mds = h2;

    // 1+ jini servers
    static private jini1 = h2;
    //static private jini2 = h1;
    static private jini = new String[]{ jini1 }; //,jini2};

    // Either 1 or 3 zookeeper machines (one instance per).
    // See the QuorumPeerMain and ZooKeeper configurations below.
    static private zoo1 = h2;
    //static private zoo2 = h1;
    //static private zoo3 = h2;
    static private zoo = new String[] { zoo1 }; // ,zoo2,zoo3};

    // 1+ client service machines (1+ instance per host).
    static private cs0  = h2;

    // 1+ data service machines (1+ instance per host).
    static private ds0  = h0;
    static private ds1  = h1;

    // client servers
    static private cs = new String[] {
    	cs0 //, ...
    };

    // The target #of client servers.
    static private clientServiceCount = 1; // was 2
    static private maxClientServicePerHost = 1; // was 2

    // data servers
    static private ds = new String[]{
	   ds0, ds1 //, ...
 	   };

    // The target #of data services.
    static private dataServiceCount = 2;// was 4

    // Maximum #of data services per host.
    static private maxDataServicesPerHost = 1; // was 2

    // @todo also specify k (replicationCount)

    // Sets the initial and maximum journal extents.
    static private journalExtent = ConfigMath.multiply(200, Bytes.megabyte);

    /**
     * A String[] whose values are the group(s) to be used for discovery
     * (no default). Note that multicast discovery is always used if
     * LookupDiscovery.ALL_GROUPS (a <code>null</code>) is specified.
     */

    // one federation, multicast discovery.
    //static private groups = LookupDiscovery.ALL_GROUPS;

    // unicast discovery or multiple federations, MUST specify groups.
    static private groups = new String[]{bigdata.fedname};

    /**
     * One or more unicast URIs of the form <code>jini://host/</code>
     * or <code>jini://host:port/</code> (no default).
     *
     * This MAY be an empty array if you want to use multicast
     * discovery <strong>and</strong> you have specified the groups as
     * LookupDiscovery.ALL_GROUPS (a <code>null</code>).
     */
    static private locators = new LookupLocator[] {

	// runs jini on the localhost using unicast locators.
	//new LookupLocator("jini://localhost/")
	
	// runs jini on two hosts using unicast locators.
	new LookupLocator("jini://"+jini1),
	//new LookupLocator("jini://"+jini2),

    };

    /**
     * The policy file that will be used to start services.
     */
    private static policy = "@POLICY_FILE@";

    /**
     * log4j configuration file (applies to bigdata and zookeeper).
     *
     * Note: The value is URI!
     *
     * Note: You should aggregate all of the log output to a single
     * host.  For example, using the log4j SocketAppender and the
     * SimpleNodeServer.
     */
    log4j = "@LOG4J_CONFIG@";

    /**
     * java.util.logging configuration file (applies to jini as used
     * within bigdata).
     *
     * Note: The value is a file path!
     */
    logging = "@LOGGING_CONFIG@";

    /*
    private static host = ConfigUtil.getHostName();
    private static port = "8081";
    private static jskdl = " http://" + host + ":" + port + "/jsk-dl.jar";
    */

}

/*
 * Service configuration defaults.  These can also be specified on a
 * per service-type basis.  When the property is an array type, the
 * value here is concatenated with the optional array value on the per
 * service-type configuration.  Otherwise it is used iff no value is
 * specified for the service-type configuration.
 */
com.bigdata.jini.start.config.ServiceConfiguration {

    /* 
     * Default java command line arguments that will be used for all
     * java-based services
     *
     * Note: [-Dcom.sun.jini.jeri.tcp.useNIO=true] enables NIO in
     * combination with the [exporter] configured below.
     */
    defaultJavaArgs = new String[]{
	"-server",
	"-ea",
	"-showversion",
	//"-Xmx2G",
     /* This is a workaround for a JVM bug which can result in a
      * lost wakeup.  This bug is fixed in JDK1.6.0_18.  However,
      * JDK1.6.0_18 has other problems which result in segfaults.
      *
      * http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6822370
      */
    "-XX:+UseMembar",
	"-Dcom.sun.jini.jeri.tcp.useNIO=@USE_NIO@",
	"-Djava.security.policy="+bigdata.policy,
	"-Djava.util.logging.config.file="+bigdata.logging,
	"-Dcom.bigdata.counters.linux.sysstat.path=@SYSSTAT_HOME@"
    };

    /* Default path for service instances and their persistent
     * data. This may be overriden on a per service-type basis. 
     *
     * Note: For logical services that support failover, the concrete
     * service directory is assigned dynamically when a physical
     * service instance is created.
     */
    serviceDir = bigdata.serviceDir;

	// The JVM to use.
	javaHome = bigdata.javaHome;

    /* The bigdata services default logging configuration (a URI!)
     */
    log4j = bigdata.log4j;

    /*
     * Set up some default properties values that will be inherited
     * (copy by value) by all clients and services started using this
     * configuration file.
     */
    properties = new NV[] {

    /* 
     * Each JiniClient (and hence all bigdata services) can run an
     * httpd that will expose performance counters for the service and
     * the host on which it is running.  This property specifies the
     * port for that httpd service.  Valid values are port number,
     * zero (0) for a random open port, MINUS ONE (-1) to disable the
     * httpd service.
     */
    //new NV(IBigdataClient.Options.HTTPD_PORT, "-1"),

    /*
     * Option to disable collection of performance counters for the
     * host on which the client or service is running.
     *
     * Note: The load balancer relies on this information!
     */
    //new NV(IBigdataClient.Options.COLLECT_PLATFORM_STATISTICS,"false"),

    /* Option to disable collection of performance counters on the
     * queues used internally by the client or service.
     *
     * Note: The load balancer relies on this information!
     */
    //new NV(IBigdataClient.Options.COLLECT_QUEUE_STATISTICS,"false"),

    /* Option controls how many times a client request will be
     * reissued on receiving notice that an index partition locator is
     * stale.  Stale locators arise when an index partition is split,
     * moved, or joined.
     *
     * Note: This option needs to be larger if we are aggressively
     * driving journal overflows and index partitions splits during
     * the "young" phase of a data service or scale-out index since a
     * LOT of redirects will result.
     */
    new NV(IBigdataClient.Options.CLIENT_MAX_STALE_LOCATOR_RETRIES,"1000"),

    };

}

/**
 * JoinManager options.
 *
 * Note: These options must be copied into the service.config (to
 * specify the service lease timeout) as well as used by the client
 * (which uses this file directly).
 */
net.jini.lookup.JoinManager {

    // The lease timeout for jini joins.
    maxLeaseDuration = bigdata.leaseTimeout;

}

/**
 * Jini service configuration.
 */
jini {

    /* This sets command line arguments for the ServiceStarter which
     * is used to run the jini services.
     */
    args = new String[] {

        "-Xmx400m",
        "-Djava.security.policy="+bigdata.policy,
        "-Djava.util.logging.config.file="+bigdata.logging,
        "-Dlog4j.configuration="+bigdata.log4j,
        "-Dlog4j.primary.configuration="+bigdata.log4j,
        "-DinitialMemberGroups="+bigdata.fedname

    };

    /**
     * The main jini configuration file.  This file contains a
     * NonActivatableServiceDescriptor[]. The elements of that array
     * describe how to start each of the jini services.
     */
    configFile = new File("@JINI_CONFIG@");

    /**
     * The #of instances to run.
     *
     * Note: A jini service instance may be started on a host if it is
     * declared in [locators].  If locators is empty, then you are
     * using multicast discovery.  In this case an instance may be
     * started on any host, unless [constraints] are imposed.  In any
     * case, no more than [serviceCount] jini services will be started
     * at any given time.  This is checked against the #of discovered
     * instances.
     */
    serviceCount = 1;

    /**
     * @see <a href="http://sourceforge.net/apps/trac/bigdata/ticket/183">trac 183</a>
     */
    timeout = 20000 ;
}

/**
 * Zookeeper server configuration.
 */
org.apache.zookeeper.server.quorum.QuorumPeerMain {

    /* Directory for zookeeper's persistent state.  The [id] will be
     * appended as another path component automatically to keep
     * instances separate.
     */
    dataDir = new File(bigdata.serviceDir,"zookeeper");

    /* Optional directory for the zookeeper log files.  The [id] will
     * be appended as another path component automatically to keep
     * instances separate.
     * 
     * Note: A dedicated local storage device is highly recommended
     * for the zookeeper transaction logs!
     */
    //dataLogDir=new File("/var/zookeeper-log");

    // required.
    clientPort=2181;

    tickTime=2000;

    initLimit=5;

    syncLimit=2;

    /* A comma delimited list of the known zookeeper servers together
     * with their assigned "myid": {myid=host:port(:port)}+
     *
     * Note: You SHOULD specify the full list of servers that are
     * available to the federation. An instance of zookeeper will be
     * started automatically on each host running ServicesManager that
     * is present in the [servers] list IF no instance is found
     * running on that host at the specified [clientPort].
     * 
     * Note: zookeeper interprets NO entries as the localhost with
     * default peer and leader ports. This will work as long as the
     * localhost is already running zookeeper.  However, zookeeper
     * WILL NOT automatically start zookeeper if you do not specify
     * the [servers] property.  You can also explicitly specify
     * "localhost" as the hostname, but that only works for a single
     * machine.
     */
    // standalone
    //servers="1=localhost:2888:3888";
    // ensemble
    /**/
    servers =  "1="+bigdata.zoo1+":2888:3888"
//            + ",2="+bigdata.zoo2+":2888:3888"
//	    + ",3="+bigdata.zoo3+":2888:3888"
	    ;

    // This is all you need to run zookeeper.
    classpath = new String[] {
    	"@LIB_DIR@/apache/zookeeper-3.2.1.jar",
        "@LIB_DIR@/apache/log4j-1.2.15.jar"
    };

    /* Optional command line arguments for the JVM used to execute
     * zookeeper.
     *
     * Note: swapping for zookeeper is especially bad since the
     * operations are serialized, so if anything hits then disk then
     * all operations in the queue will have that latency as well.
     * However, bigdata places a very light load on
     * zookeeper so a modest heap should be Ok. For example, I have
     * observed a process size of only 94m after 10h on a 15-node
     * cluster.
     */
    args = new String[]{
	"-Xmx200m",
	/*
	 * Enable JXM remote management.
	 *
	"-Dcom.sun.management.jmxremote.port=9997",
	"-Dcom.sun.management.jmxremote.authenticate=false",
	"-Dcom.sun.management.jmxremote.ssl=false",
	 */
};

    // zookeeper server logging configuration (value is a URI!)
    log4j = bigdata.log4j;

}

/*
 * Zookeeper client configuration.
 */
org.apache.zookeeper.ZooKeeper {

    /* Root znode for the federation instance. */
    zroot = "/"+bigdata.fedname;

    /* A comma separated list of host:port pairs, where the port is
     * the CLIENT port for the zookeeper server instance.
     */
    // standalone.
    // servers = "localhost:2181";
    // ensemble
    servers =   bigdata.zoo1+":2181" // @TODO enable other instances.
//             + ","+bigdata.zoo2+":2181"
// 	    + ","+bigdata.zoo3+":2181"
	    ;

    /* Session timeout (optional). */
    sessionTimeout = bigdata.sessionTimeout;

    /* 
     * ACL for the zookeeper nodes created by the bigdata federation.
     *
     * Note: zookeeper ACLs are not transmitted over secure channels
     * and are placed into plain text Configuration files by the
     * ServicesManagerServer.
     */
    acl = new ACL[] {

	new ACL(ZooDefs.Perms.ALL, new Id("world", "anyone"))

    };

}

/*
 * Jini client configuration
 */
com.bigdata.service.jini.JiniClient {

    /* Default Entry[] for jini services.  Also used by the
     * ServicesManagerService as is.
     *
     * Note: A Name attribute will be added automatically using the
     * service type and the znode of the service instance.  That Name
     * will be canonical.  It is best if additional service names are
     * NOT specified as that might confuse somethings :-)
     *
     * Note: A Hostname attribute will be added dynamically.
     */
    entries = new Entry[] {
	// Purely informative.
	new Comment(bigdata.fedname),
    };

    groups = bigdata.groups;

    locators = bigdata.locators;

    // optional JiniClient properties.
    // properties = new NV[] {};

    /*
     * Overrides for jini SERVICES (things which are started
     * automatically) BUT NOT CLIENTs (things which you start by hand
     * and which read this file directly).
     *
     * The difference here is whether or not a service.config file is
     * being generated.  When it is, the jiniOptions[] will be
     * included in how that service is invoked and will operate as
     * overrides for the parameters specified in the generated
     * service.config file.  However, normal clients directly consume
     * this config file rather than the generated one and therefore
     * you must either specify their overrides directly on the command
     * line when you start the client or specify them explicitly in
     * the appropriate component section within this configuration
     * file.
     *
     * In practice, this means that you must specify some parameters
     * both here and in the appropriate component configuration. E.g.,
     * see the component section for "net.jini.lookup.JoinManager"
     * elsewhere in this file.
     */
    jiniOptions = new String[] {

	// The lease timeout for jini joins.
	"net.jini.lookup.JoinManager.maxLeaseDuration="+bigdata.leaseTimeout,

    };

}

/**
 * Options for the bigdata services manager.
 */
com.bigdata.jini.start.ServicesManagerServer {

    /*
     * This object is used to export the service proxy.  The choice
     * here effects the protocol that will be used for communications
     * between the clients and the service.
     */
    exporter = new BasicJeriExporter(TcpServerEndpoint.getInstance(0),
                                     new BasicILFactory()); 

    /*                                          
     * The data directory and the file on which the serviceID will be
     * written.
     *
     * Note: These properties MUST be specified explicitly for the
     * ServicesManager since it uses this as its Configuration file.
     * For other services, it generates the Configuration file and
     * will generate this property as well.
     */

    serviceDir = new File(bigdata.serviceDir,"ServicesManager");

    serviceIdFile = new File(serviceDir,"service.id");
    
    /* The services that will be started.  For each service, there
     * must be a corresponding component defined within this
     * configuration file.  For each "ManagedServiceConfiguration", an
     * entry will be made in zookeeper and logical and physical
     * service instances will be managed automatically.  For unmanaged
     * services, such as jini and zookeeper itself, instances will be
     * started iff necessary by the services manager when it starts
     * up.
     */
    services = new String[] {
	
      	"jini",
	"org.apache.zookeeper.server.quorum.QuorumPeerMain",
	"com.bigdata.service.jini.TransactionServer",
    	"com.bigdata.service.jini.MetadataServer",
    	"com.bigdata.service.jini.DataServer",
    	"com.bigdata.service.jini.LoadBalancerServer",
    	"com.bigdata.service.jini.ClientServer"
	
    };

    /*
     * Additional properties passed through to the JiniClient or the
     * service.
     *
     * Note: The services manager is used to collect statistics from the
     * OS for each host so we have performance counters for hosts which
	 * are only running non-bigdata services, such as jini or zookeeper.
     */
    properties = new NV[]{

    };

    /* The services manager MUDT be run on every host so that it may
     * start both bigdata and non-bigdata services (jini, zookeeper).
     * This is also used to report per-host performance counters to
     * the load balancer for hosts that are not running bigdata 
     * services.
     */
    constraints = new IServiceConstraint[] {

    };

}

com.bigdata.service.jini.TransactionServer {

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),

	new HostAllowConstraint(bigdata.txs)

    };

    args = new String[]{
	
	// Does not need much RAM.
	"-Xmx200m"
	
    };
	
    properties = new NV[] {
	
	/* The #of milliseconds that the database will retain history no
	 * longer required to support the earliest active transaction.
	 *
	 * A value of ZERO means that only the last commit point will
	 * be retained.  The larger the value the more history will be
	 * retained.  You can use a really big number if you never want
	 * to release history and you have lots of disk space :-)
	 *
	 * Note: The most recent committed state of the database is
	 * NEVER released.
	 */
	new NV(TransactionServer.Options.MIN_RELEASE_AGE, "0"),

	};
	
}

com.bigdata.service.jini.MetadataServer {

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),
	//new TXRunningConstraint(),

	new HostAllowConstraint(bigdata.mds),

    };

    args = new String[]{
	
		// Does not need much RAM.
		"-Xmx200m"
	
    };

    properties = new NV[]{

    /*
	 * The MDS does not support overflow at this time so
	 * overflow MUST be disabled for this service.
	 */
        new NV(MetadataServer.Options.OVERFLOW_ENABLED,"false")    

    };

}

com.bigdata.service.jini.DataServer {

    args = new String[]{
	/* 
	 * Grant lots of memory, but read on.
	 *
	 * Note: 32-bit JVMs have a 2G limit on the heap, but the practical limit
	 * is often much less - maybe 1400m.  64-bit JVMs can use much more RAM.
	 * However, the heap which you grant to java DOES NOT determine the total
	 * process heap.  I have seen 64-bit java processes using an additional
	 * 3-4GB of heap beyond what is specified here.  So, you need to consider
	 * the total RAM, subtract out enough for the other processes and the OS
	 * buffers, divide by the #of client/data services you plan to run on that
	 * host (generally 1-2) and then subtract out some more space for the JVM
	 * itself.  
	 *
	 * For example, if you have 32G RAM and a 64-bit JVM and plan to run two
	 * CS/DS on the host, I would recommend 10G for the Java heap.  You can
	 * expect to see Java grab another 4G per process over time.  That makes
	 * the per CS/DS heap 14G.  With two processes you have taken 28G leaving
	 * 4G for everything else.
	 *
	 * Here is another example: 4G RAM, 32-bit JVM, and 2 CS/DS per host.  I
	 * would stick to 800m for the Java heap.  You don't have a problem unless
	 * you see an OOM (OutOfMemoryException) or a process killed because GC is
	 * taking too much time.
	 *
	 * See http://www.ibm.com/developerworks/linux/library/j-nativememory-linux/index.html?ca=dgr-lnxw07Linux-JVM&S_TACT=105AGX59&S_CMP=grlnxw07
	 *
	 * Note: for linux, "sysctl -w vm.swappiness=0" will keep the RAM you do
	 * have for your applications!
	 */
	"-Xmx1600m",// was 800
	/* Pre-allocation of the DS heap is no longer recommended.
	 *
	 * See https://sourceforge.net/apps/trac/bigdata/ticket/157
	"-Xms800m",
	 */
	/*
	 * This option will keep the JVM "alive" even when it is memory starved
	 * but perform of a memory starved JVM is terrible.
	 */
	//"-XX:-UseGCOverheadLimit",
	/* Configure GC for higher throughput.  Together these options
	 * request parallel old generation collection using N threads.
	 * The application will be paused when this occurs, but GC will
	 * be faster.  Hence throughput will be higher.  However, be
	 * sure to use JDK 6u10+ (6676016 : ParallelOldGC leaks memory).
	 * 
	 * http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6676016
	 */
	"-XX:+UseParallelOldGC",
	//"-XX:ParallelGCThreads=8",
	/*
	 * Enable JXM remote management for the data service.
	 *
	 * Note: This will not work if you have two data services on a host 
	 * because it will assign the same port to each service.  In order
	 * to work around that the argument would have to be specified by
	 * the service starter and then published in the Entry[] attributes.
	 *
	 * However, you can use ssh -X to open a tunnel with X
	 * forwarding and then run jconsole locally on the target host
	 * and bring up these data services without enabling remote
	 * JMX.
	 *
	"-Dcom.sun.management.jmxremote.port=9999",
	"-Dcom.sun.management.jmxremote.authenticate=false",
	"-Dcom.sun.management.jmxremote.ssl=false",
	 */
         /*
         * Override the size of the default pool of direct (native) byte
         * buffers.  This was done to ensure that the nodes region for
         * index segments remain fully buffered as the index partitions
         * approach their maximum size before a split.
         */
         "-Dcom.bigdata.io.DirectBufferPool.bufferCapacity="+
                ConfigMath.multiply(Bytes.kilobyte,1250),
     };

    serviceCount = bigdata.dataServiceCount;

    // restrict where the data services can run.
    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),
	//new TXRunningConstraint(),

	new HostAllowConstraint(bigdata.ds),

	new MaxDataServicesPerHostConstraint(bigdata.maxDataServicesPerHost),

    };

    /*
     * Note: the [dataDir] will be filled in when a new service
     * instance is created based on the [servicesDir], so don't set it
     * here yourself.
     */
    properties = new NV[]{

	new NV(DataServer.Options.BUFFER_MODE,
	    //""+com.bigdata.journal.BufferMode.Direct
	    ""+com.bigdata.journal.BufferMode.DiskWORM
	    ),

	/* Option disables synchronous overflow after N times and
	 * configures the offset bits for the journal for a scale-up
	 * configuration so we may use very large journals.
	 */
	//new NV(DataServer.Options.OVERFLOW_MAX_COUNT,"5"),
	//new NV(DataServer.Options.OFFSET_BITS,""+com.bigdata.rawstore.WormAddressManager.SCALE_UP_OFFSET_BITS),

	/* Synchronous overflow is triggered when the live journal is
	 * this full (the value is a percentage, expressed as a
	 * floating point number in [0:1]).
	 */
	//new NV(DataServer.Options.OVERFLOW_THRESHOLD,".9"),

	/* Override the initial and maximum extent so that they are more
	 * more suited to large data sets.  Overflow will be triggered as
	 * the size of the journal approaches the maximum extent.  The
	 * initial and maximum extent are configured up above.
	 */

	new NV(DataServer.Options.INITIAL_EXTENT, "" + bigdata.journalExtent),
	new NV(DataServer.Options.MAXIMUM_EXTENT, "" + bigdata.journalExtent),

	/* Specify the queue capacity for the write service (unisolated
	 * write operations).
	 * 
	 * 0 := SynchronousQueue.
	 * N := bounded queue of capacity N
	 * Integer.MAX_VALUE := unbounded queue. 
	 *
	 * Note: The corePoolSize will never increase for an unbounded
	 * queue so the value specified for maximumPoolSize will
	 * essentially be ignored in this case. 
	 *
	 * Note: A SynchronousQueue is a good choice here since it allows
	 * the #of threads to change in response to demand.  The pool
	 * size should be unbounded when using a SynchronousQueue.
	 */
	new NV(DataServer.Options.WRITE_SERVICE_QUEUE_CAPACITY,"0"), // synchronous queue.
	new NV(DataServer.Options.WRITE_SERVICE_CORE_POOL_SIZE,"50"), //
	new NV(DataServer.Options.WRITE_SERVICE_MAXIMUM_POOL_SIZE,""+Integer.MAX_VALUE),
	new NV(DataServer.Options.WRITE_SERVICE_PRESTART_ALL_CORE_THREADS,"true"),

	/*
	 * Options turns off overflow processing (debugging only).
	 * All writes will go onto the live journal, no index segments
	 * will be built, and indices will not be split, moved,
	 * joined, etc.
	 */
	//new NV(DataServer.Options.OVERFLOW_ENABLED,"false"),

	/* Maximum #of index partition moves per overflow.
	 */
	new NV(DataServer.Options.MAXIMUM_MOVES,"1"),

	/* Option controls how many index partitions may be moved onto
	 * any given target data service in a single overflow cycle
	 * and may be used to disable index partition moves (for
	 * debugging purposes).
	 */
	new NV(DataServer.Options.MAXIMUM_MOVES_PER_TARGET,"1"),

	/* The minimum CPU activity on a host before it will consider moving an
	* index partition to shed some load.
	*
	* @todo A high threshold was chosen for the 3-node cluster since there
	* are only 2 machines running data services.  A "feature" in the load
	* balancer allows moves between two heavily loaded hosts even when they
	* are very close in their load, which is typically the case if you have
	* only 2 machines running data services.  The high threshold here is a
	* workaround until the load balancer is modified to take into account
	* whether or not a significant difference exists in the load between
	* the source and possible target data service hosts.
	*/
	new NV(DataServer.Options.MOVE_PERCENT_CPU_TIME_THRESHOLD,".99"),//was .7

	/* Option limits the #of index segments in a view before a
	 * compacting merge is forced.
	 */
	new NV(DataServer.Options.MAXIMUM_SEGMENTS_PER_VIEW,"5"), // default 6

	/* Option limits the #of optional merges that are performed in each
	 * overflow cycle.
	 */
	new NV(DataServer.Options.MAXIMUM_OPTIONAL_MERGES_PER_OVERFLOW,"1"),

	/* Option effects how much splits are emphasized for a young
	 * scale-out index.  If the index has fewer than this many
	 * partitions, then there will be a linear reduction in the
	 * target index partition size which will increase the likelyhood
	 * of an index split under heavy writes. This helps to distribute
	 * the index early in its life cycle.
	 */
	new NV(DataServer.Options.ACCELERATE_SPLIT_THRESHOLD,"20"),//20//50

	/* Options accelerates overflow for data services have fewer than
	 * the threshold #of bytes under management.  Acceleration is
	 * accomplished by reducing the maximum extent of the live journal
	 * linearly, but with a minimum of a 10M maximum extent.  When the
	 * maximum extent is reduced by this option, the initial and the
	 * maximum extent will always be set to the same value for that
	 * journal.
	 */
	new NV(DataServer.Options.ACCELERATE_OVERFLOW_THRESHOLD,
	    //"0"
	    //""+com.bigdata.rawstore.Bytes.gigabyte
    	    "2147483648" // 2G
	    ),

    // #of threads for index segment builds (default 3).
    new NV(DataServer.Options.BUILD_SERVICE_CORE_POOL_SIZE,"5"),

    // #of threads for compacting merges (default 1).
    new NV(DataServer.Options.MERGE_SERVICE_CORE_POOL_SIZE,"2"),

//	// Zero is full parallelism; otherwise #of threads in the pool.
//	new NV(DataServer.Options.OVERFLOW_TASKS_CONCURRENT,"5"),

	/* Use Long.MAX_VALUE to always run overflow processing to
	 * completion (until no more data remains on the old journal).
	 */
	new NV(DataServer.Options.OVERFLOW_TIMEOUT,""+Long.MAX_VALUE),

	new NV(DataServer.Options.OVERFLOW_CANCELLED_WHEN_JOURNAL_FULL,"false"),

    new NV(DataServer.Options.LIVE_INDEX_CACHE_CAPACITY,"10"), // was 60

    new NV(DataServer.Options.HISTORICAL_INDEX_CACHE_CAPACITY,"10"), // was 60

        /* The maximum #of clean indices that will be retained on the
         * hard reference queue (default 20).
         */
        new NV(DataServer.Options.INDEX_CACHE_CAPACITY,"10"), // was 50

        /* The timeout for unused index references before they are
         * cleared from the hard reference queue (default is 1m).
         * After this timeout the index reference is cleared from the
         * queue and the index will be closed unless a hard reference
         * exists to the index.
         */
//       new NV(DataServer.Options.INDEX_CACHE_TIMEOUT,"1200000"), // 20m vs 1m

        /* The maximum #of clean index segments that will be retained
         * on the hard reference queue (default 60).  Note that ALL
         * index segments are clean (they are read-only).
         */
        new NV(DataServer.Options.INDEX_SEGMENT_CACHE_CAPACITY,"20"), // was 100

        /* The timeout for unused index segment references before they
         * are cleared from the hard reference queue (default is 1m).
         * After this timeout the index segment reference is cleared
         * from the queue and the index segment will be closed unless
         * a hard reference exists to the index segment.
         */
//       new NV(DataServer.Options.INDEX_SEGMENT_CACHE_TIMEOUT,"60000000"), // 10m vs 1m

        /* The #of store files (journals and index segment stores)
         * whose hard references will be maintained on a queue.  The
         * value should be slightly more than the index segment cache
         * capacity since some journals also used by the views, but
         * same journals are shared by all views so adding 3 is plenty..
         */
        new NV(DataServer.Options.STORE_CACHE_CAPACITY,"23"),// was 110

//       new NV(DataServer.Options.STORE_CACHE_TIMEOUT,"1200000"),//20m vs 1m. 
	
    };

}

/**
 * Configuration options for the containers used to distribute application tasks
 * across a federation.
 *
 * @todo There should be a means to tag certain client servers for one purpose
 * or another.  This could be handled by subclassing, but it really should be
 * declarative.
 */
com.bigdata.service.jini.ClientServer {

    args = new String[]{
	/* 
	 * Grant lots of memory, but read on.
	 *
	 * Note: 32-bit JVMs have a 2G limit on the heap, but the practical limit
	 * is often much less - maybe 1400m.  64-bit JVMs can use much more RAM.
	 * However, the heap which you grant to java DOES NOT determine the total
	 * process heap.  I have seen 64-bit java processes using an additional
	 * 3-4GB of heap beyond what is specified here.  So, you need to consider
	 * the total RAM, subtract out enough for the other processes and the OS
	 * buffers, divide by the #of client/data services you plan to run on that
	 * host (generally 1-2) and then subtract out some more space for the JVM
	 * itself.  
	 *
	 * For example, if you have 32G RAM and a 64-bit JVM and plan to run two
	 * CS/DS on the host, I would recommend 10G for the Java heap.  You can
	 * expect to see Java grab another 4G per process over time.  That makes
	 * the per CS/DS heap 14G.  With two processes you have taken 28G leaving
	 * 4G for everything else.
	 *
	 * Here is another example: 4G RAM, 32-bit JVM, and 2 CS/DS per host.  I
	 * would stick to 800m for the Java heap.  You don't have a problem unless
	 * you see an OOM (OutOfMemoryException) or a process killed because GC is
	 * taking too much time.
	 *
	 * See http://www.ibm.com/developerworks/linux/library/j-nativememory-linux/index.html?ca=dgr-lnxw07Linux-JVM&S_TACT=105AGX59&S_CMP=grlnxw07
	 *
	 * Note: for linux, "sysctl -w vm.swappiness=0" will keep the RAM you do
	 * have for your applications!
	 */
	"-Xmx1600m", // was 800m
	/*
	 * This option will keep the JVM "alive" even when it is memory starved
	 * but perform of a memory starved JVM is terrible.
	 */
	//"-XX:-UseGCOverheadLimit",
	/* Configure GC for higher throughput.  Together these options
	 * request parallel old generation collection using N threads.
	 * The application will be paused when this occurs, but GC will
	 * be faster.  Hence throughput will be higher.
	 */
	"-XX:+UseParallelOldGC",
	//"-XX:ParallelGCThreads=8",
	/*
	 * Enable JXM remote management for the data service.
	 *
	 * Note: This will not work if you have two such services on a host 
	 * because it will assign the same port to each service.  In order
	 * to work around that the argument would have to be specified by
	 * the service starter and then published in the Entry[] attributes.
	 *
	 * However, you can use ssh -X to open a tunnel with X
	 * forwarding and then run jconsole locally on the target host
	 * and bring up these data services without enabling remote
	 * JMX.
	 *
	"-Dcom.sun.management.jmxremote.port=9996",
	"-Dcom.sun.management.jmxremote.authenticate=false",
	"-Dcom.sun.management.jmxremote.ssl=false",
	 */
    };

    serviceCount = bigdata.clientServiceCount;

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),

	new HostAllowConstraint(bigdata.cs),

	new MaxClientServicesPerHostConstraint(bigdata.maxClientServicePerHost),

    };

    properties = new NV[] {
	
    };

}

com.bigdata.service.jini.LoadBalancerServer {

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),

	new HostAllowConstraint(bigdata.lbs)

    };

    args = new String[]{
    /*
     * FIXME The load balancer is a big piggish on long runs because it
     * keeps the performance counter histories in RAM.  While those histories
     * are bounded, it still uses more RAM than it should.
     */
	"-Xmx1G",
	/*
	 * Enable JXM remote management for the data service.
	 *
	 * Note: This will not work if you have two data services on a host 
	 * because it will assign the same port to each service.  In order
	 * to work around that the argument would have to be specified by
	 * the service starter and then published in the Entry[] attributes.
	 *
	"-Dcom.sun.management.jmxremote.port=9998",
	"-Dcom.sun.management.jmxremote.authenticate=false",
	"-Dcom.sun.management.jmxremote.ssl=false",
	 */
    };

    /*
     * Override some properties.
     */
    properties = new NV[] {

    /* 
     * Each JiniClient (and hence all bigdata services) can run an
     * httpd that will expose performance counters for the service and
     * the host on which it is running.  This property specifies the
     * port for that httpd service.  Valid values are port number,
     * zero (0) for a random open port, MINUS ONE (-1) to disable the
     * httpd service.
     *
     * Note: The load balancer httpd normally uses a known port so
     * that it is easy to find.  This is where you will find all of
     * the performance counters aggregated for the entire federation,
     * including their history.
     */
    new NV(IBigdataClient.Options.HTTPD_PORT, "@LOAD_BALANCER_PORT@"),

    /*
     * Note: The load balancer SHOULD NOT collect platform statistics
     * itself since that interfers with its ability to aggregate
     * statistics about the host on which it is running.  Instead it
     * should rely on the presence of at least one other service
     * running on the same host to report those statistics to the load
     * balancer.
     */
    new NV(IBigdataClient.Options.COLLECT_PLATFORM_STATISTICS,"false"),
		
    /* 
     * The directory where the aggregated statistics will be logged.
     * The load balancer will write snapshots of the historical
     * counters into this directory.  See LoadBalancerService javadoc
     * for configuration options which effect how frequently it will
     * log its counters and how many snapshots will be preserved.
     *
     * Note: You only need to specify this option if you want to put
     * the files into a well known location, e.g, on a shared volume.
     */
    //new NV(LoadBalancerServer.Options.LOG_DIR,"/opt2/var/log/bigdata"),

    /* Option essentially turns off the load-based decision making for
     * this many minutes and substitutes a round-robin policy for
     * recommending the least utilized data services.  The main reason
     * to this is to force the initial allocation to be distributed as
     * evenly as possible across the data services in the cluster.
     */
    new NV(LoadBalancerServer.Options.INITIAL_ROUND_ROBIN_UPDATE_COUNT,"10"),

    };

}

/**
 * Configuration options for the KB instance.
 */
lubm {

    // The #of universities to generate. 
    // U8000   is 1.2B told triples
    // U25000  is 3.4B told triples.
    // U50000  is 6.7B told triples.
    // U100000 is ~12B told triples.
    static private univNum = 100;

    // the KB namespace (based on the #of universities by default).
    static private namespace = "U"+univNum+"";

    // minimum #of data services to run.
    static private minDataServices = bigdata.dataServiceCount;

    // How long the master will wait to discover the minimum #of data
    // services that you specified (ms).
    static private awaitDataServicesTimeout = 8000;

    /* Multiplier for the scatter effect.
     */
    static private scatterFactor = 2;
    static private scatterFactor_term2id = 2; // use 1 @ 4DS and up.

    /* The #of index partitions to allocate on a scatter split.  ZERO
     * (0) means that 2 index partitions will be allocated per
     * data service which partiticpates in the scatter split.
     * Non-zero values directly give the #of index partitions to
     * create.  
     */
    static private scatterSplitIndexPartitionCount = ConfigMath.multiply
	( scatterFactor,
	  bigdata.dataServiceCount
	  );
    static private scatterSplitIndexPartitionCount_term2id = ConfigMath.multiply
	( scatterFactor_term2id,
	  bigdata.dataServiceCount
	  );

    // Use all discovered data services when scattering an index.
    static private scatterSplitDataServiceCount = 0;

    /* Scatter split trigger point.  The scatter split will not be
     * triggered until the initial index partition has reached
     * this percentage of a nominal index partition in size.
     */
    static private scatterSplitPercentOfSplitThreshold = 0.5;//was .5

	/* 
	 * Multipliers that compensate for the consumer/producer ratio for
	 * the asynchronous index write API.  These are empirical factors
	 * based on observing the ratio (chunkWritingTime/chunkWaitingTime).
	 * Assuming a constant chunk writing time, if the chunk size for each
	 * index is adjusted by its multiplier then this ratio would be 1:1.
	 * In practice, the chunk writing time is not a linear function of
	 * the chunk size, which is one reason why we prefer larger chunks
	 * and why the asynchronous write API is a win.
	 *
	 * Note: These factors were set relative to TERM2ID.  However, when
	 * I reduced the scatterFactor for TERM2ID by 1/2, I doubled its
	 * chunk size to keep up the same throughput so it is now at 2.00
	 * rather than 1.00.
	 */
	static private chunkSizeFactor_id2term =  1.79;
	static private chunkSizeFactor_term2id =  2.00;
	static private chunkSizeFactor_spo     =  8.00; // was 3.89
	static private chunkSizeFactor_pos     =  8.00; // was 13.37
	static private chunkSizeFactor_osp     =  8.00; // was 27.35
	
	/* The nominal sink chunk size.  For each index, this is adjusted
	 * by the factor specified above.
	 */
	static private sinkChunkSize = 10000;
	 
    /*
     * Specify / override some triple store properties.
     *
     * Note: You must reference this object in the section for the
     * component which will actually create the KB instance, e.g.,
     * either the RDFDataLoadMaster or the LubmGeneratorMaster.
     */
    static private properties = new NV[] {
	
        /*
         * When "true", the store will perform incremental closure as
         * the data are loaded. When "false", the closure will be
         * computed after all data are loaded. (Actually, since we are
         * not loading through the SAIL making this true does not
         * cause incremental TM but it does disable closure, so
         * "false" is what you need here).
         */
        new NV(BigdataSail.Options.TRUTH_MAINTENANCE, "false" ),

        /*
         * Enable rewrites of high-level queries into native rules (native JOIN
         * execution). (Can be changed without re-loading the data to compare
         * the performance of the Sesame query evaluation against using the
         * native rules to perform query evaluation.)
         */
        new NV(BigdataSail.Options.NATIVE_JOINS, "true"),

        /*
         * May be used to turn off inference during query, but will
	 * cause ALL inferences to be filtered out when reading on the
	 * database.
         */
        // new NV(BigdataSail.Options.INCLUDE_INFERRED, "false"),

        /*
         * May be used to turn off query-time expansion of entailments such as
         * (x rdf:type rdfs:Resource) and owl:sameAs even through those
         * entailments were not materialized during forward closure (this
	 * disables the backchainer!)
         */
        new NV(BigdataSail.Options.QUERY_TIME_EXPANDER, "false"),

        /*
         * Option to restrict ourselves to RDFS only inference. This
         * condition may be compared readily to many other stores.
         * 
         * Note: While we can turn on some kinds of owl processing
         * (e.g., TransitiveProperty, see below), we can not compute
         * all the necessary entailments (only queries 11 and 13
         * benefit).
         * 
         * Note: There are no owl:sameAs assertions in LUBM.
         * 
         * Note: lubm query does not benefit from owl:inverseOf.
         * 
         * Note: lubm query does benefit from owl:TransitiveProperty
         * (queries 11 and 13).
         * 
         * Note: owl:Restriction (which we can not compute) plus
         * owl:TransitiveProperty is required to get all the answers
         * for LUBM.
         */
        new NV(BigdataSail.Options.AXIOMS_CLASS, "com.bigdata.rdf.axioms.RdfsAxioms"),
        // new NV(BigdataSail.Options.AXIOMS_CLASS,"com.bigdata.rdf.axioms.NoAxioms"),

        /*
         * Produce a full closure (all entailments) so that the
         * backward chainer is always a NOP. Note that the
         * configuration properties are stored in the database (in the
         * global row store) so you always get exactly the same
         * configuration that you created when reopening a triple
         * store.
         */
        // new NV(BigdataSail.Options.FORWARD_CHAIN_RDF_TYPE_RDFS_RESOURCE, "true"),
        // new NV(BigdataSail.Options.FORWARD_CHAIN_OWL_SAMEAS_PROPERTIES, "true"),

        /*
         * Additional owl inferences. LUBM only both inverseOf and
         * TransitiveProperty of those that we support (owl:sameAs,
         * owl:inverseOf, owl:TransitiveProperty), but not owl:sameAs.
         */
        new NV(BigdataSail.Options.FORWARD_CHAIN_OWL_INVERSE_OF, "true"),
        new NV(BigdataSail.Options.FORWARD_CHAIN_OWL_TRANSITIVE_PROPERTY, "true"),

        // Note: FastClosure is the default.
        // new NV(BigdataSail.Options.CLOSURE_CLASS, "com.bigdata.rdf.rules.FullClosure"),

        /*
         * Various things that effect native rule execution.
         */
        // new NV(BigdataSail.Options.FORCE_SERIAL_EXECUTION, "false"),
        // new NV(BigdataSail.Options.MUTATION_BUFFER_CAPACITY, "20000"),
        new NV(BigdataSail.Options.CHUNK_CAPACITY, "100"),
        // new NV(BigdataSail.QUERY_BUFFER_CAPACITY, "10000"),
        // new NV(BigdataSail.FULLY_BUFFERED_READ_THRESHOLD, "10000"),

        /*
         * Turn off incremental closure in the DataLoader object.
         */
        new NV(com.bigdata.rdf.store.DataLoader.Options.CLOSURE, "None"),
	//com.bigdata.rdf.store.DataLoader.ClosureEnum.None.toString()),

        /*
         * Turn off commit in the DataLoader object. We do not need to commit
         * anything until we have loaded all the data and computed the closure
         * over the database.
         */
        new NV(com.bigdata.rdf.store.DataLoader.Options.COMMIT,"None"),
	//com.bigdata.rdf.store.DataLoader.CommitEnum.None.toString()),

        /*
         * Provide Unicode support for keys with locale-based string
         * collation. This is more expensive in key creation during
         * loading, but allows key comparison and sorting in the
         * specified locale in queries.
	 *
	 * @see com.bigdata.btree.keys.CollatorEnum
         */
        new NV(KeyBuilder.Options.COLLATOR,"ICU"),
        new NV(KeyBuilder.Options.USER_LANGUAGE,"en"),
        new NV(KeyBuilder.Options.USER_COUNTRY,"US"),
        new NV(KeyBuilder.Options.USER_VARIANT,""),

        /*
         * Turn off the full text index (search for literals by keyword).
         */
	new NV(BigdataSail.Options.TEXT_INDEX, "false"),

        /*
         * Turn on bloom filter for the SPO index (good up to ~2M
         * index entries for scale-up -or- for any size index for
         * scale-out).  This is a big win for some queries on
         * scale-out indices since we can avoid touching the disk if
         * the bloom filter reports "false" for a key.
         */
        new NV(BigdataSail.Options.BLOOM_FILTER, "true"),

	/* The #of low order bits from the TERM2ID index partition
	 * local counter that will be reversed and written into the
	 * high-order bits of the term identifier.  This has a strong
	 * effect on the distribution of bulk index read/write
	 * operations for the triple store.  For a given value of N, a
	 * bulk write will tend to touch 2^N index partitions.
	 * Therefore if this is is even roughly on the order of the number
	 * of index partitions, each bulk write will tend to be scattered
	 * to all index partitions.
	 *
	 * Note: If this value is too large then the writes WITHIN the
	 * index partitions will become uniformly distributed, which
	 * will negatively impact index performance.
	 */
        new NV(BigdataSail.Options.TERMID_BITS_TO_REVERSE,"6"),//was 8, was 6

        /*
         * Turn off statement identifiers (support for statements
         * about statements).
         */
        new NV(BigdataSail.Options.STATEMENT_IDENTIFIERS, "false"),

        /*
	 * Option may be enabled to store blank nodes such that they
	 * are stable (they are not stored by default).
	 *
         * @todo LUBM uses blank nodes. Therefore re-loading LUBM will
         * always cause new statements to be asserted and result in
         * the closure being updated if it is recomputed. Presumably
         * you can tell bigdata to store the blank nodes and RIO to
         * preserve them, but it does not seem to work (RIO creates
         * new blank nodes on reparse). Maybe this is a RIO bug?
         */
        // new NV(BigdataSail.Options.STORE_BLANK_NODES,"true");

        /*
         * Turn off justification chains.  This impacts only the load
         * performance, but it is a big impact and only required if
         * you will be doing truth maintenance (TM). Also, truth
         * maintenance based on the justification chains does not
         * scale-out.  (We are planning a magic sets integration to
         * take care of that).
         */
        new NV(BigdataSail.Options.JUSTIFY, "false"),

        /*
         * Choice of the join algorithm.
         * 
         * false is pipeline, which scales-out.
         * 
         * true is nested, which is also the default right now but
         * does not scale-out.
         */
        new NV(BigdataSail.Options.NESTED_SUBQUERY, "false"),

        /*
         * Maximum #of subqueries to evaluate concurrently for the 1st
         * join dimension for native rules. Zero disables the use of
         * an executor service. One forces a single thread, but runs
         * the subquery on the executor service. N>1 is concurrent
         * subquery evaluation.
	 *
	 * Note: parallel subquery does not work for pipeline joins at
	 * this time so this option is only safe for nested subquery
	 * join, and nested subquery joins do not scale-out.
         */
        // new NV(BigdataSail.Options.MAX_PARALLEL_SUBQUERIES, "5"),
        new NV(BigdataSail.Options.MAX_PARALLEL_SUBQUERIES, "0"),

	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.WRITE_RETENTION_QUEUE_CAPACITY
		 ), "500"),// default 500

    new NV(com.bigdata.config.Configuration.getOverrideProperty
           ( namespace,
         IndexMetadata.Options.BTREE_BRANCHING_FACTOR
         ), "32"),// default 64

	/*
	 * Turn on direct buffering for index segment nodes.
	 */
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.INDEX_SEGMENT_BUFFER_NODES
		  ), "true"),

	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.INDEX_SEGMENT_BRANCHING_FACTOR
		 ), "512"),// default 512

	/*
	 * Tweak the asynchronous write API parameters.
	 */

	// Dial down the master/sink queue capacities to rein in RAM.
	// default is 5k.  The sink is the big RAM consumer since there
	// is one since per index partition (and one thread per sink).
	// Note: A cluster with 32-bit JVMs needs to constrain the size
	// of the sink queues or it will run out of memory.
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.MASTER_QUEUE_CAPACITY
		  ), "10"), // was 5; was 1000; was 5000
	// default is 5k
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SINK_QUEUE_CAPACITY
		  ), "10"), // was 25; was 500, was 5000

	// increase the poll duration for the sinks to reduce CPU demand.
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SINK_POLL_TIMEOUT_NANOS
		  ), ""+ConfigMath.ms2ns(2000)),

 	// Override master/sink queue capacity for TERM2ID since uses KVOLatch
 	// and requires all results to be in before writes on the other indices
 	// may proceed.
// 	new NV(com.bigdata.config.Configuration.getOverrideProperty
// 	       ( namespace + "."
// 				    + LexiconRelation.NAME_LEXICON_RELATION + "."
// 				    + LexiconKeyOrder.TERM2ID,
// 		 IndexMetadata.Options.MASTER_QUEUE_CAPACITY
// 		 ), "1000"),// default 5000
// 	new NV(com.bigdata.config.Configuration.getOverrideProperty
// 	       ( namespace + "."
// 				    + LexiconRelation.NAME_LEXICON_RELATION + "."
// 				    + LexiconKeyOrder.TERM2ID,
// 		 IndexMetadata.Options.SINK_QUEUE_CAPACITY
//		 ), "1000"),// default 5000

	// default is 10k (global adjustment)
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SINK_CHUNK_SIZE
		  ), ""+sinkChunkSize), // was 20000

	/* per index adjustment. */
 	new NV(com.bigdata.config.Configuration.getOverrideProperty
 	       ( namespace + "."
 				    + LexiconRelation.NAME_LEXICON_RELATION + "."
 				    + LexiconKeyOrder.TERM2ID,
 		 IndexMetadata.Options.SINK_CHUNK_SIZE
		 ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_term2id)),
 	new NV(com.bigdata.config.Configuration.getOverrideProperty
 	       ( namespace + "."
 				    + LexiconRelation.NAME_LEXICON_RELATION + "."
 				    + LexiconKeyOrder.ID2TERM,
 		 IndexMetadata.Options.SINK_CHUNK_SIZE
		 ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_id2term)),
    new NV(com.bigdata.config.Configuration.getOverrideProperty
           ( namespace + "."    + SPORelation.NAME_SPO_RELATION + "."
                                + SPOKeyOrder.SPO,
             IndexMetadata.Options.SINK_CHUNK_SIZE
             ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_spo)),
    new NV(com.bigdata.config.Configuration.getOverrideProperty
       ( namespace + "."        + SPORelation.NAME_SPO_RELATION + "."
                                + SPOKeyOrder.POS,
             IndexMetadata.Options.SINK_CHUNK_SIZE
             ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_pos)),
    new NV(com.bigdata.config.Configuration.getOverrideProperty
           ( namespace + "."    + SPORelation.NAME_SPO_RELATION + "."
                                + SPOKeyOrder.OSP,
             IndexMetadata.Options.SINK_CHUNK_SIZE
             ), ""+(int)ConfigMath.multiply(sinkChunkSize,chunkSizeFactor_osp)),

// 	// default is infinite
// 	new NV( com.bigdata.config.Configuration.getOverrideProperty
// 		( namespace,
// 		  IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
// 		  ), ""+Long.MAX_VALUE),//ConfigMath.s2ns(120)),

 	/* The default is infinite.  A smaller value is necessary to close down sinks
 	 * on which the client never writes but which are not closed by a redirect. 
 	 * The idle timeout will cause sinks which are not receiving ANY chunks to
 	 * close themselves.  Sinks which are receiving chunks but are not filling
 	 * their target chunk size in time will be flushed when the chunk timeout is
 	 * exceeded.
 	 */
 	new NV( com.bigdata.config.Configuration.getOverrideProperty
 		( namespace,
 		  IndexMetadata.Options.SINK_IDLE_TIMEOUT_NANOS
		  ), ""+ConfigMath.s2ns(60)),

	/* The TERM2ID index uses idle timeout to preserve liveness for async writes.
	 * This is required in order for the async loader to not wait forever for the
	 * last full chunk when bulk loading with async writes on the TERM2ID index.
	 * The value of this timeout effects both how large the chunks will be and
	 * how long after the last document is parsed it will be until the TERM2ID
	 * async write buffers decide that they are idle and evict the last of their
	 * chunks.  This also effects the throughput since the larger chunk sizes
	 * correlates very well with higher throughput.
	 */
         new NV(com.bigdata.config.Configuration.getOverrideProperty
 	       ( namespace + "." + LexiconRelation.NAME_LEXICON_RELATION + "." + LexiconKeyOrder.TERM2ID,
 	         IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
 	         ), ""+ConfigMath.s2ns(20)),

	/* The following timeouts were derived from performance after 2 hours
	 * based on U100000b/run4.  TPS was 183k at that point in run4.  I introduced
	 * the idle timeout for these indices in U100000b/run7 since clients were
	 * slowing down as the #of index partitions increased over time since they
	 * have to fill a chunk before writing on the corresponding index partition
	 * but the #of index partitions was increasing, forcing the clients to 
	 * process more documents before driving a write on any given index partition.
	 * I chose a value at 2h since the queue length on the data services will
	 * increase over time as more index partitions are created and the clients
	 * submit more tasks which are queued concurrenty.  This is just a heuristic.
	 * A different mechansim should be used to increase client load in proportion
	 * with the database scale.
	 */
         new NV(com.bigdata.config.Configuration.getOverrideProperty
 	       ( namespace + "." + LexiconRelation.NAME_LEXICON_RELATION + "." + LexiconKeyOrder.ID2TERM,
 	         IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
 	         ), ""+ConfigMath.s2ns(95)),
         new NV(com.bigdata.config.Configuration.getOverrideProperty
 	       ( namespace + "." + SPORelation.NAME_SPO_RELATION + "." + SPOKeyOrder.SPO,
 	         IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
 	         ), ""+ConfigMath.s2ns(130)),
         new NV(com.bigdata.config.Configuration.getOverrideProperty
 	       ( namespace + "." + SPORelation.NAME_SPO_RELATION + "." + SPOKeyOrder.OSP,
 	         IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
 	         ), ""+ConfigMath.s2ns(130)),
         new NV(com.bigdata.config.Configuration.getOverrideProperty
 	       ( namespace + "." + SPORelation.NAME_SPO_RELATION + "." + SPOKeyOrder.POS,
 	         IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
 	         ), ""+ConfigMath.s2ns(110)),

	//
	// scatter split overrides
	//

	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SCATTER_SPLIT_PERCENT_OF_SPLIT_THRESHOLD
		  ), ""+scatterSplitPercentOfSplitThreshold),

	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SCATTER_SPLIT_DATA_SERVICE_COUNT
		  ), ""+scatterSplitDataServiceCount),

	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SCATTER_SPLIT_INDEX_PARTITION_COUNT
		  ), ""+scatterSplitIndexPartitionCount),
    new NV(com.bigdata.config.Configuration
       .getOverrideProperty(namespace + "."
			    + LexiconRelation.NAME_LEXICON_RELATION + "."
			    + LexiconKeyOrder.TERM2ID,
			    IndexMetadata.Options.SCATTER_SPLIT_INDEX_PARTITION_COUNT),
       "" +scatterSplitIndexPartitionCount_term2id),

    };

}

com.bigdata.samples.ScaleOut {

    properties = lubm.properties;
    
}

/**
 * Mapped, distributed bulk loader configuration.
 */
com.bigdata.rdf.load.MappedRDFDataLoadMaster {

    // The job name (same as the KB namespace by default).
    jobName = lubm.namespace;

	// The #of client tasks to execute.
    nclients = bigdata.clientServiceCount;

    /* The directory for scheduled index partition dumps for runs. May be NAS
     * or a local file system.  Only the master writes on this directory.
     */
	indexDumpDir = new File("@NAS@/"+jobName+"-indexDumps");

	// must be specified for indexDumpDir to work.
	indexDumpNamespace = lubm.namespace;
	
    // The KB name.
    namespace = lubm.namespace;
    
	// KB properties made visible to JiniFederation#getProperties()
	properties = lubm.properties;

	// When true, a pre-existing job with the same name is deleted first.
	deleteJob = true;

    // Scanner identifies resources to be loaded.
    resourceScannerFactory = com.bigdata.service.jini.master.FileSystemScanner.newFactory(
    	new File("@NAS@/lubm/U10"), // dataDir
        //new File("/nas/metrics/lehigh/U10-compressed"), // dataDir
    	new com.bigdata.rdf.load.RDFFilenameFilter() // optional filename filter.
    	);

    // The ontology to load (file or directory) when the KB is created.
    ontology = new File("@install.lubm.config.dir@/univ-bench.owl");
	//ontology = new File("/nas/metrics/lehigh/univ-bench.owl");

	// The maximum thread pool size for RDF parser tasks.
    //parserPoolSize = 5;

	// The capacity of the work queue for the parser thread pool.
	//parserQueueCapacity = parserPoolSize;

	// The maximum #of parsed but not yet buffered statements before the parser
	// thread pool is paused (limits RAM demand by the client).
	unbufferedStatementThreshold = ConfigMath.multiply(1,Bytes.megabyte);

	// The maximum thread pool size for buffering writes for the TERM2ID index.
	//term2IdWriterPoolSize = 5;
	
	// The maximum thread pool size for buffering writes for the TERM2ID index.
	//otherWriterPoolSize = 5;

	// The maximum thread pool size asynchronous notifications.
	// notifyPoolSize = 5;

	// Size of chunks written onto the asynchronous write API buffers.
	producerChunkSize = 20000;
	
	// Initial capacity of the RDF Value hash map.
	valuesInitialCapacity = 20000;
	
	// Initial capacity of the RDF Blank node hash map.
	bnodesInitialCapacity = 16;

    // create the KB if not found.
    create = true;

    // when true, deletes each source file once loaded successfully.
    deleteAfter = false;

    // when true, loads data.
    loadData = true;

    // when true, computes closure.
    computeClosure = true;

	/*
	 * When true, requests a compacting merge of the data services in
	 * the federation before computing the closure.
	 */
	forceOverflowBeforeClosure = false;

	// force overflow of all data services after the job ends (optimizes for query).
	forceOverflow = false;

    /* How long the master will wait in milliseconds to discover the services
     * that you specify for [servicesTemplates] and [clientsTemplate].
     */
    awaitServicesTimeout = 10000;
    
	/* The minimum set of services which must be discovered before the master
	 * can start.
	 */
	servicesTemplates = new ServicesTemplate[] {

		// the metadata service
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IMetadataService.class
				},
				null/*attributes*/),
			null/*filter*/
			),

		// the data services (filter is required to exclude metadata services)
		new ServicesTemplate(
			bigdata.dataServiceCount/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IDataService.class
				},
				null/*attributes*/),
			DataServiceFilter.INSTANCE/*filter*/
			),

		// the load balancer
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[] {
					com.bigdata.service.ILoadBalancerService.class
				},
				null/*attributes*/),
			null/*filter*/
			)
		
	};

    /* Template for matching the services to which the clients will be
     * distributed for execution.  Normally you will specify
     * IClientService as the interface to be discovered.  While it is
     * possible to run tasks on an IDataService or even an
     * IMetadataService since they both implement IRemoteExecutor, it
     * is generally discouraged unless the tasks require explicit
     * access to the local index partitions for their execution.
     */
	clientsTemplate = new ServicesTemplate(
			bigdata.clientServiceCount, // minMatches
			new ServiceTemplate(
				null, //serviceID
				new Class[]{
					com.bigdata.service.IClientService.class
				},
				null // attributes
			    ),
			null // filter
			);

	/*
	 * RDF distributed data loader options.
	 */

}

/**
 * LUBM distributed bulk loader configuration.  This is designed to
 * dynamically generate and load the files so you can load very large
 * data sets without having to pre-generate and store the data on a
 * shared file system.
 */
edu.lehigh.swat.bench.ubt.bigdata.LubmGeneratorMaster {

	/*
	 * General options for a master executing distributed tasks.
	 */

    // The job name (same as the KB namespace by default).
    jobName = lubm.namespace;

	// The #of client tasks to execute.
    nclients = bigdata.clientServiceCount;

	// Properties to be made visible to JiniFederation#getProperties()
	properties = lubm.properties;

    /* The directory for scheduled index partition dumps for runs. May be NAS
     * or a local file system.  Only the master writes on this directory.
     */
	indexDumpDir = new File("@NAS@/lubm/"+jobName+"-indexDumps");

	// must be specified for indexDumpDir to work.
	indexDumpNamespace = lubm.namespace;
	
	// force overflow of all data services after the job ends (optimizes for query).
	forceOverflow = true;

    /* How long the master will wait in milliseconds to discover the services
     * that you specify for [servicesTemplates] and [clientsTemplate].
     */
    awaitServicesTimeout = 10000;
    
	/* The minimum set of services which must be discovered before the master
	 * can start.
	 */
	servicesTemplates = new ServicesTemplate[] {

		// the metadata service
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IMetadataService.class
				},
				null/*attributes*/),
			null/*filter*/
			),

		// the data services (filter is required to exclude metadata services)
		new ServicesTemplate(
			bigdata.dataServiceCount/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IDataService.class
				},
				null/*attributes*/),
			DataServiceFilter.INSTANCE/*filter*/
			),

		// the load balancer
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[] {
					com.bigdata.service.ILoadBalancerService.class
				},
				null/*attributes*/),
			null/*filter*/
			)
		
	};

    /* Template for matching the services to which the clients will be
     * distributed for execution.  Normally you will specify
     * IClientService as the interface to be discovered.  While it is
     * possible to run tasks on an IDataService or even an
     * IMetadataService since they both implement IRemoteExecutor, it
     * is generally discouraged unless the tasks require explicit
     * access to the local index partitions for their execution.
     */
	clientsTemplate = new ServicesTemplate(
			bigdata.clientServiceCount, // minMatches
			new ServiceTemplate(
				null, //serviceID
				new Class[]{
					com.bigdata.service.IClientService.class
				},
				null // attributes
			    ),
			null // filter
			);

	/*
	 * RDF distributed data loader options.
	 */

    // The KB name.
    namespace = lubm.namespace;
    
    // Used if reading files from the file system rather than a queue.
    dataDir = new File(lubm.dataDir);

    // The ontology to load (file or directory)
    ontology = new File("@install.lubm.config.dir@/univ-bench.owl");

	// The maximum thread pool size for RDF parser tasks.
    parserPoolSize = 5;

	// The capacity of the work queue for the parser thread pool.
	parserQueueCapacity = parserPoolSize;

	// The maximum #of parsed but not yet buffered statements before the parser
	// thread pool is paused (limits RAM demand by the client).
	unbufferedStatementThreshold = ConfigMath.multiply(2,Bytes.megabyte);

	// The maximum thread pool size for buffering writes for the TERM2ID index.
	term2IdWriterPoolSize = 5;
	
	// The maximum thread pool size for buffering writes for the TERM2ID index.
	otherWriterPoolSize = 5;

	// Size of chunks written onto the asynchronous write API buffers.
	producerChunkSize = 20000;
	
	// Initial capacity of the RDF Value hash map.
	valuesInitialCapacity = 20000;
	
	// Initial capacity of the RDF Blank node hash map.
	bnodesInitialCapacity = 16;

    // create the KB if not found.
    create = true;

    // when true, deletes each source file once loaded successfully.
    deleteAfter = false;

    // when true, loads data.
    loadData = true;

    // when true, computes closure.
    computeClosure = true;

	/*
	 * When true, requests a compacting merge of the data services in
	 * the federation before computing the closure.
	 */
	forceOverflowBeforeClosure = true;

	/*
	 * LUBM Generator integration options.
	 */

    /* You have a choice of "Generate", "Load", or "GenerateAndLoad".
     * The lubm generator is single threaded.  On a cluster of server
     * class machines, the individual lubm data generators can not
     * keep up with the RDF parser and the database.
     */
    //runMode = "Generate";
    //runMode = "Load";
    runMode = "GenerateAndLoad";

    // The #of universities to generate. 
    univNum = lubm.univNum;
    
    /* Specify where to put the generated data files and the generated
       log files.  The big choice is whether to put the data onto a
       shared volume or to write the data onto a local volume, in
       which case it must be consumed by the same host which generated
       the data. */
    outDir = dataDir;

    /* The LUBM generator log directory.  May be NAS or local file system.
     * Each client writes on this directory.
     *
     * Note: These log files can grow quite large for prolonged runs and are
     * frankly of little interest.  As a special case, this may be [/dev/null]
     * to direct the log output to that device.
     */
    logDir = new File("/dev/null"); //"@LAS@/lubm/"+jobName+"-log"

	/* The #of files that can be queued up by the generator before it
	   will block. */
//	generatorQueueCapacity = 0; // SynchronousQueue
     generatorQueueCapacity = (int)ConfigMath.multiply(2,parserPoolSize); // blocking
	// generatorQueueCapacity = Integer.MAX_VALUE; // unbounded 

    /* More lubm options (not very likely to change).
     */
    
    startIndex = 0;
    
    ontologyURL = "http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl";

}

com.bigdata.service.jini.DumpFederation {

	namespace = lubm.namespace;

}

com.bigdata.service.jini.BroadcastSighup {

	pushConfig = false;
	
	restartServices = true;

}

/**
 * Configuration for a throughput testing utility.
 */
com.bigdata.service.jini.benchmark.ThroughputMaster {

	//forceOverflow=true;

	jobName = "test_1";
	
	namespace = "test.throughputTest.test_1";

    /* How long the master will wait in milliseconds to discover the services
     * that you specify
     */
    awaitServicesTimeout = 5000;
    
	/* The minimum set of services which must be discovered before the master
	 * can start.
	 */
	servicesTemplates = new ServicesTemplate[] {

		// the metadata service
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IMetadataService.class
				},
				null/*attributes*/),
			null/*filter*/
			),

		// the data services (filter is required to exclude metadata services)
		new ServicesTemplate(
			bigdata.dataServiceCount/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IDataService.class
				},
				null/*attributes*/),
			DataServiceFilter.INSTANCE/*filter*/
			),

		// the load balancer
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[] {
					com.bigdata.service.ILoadBalancerService.class
				},
				null/*attributes*/),
			null/*filter*/
			)
		
	};

    /* Template for matching the services to which the clients will be
     * distributed for execution.  Normally you will specify
     * IClientService as the interface to be discovered.  While it is
     * possible to run tasks on an IDataService or even an
     * IMetadataService since they both implement IRemoteExecutor, it
     * is generally discouraged unless the tasks require explicit
     * access to the local index partitions for their execution.
     */
	clientsTemplate = new ServicesTemplate(
			bigdata.clientServiceCount, // minMatches
			new ServiceTemplate(
				null, //serviceID
				new Class[]{
					com.bigdata.service.IClientService.class
				},
				null), // attributes
			null // filter
			);

	/* The initial #of index partitions for the scale-out index
	 * (computed as #partitions per data service). Choose at least
	 * one per core if you want to fully load your machines.
	 */
	npartitions = ConfigMath.multiply(10, bigdata.dataServiceCount);

	/* #of clients to start across the federation.
	 * 
	 * Note: If the clients choose the index partition for each
	 * operation randomly then this must be at least N times the
	 * #of index partitions to expect an average queue length of N
	 * for that index partition.  If index partitions are split,
	 * then you will need more clients to achieve the same queue
	 * length. Also, since client operations have RMI latency, you
	 * will need more than N to achieve a queue length of N.
	 * 
	 * Note: If the clients choose the key range on which they
	 * will write statically [startKeyPartitions := true] then it
	 * makes the most sense to directly specify the #of clients or
	 * to specify the #of clients per data service.
	 */
	//nclients = ConfigMath.multiply(200, npartitions);
	nclients = ConfigMath.multiply(10, bigdata.dataServiceCount);

	startKeyPartitions = true;
	
	operationCount =    1000000; // 1M
//	operationCount =   10000000; // 10M
//	operationCount =  100000000; // 100M
//	operationCount = 1000000000; // 1B

	maxKeysPerOp = 1000;
	
//	incRange = 100; // very tight
	//incRange = 1000; // tight
	incRange = 10000; // loose
	
    /* When true, uses the asynchronous write API.  Otherwise use the
     * synchronous RPC API.  The asynchronous write API buffers writes
     * destined for each index partition and can substantially increase
     * the throughput of distributed index writes, especially when the
     * the application performs a large #of small writes and those writes
     * are scattered across a large #of index partitions.
     */
    asynchronous = true;

	// disable zookeeper updates by the test clients.
	zookeeperUpdateInterval = 0;
	//zookeeperUpdateInterval = 100000;

}

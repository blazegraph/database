import net.jini.jeri.BasicILFactory;
import net.jini.jeri.BasicJeriExporter;
import net.jini.jeri.tcp.TcpServerEndpoint;

import net.jini.discovery.LookupDiscovery;
import net.jini.core.discovery.LookupLocator;
import net.jini.core.entry.Entry;
import net.jini.lookup.entry.Name;
import net.jini.lookup.entry.Comment;
import net.jini.lookup.entry.Address;
import net.jini.lookup.entry.Location;
import net.jini.lookup.entry.ServiceInfo;
import net.jini.core.lookup.ServiceTemplate;

import java.io.File;
import java.net.InetAddress;
import java.net.InetSocketAddress;
import java.util.UUID;

import com.bigdata.util.NV;
import com.bigdata.util.config.NicUtil;
import com.bigdata.journal.Options;
import com.bigdata.journal.BufferMode;
import com.bigdata.journal.Journal;
import com.bigdata.journal.jini.ha.HAJournal;
import com.bigdata.jini.lookup.entry.*;
import com.bigdata.service.IBigdataClient;
import com.bigdata.service.AbstractTransactionService;
import com.bigdata.service.jini.*;
import com.bigdata.service.jini.lookup.DataServiceFilter;
import com.bigdata.service.jini.master.ServicesTemplate;
import com.bigdata.jini.start.config.*;
import com.bigdata.jini.util.ConfigMath;

import org.apache.zookeeper.ZooDefs;
import org.apache.zookeeper.data.ACL;
import org.apache.zookeeper.data.Id;

// imports for various options.
import com.bigdata.btree.IndexMetadata;
import com.bigdata.btree.keys.KeyBuilder;
import com.bigdata.rdf.sail.BigdataSail;
import com.bigdata.rdf.spo.SPORelation;
import com.bigdata.rdf.spo.SPOKeyOrder;
import com.bigdata.rdf.lexicon.LexiconRelation;
import com.bigdata.rdf.lexicon.LexiconKeyOrder;
import com.bigdata.rawstore.Bytes;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeUnit.*;

/*
 * This is a sample configuration file for a highly available Journal. A
 * version of this file must be available to each HAJournalServer in the
 * pipeline.  The pipeline depends on the stable assignment of ServiceID
 * to HAJournalServers.  A unique ServiceID must be explicitly assigned to 
 * each HAJournalServer in its configuration entry.  The ordered list of 
 * those ServiceIDs is shared by all services and defines the write 
 * replication pipeline. The first entry in the write replication pipeline
 * is the leader (aka master).  You can use UUID.randomUUID() or GenUUID
 * to create UUIDs.
 *
 * Note: The ServiceUUID Entry MUST be different for each file.  It assigns
 * a ServiceID to the service!
 */

/*
 * Globals.
 */
bigdata {

   private static fedname = "benchmark";

   // NanoSparqlServer (http) port.
   private static nssPort = 8090;
   
   // write replication pipeline port (listener).
   private static haPort = 9090;
   
   // The #of services in the write pipeline.
   private static replicationFactor = 3;

   // The logical service identifier shared by all members of the quorum.
   private static logicalServiceId = "HAJournal-1";
   
   // The ServiceID for *this* service -or- null to assign it dynamically.
   private static serviceId = null;
   
   // The service directory (if serviceId is null, then you must override).
   // private static serviceDir = new File(fedname,""+serviceId);
   private static serviceDir = new File(fedname,"HAJournalServer");
   
   // journal data directory.
   private static dataDir = serviceDir;

   // HA log directory.
   private static logDir = new File(serviceDir,"logs");
   
   // one federation, multicast discovery.
   //static private groups = LookupDiscovery.ALL_GROUPS;

   // unicast discovery or multiple setups, MUST specify groups.
   static private groups = new String[]{bigdata.fedname};

    /**
     * One or more unicast URIs of the form <code>jini://host/</code>
     * or <code>jini://host:port/</code> (no default).
     *
     * This MAY be an empty array if you want to use multicast
     * discovery <strong>and</strong> you have specified the groups as
     * LookupDiscovery.ALL_GROUPS (a <code>null</code>).
     */
    static private locators = new LookupLocator[] {

      // runs jini on the localhost using unicast locators.
      new LookupLocator("jini://bigdata17/")
   
      // runs jini on one or more hosts using unicast locators.
      //new LookupLocator("jini://"+jini1),
      //new LookupLocator("jini://"+jini2),

    };

    /**
     * A common point to set the Zookeeper client's requested
     * sessionTimeout and the jini lease timeout.  The default lease
     * renewal period for jini is 5 minutes while for zookeeper it is
     * more like 5 seconds.  This puts the two systems onto a similar
     * timeout period so that a disconnected client is more likely to
     * be noticed in roughly the same period of time for either
     * system.  A value larger than the zookeeper default helps to
     * prevent client disconnects under sustained heavy load.
     *
     * If you use a short lease timeout (LT 20s), then you need to override 
     * properties properties for the net.jini.lease.LeaseRenewalManager
     * or it will run in a tight loop (it's default roundTripTime is 10s
     * and it schedules lease renewals proactively.)
     */

    // jini
    static private leaseTimeout = ConfigMath.s2ms(60); // 20

    // zookeeper
    static private sessionTimeout = (int)ConfigMath.s2ms(60); // 5 

    /*
     * Configuration for default KB.
     */

    private static namespace = "kb";
    
    private static kb = new NV[] {
      
      /* Setup for QUADS mode without the full text index. 
      new NV(BigdataSail.Options.TRUTH_MAINTENANCE, "false" ),
      new NV(BigdataSail.Options.QUADS, "true"),
      new NV(BigdataSail.Options.STATEMENT_IDENTIFIERS, "false"),
      new NV(BigdataSail.Options.TEXT_INDEX, "false"),
      new NV(BigdataSail.Options.AXIOMS_CLASS,"com.bigdata.rdf.axioms.NoAxioms"),
      */
      /* Setup for triples without inference or the full text index. */
      new NV(BigdataSail.Options.TRUTH_MAINTENANCE, "false" ),
      new NV(BigdataSail.Options.QUADS, "false"),
      new NV(BigdataSail.Options.STATEMENT_IDENTIFIERS, "false"),
      new NV(BigdataSail.Options.TEXT_INDEX, "false"),
      new NV(BigdataSail.Options.AXIOMS_CLASS,"com.bigdata.rdf.axioms.NoAxioms"),
      
      new NV(BigdataSail.Options.QUERY_TIME_EXPANDER, "false"),

      // Bump up the branching factor for the lexicon indices on the named kb.
      // com.bigdata.namespace.kb.lex.com.bigdata.btree.BTree.branchingFactor=400
      new NV(com.bigdata.config.Configuration.getOverrideProperty
          ( namespace + "." + LexiconRelation.NAME_LEXICON_RELATION,
            IndexMetadata.Options.BTREE_BRANCHING_FACTOR
            ), "400"),

      // Bump up the branching factor for the statement indices on the named kb.
      // com.bigdata.namespace.kb.spo.com.bigdata.btree.BTree.branchingFactor=1024
      new NV(com.bigdata.config.Configuration.getOverrideProperty
          ( namespace + "." + SPORelation.NAME_SPO_RELATION,
            IndexMetadata.Options.BTREE_BRANCHING_FACTOR
            ), "1024"),

/* BSBM 100M optimizations.

      new NV(com.bigdata.config.Configuration.getOverrideProperty
          ( namespace + "." + LexiconRelation.NAME_LEXICON_RELATION+".TERM2ID",
            IndexMetadata.Options.BTREE_BRANCHING_FACTOR
            ), "800"),

      new NV(com.bigdata.config.Configuration.getOverrideProperty
          ( namespace + "." + LexiconRelation.NAME_LEXICON_RELATION+".ID2TERM",
            IndexMetadata.Options.BTREE_BRANCHING_FACTOR
            ), "800"),

      // Tighter coding for the BSBM vocabulary.
      new NV(BigdataSail.Options.VOCABULARY_CLASS,"com.bigdata.rdf.vocab.BSBMVocabulary"),
      
      // Inlining for "USD" datatype.
      new NV(BigdataSail.Options.EXTENSION_FACTORY_CLASS,"com.bigdata.rdf.internal.BSBMExtensionFactory"),

    // Override the number of statements for batch inserts (default 10k)
    new NV(com.bigdata.rdf.sail.BigdataSail.Options.BUFFER_CAPACITY,"1000000"),

    // Override the #of write cache buffers.                                                                                           
    new NV(com.bigdata.journal.Options.WRITE_CACHE_BUFFER_COUNT,"2000"),
    new NV(com.bigdata.journal.Options.WRITE_CACHE_MAX_DIRTY_LIST_SIZE,"1900"),
    new NV(com.bigdata.journal.Options.WRITE_CACHE_COMPACTION_THRESHOLD,"20"),
    };

*/

}

/*
 * Zookeeper client configuration.
 */
org.apache.zookeeper.ZooKeeper {

    /* Root znode for the federation instance. */
    zroot = "/" + bigdata.fedname;

    /* A comma separated list of host:port pairs, where the port is
     * the CLIENT port for the zookeeper server instance.
     */
    // standalone.
    servers = "bigdata17:2081";
    // ensemble
//  servers =   bigdata.zoo1+":2181"
//             + ","+bigdata.zoo2+":2181"
//        + ","+bigdata.zoo3+":2181"
//       ;

    /* Session timeout (optional). */
    sessionTimeout = bigdata.sessionTimeout;

    /* 
     * ACL for the zookeeper nodes created by the bigdata federation.
     *
     * Note: zookeeper ACLs are not transmitted over secure channels
     * and are placed into plain text Configuration files by the
     * ServicesManagerServer.
     */
    acl = new ACL[] {

       new ACL(ZooDefs.Perms.ALL, new Id("world", "anyone"))

    };

}

/*
 * You should not have to edit below this line.
 */

/*
 * Jini client configuration.
 */
com.bigdata.service.jini.JiniClient {

    groups = bigdata.groups;

    locators = bigdata.locators;
    
    entries = new Entry[] {
       
       // Optional metadata entries.
           
    };

}

net.jini.lookup.JoinManager {

   maxLeaseDuration = bigdata.leaseTimeout;
   
}

/*
 * Server configuration options.
 */
com.bigdata.journal.jini.ha.HAJournalServer {

   serviceDir = bigdata.serviceDir;

   logicalServiceId = bigdata.logicalServiceId;
   
   writePipelineAddr = new InetSocketAddress(//
                    InetAddress.getByName(//
                            NicUtil.getIpAddress("default.nic", "default",
                                    false// loopbackOk
                            )), //
                    bigdata.haPort
            );

   pipelineUUIDs = bigdata.pipeline;

   replicationFactor = bigdata.replicationFactor;

}

/*
 * Journal configuration.
 */
com.bigdata.journal.jini.ha.HAJournal {

   properties = (NV[]) ConfigMath.concat(new NV[] {
   
      new NV(Options.FILE,
         ConfigMath.getAbsolutePath(new File(bigdata.dataDir,"bigdata-ha.jnl"))),
   
      new NV(Options.BUFFER_MODE,""+BufferMode.DiskRW),

      new NV(Options.WRITE_CACHE_BUFFER_COUNT,"12"),// TODO Experimental value (default is 6).

      new NV(IndexMetadata.Options.WRITE_RETENTION_QUEUE_CAPACITY,"4000"),

      new NV(IndexMetadata.Options.BTREE_BRANCHING_FACTOR,"128"),

      new NV(AbstractTransactionService.Options.MIN_RELEASE_AGE,"1"),

//      new NV(HAJournal.Options.HA_LOG_DIR, ""+bigdata.logDir),

      /* Enable statistics collection and reporting. */

      new NV(Journal.Options.COLLECT_QUEUE_STATISTICS,"true"),
      new NV(Journal.Options.COLLECT_PLATFORM_STATISTICS,"true"),
      new NV(Journal.Options.GANGLIA_REPORT,"true"),
      //new NV(Journal.Options.GANGLIA_LISTEN,"true"),

   }, bigdata.kb);

}

/*
 * NanoSparqlServer configuration.
 */
com.bigdata.rdf.sail.webapp.NanoSparqlServer {

    namespace = bigdata.namespace;

    create = true;
    
    queryThreadPoolSize = 16;
    
    describeEachNamedGraph = true;

    port = bigdata.nssPort;
    
}

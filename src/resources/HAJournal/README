Bigdata Highly Available Replication Cluster

*** See the HAJournalServer on the wiki for more information ***

========== INSTALL ==========

0. The nodes MUST have synchronized clocks, both for logging and to
   ensure that the transaction services have closely correlated clocks
   for assigning timestamps.  Make sure that ntp or a similar service
   is running to synchronize the clocks.

1. Edit the various configuration files.  At a minimum, you must edit
   bigdataHA.conf.

2. Make sure that ZooKeeper is up and running with a consistent
   configuration and that it is logging somewhere where you can find
   the log later. For a highly available ZooKeeper configuration, you
   need to be running at least 3 ZooKeeper nodes.  Consult the
   ZooKeeper documentation for more information.

   Bigdata does NOT start/stop Apache ZooKeeper. ZooKeeper is
   generally administered separately.  If you are not already using
   Apache ZooKeeper, then you should install three VMs with Apache
   ZooKeeper onto machines with fixed IP addresses.

   Note: If you begin with a standalone ZooKeeper instance, then you
   WILL NOT be able to automatically migrate to a highly available
   configuration without stopping your standalone ZooKeeper instance.
   Your life will be significantly easier if you begin with a highly
   available ZooKeeper instance.  Bigdata does not put a heavy load on
   ZooKeeper, but running bigdata and ZooKeeper on the same instances
   will make it more complex to administer your environment since
   stopping a single node will reduce availability for both ZooKeeper
   and bigdata.  A recommended practice is to allocate three ZooKeeper
   VMs with fixed IP addresses when you begin to setup your bigdata
   cluster.

3. Once Apache ZooKeeper is setup, do:

      sudo /etc/init.d bigdataHA start

   This will start the ClassServer, the service registrar (Reggie),
   and the HAJournalServer.  All of these processes will run inside of
   a single JVM named "ServiceStarter". See below for more information
   on these services.

========== KEY FILES ==========

/etc/init.d/bigdataHA

   An init.d script to start/stop of bigdata HA.

/etc/bigdata/bigdataHA.conf - configuration for the HA installation.

   This script is sourced by /etc/init.d/bigdataHA and provides the
   critical configuration variables for your installation.  The
   environment variables set in this script are passed through into
   startHAServices and from there into the HAJournal.config file.  You
   need to review these settings.

The following are located under the installation root:

bin/startHAServices

  Script runs the Apache River ServiceStarter.

bin/disco-tool

  A utility that can be used to identify running Apache River
  services.

doc/

  Documentation.

lib/

  The bigdata jar and other dependencies.

lib-dl/

  Downloadable jars for Apache River.

lib-ext/

  Security policy provider for Apache River.

var/config/startHAServices.config 

  An Apache River ServiceStarter configuration for starting:
  
    - ClassServer : This provides downloadable code for the lib-dl
      directory required to run Reggie.  An instance of this service
      is started on every node.

    - Reggie : This is the Apache River service registrar.  Bigdata
      services discover service registrars using locators and then
      register themselves. The service registrar is also used by the
      bigdata services to discover one another. The set of locators is
      defined using the LOCATORS environment variable in
      /etc/bigdata/bigdataHA.config; and

    - HAJournalServer : This is the highly available bigdata graph
      database engine and RDF/SPARQL end point.  The service process
      maintains all of its state in the "serviceDir". The location of
      that directory is determined by the FED_DIR environment variable
      and the HAJournal.config file.  Important files in this
      directory include:

	serviceDir/service.id     - the assigned ServiceID for this service.
	serviceDir/bigdata-ha.jnl - the journal data file.
   	serviceDir/HALog/*        - the transaction log files.
	serviceDir/snapshot       - full backups of the journal.

var/config/HAJournal.config 

  An Apache River configuration file for HAJournalServer.  You should
  review the settings in this file.  The most relevant will be the
  configuration parameters for the default kb instance, especially
  whether it is in triples mode or quads mode.  The configuration
  parameters for the journal are also specified in this file.  Many,
  but not all, parameters can be overridden through environment
  variables defined in /etc/bigdata/bigdataHA.config. This file is
  also used to configure the online backup policy (snapshotPolicy) and
  point in time restore window (restorePolicy).

var/logging/log4jHA.properties

   The HAJournalServer log4j configuration file.  Note: The default
   log4j configuration writes onto a file named "HAJournalServer.log"
   -- that is where you need to look for errors and any other
   information about the running HAJournalServer process.

var/logging/logging.properties

   A default Java logging configuration.  This may be used to control
   the log levels for jini/river components inside of the bigdata
   services. Those components use java logging rather than log4j.

var/policy/policy.all
   
   A default java permissions file.  This file grants ALL permissions.
   You may specify a more rigorous security policy.
